{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "![Lancaster University](https://www.lancaster.ac.uk/media/lancaster-university/content-assets/images/fst/logos/SCC-Logo.svg)\n",
        "\n",
        "# SCC.454: Large Scale Platforms for AI and Data Analysis\n",
        "## Lab: Locality-Sensitive Hashing for Scalable Similarity Search\n",
        "\n",
        "**Duration:** 2 hours\n",
        "\n",
        "**Learning Objectives:**\n",
        "- Understand the theory and intuition behind Locality-Sensitive Hashing (LSH)\n",
        "- Master shingling techniques for document representation\n",
        "- Implement MinHash signatures for efficient similarity estimation\n",
        "- Use Spark's built-in MinHashLSH for approximate similarity search\n",
        "- Learn Weighted MinHash for preserving term frequency information\n",
        "- Build production-ready pipelines combining MinHash with transformer reranking"
      ],
      "metadata": {
        "id": "intro_header"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Table of Contents\n",
        "\n",
        "1. **Part 1: Introduction to LSH and Similarity Search** (15 minutes)\n",
        "   - The Challenge of Similarity Search at Scale\n",
        "   - Introduction to Locality-Sensitive Hashing\n",
        "   - Setting up the Environment\n",
        "\n",
        "2. **Part 2: Document Shingling** (15 minutes)\n",
        "   - Character Shingles vs Word Shingles\n",
        "   - Implementing Shingling in Spark\n",
        "   - Representing Documents as Sets\n",
        "   - Practical Exercise: Shingling a Document Corpus\n",
        "\n",
        "3. **Part 3: MinHash Fundamentals** (20 minutes)\n",
        "   - Jaccard Similarity\n",
        "   - The MinHash Algorithm\n",
        "   - Building MinHash Signatures\n",
        "   - Estimating Similarity from Signatures\n",
        "   - Practical Exercise: Manual MinHash Implementation\n",
        "\n",
        "4. **Part 4: LSH with Spark's MinHashLSH** (20 minutes)\n",
        "   - Understanding Spark ML's MinHashLSH\n",
        "   - Feature Hashing and Vectorization\n",
        "   - Approximate Similarity Joins\n",
        "   - Approximate Nearest Neighbors\n",
        "   - Practical Exercise: Near-Duplicate Document Detection\n",
        "\n",
        "5. **Part 5: Weighted MinHash** (15 minutes)\n",
        "   - Limitations of Classic MinHash\n",
        "   - Term Frequency Weighting\n",
        "   - Weighted MinHash Algorithm\n",
        "   - Implementation in Spark\n",
        "\n",
        "6. **Part 6: Production Pipeline - MinHash + Transformer Reranking** (30 minutes)\n",
        "   - Two-Stage Retrieval Architecture\n",
        "   - Stage 1: MinHash + LSH for Candidate Retrieval\n",
        "   - Stage 2: Sentence Transformer Reranking with Spark\n",
        "   - Complete Pipeline Implementation\n",
        "   - Performance Analysis\n",
        "\n",
        "7. **Part 7: Final Challenge** (20 minutes)\n",
        "   - End-to-End Document Similarity System"
      ],
      "metadata": {
        "id": "toc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "# Part 1: Introduction to LSH and Similarity Search\n",
        "---\n",
        "\n",
        "## 1.1 The Challenge of Similarity Search at Scale\n",
        "\n",
        "Finding similar items is fundamental to many applications: duplicate detection, plagiarism checking, recommendation systems, and information retrieval. However, at scale, this becomes computationally challenging.\n",
        "\n",
        "**The Quadratic Problem:**\n",
        "- Comparing all pairs of N documents requires O(N²) comparisons\n",
        "- For 1 million documents: ~500 billion comparisons\n",
        "- Even at 1μs per comparison: ~6 days of computation\n",
        "\n",
        "**Traditional Approaches and Their Limitations:**\n",
        "```\n",
        "Brute Force:     O(N²) comparisons → Infeasible for large N\n",
        "Tree Structures: Work well in low dimensions, fail in high dimensions\n",
        "Exact Methods:   Cannot scale to billions of documents\n",
        "```\n",
        "\n",
        "**The Need for Approximate Methods:**\n",
        "- We often don't need exact answers\n",
        "- Finding \"probably similar\" items is usually sufficient\n",
        "- Trading small accuracy loss for massive speed gains"
      ],
      "metadata": {
        "id": "part1_intro"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.2 Introduction to Locality-Sensitive Hashing\n",
        "\n",
        "**Core Intuition:**\n",
        "LSH is a technique that hashes similar items into the same \"buckets\" with high probability. Unlike traditional hash functions (designed to minimize collisions), LSH *maximizes* collisions for similar items.\n",
        "\n",
        "**Key Properties of LSH:**\n",
        "1. Similar items have high probability of same hash\n",
        "2. Dissimilar items have low probability of same hash\n",
        "3. Hashing is much faster than pairwise comparison\n",
        "\n",
        "**The LSH Framework:**\n",
        "```\n",
        "Documents → Shingling → MinHash Signatures → LSH Buckets → Candidate Pairs\n",
        "```\n",
        "\n",
        "**Why LSH Works:**\n",
        "- Reduces O(N²) to approximately O(N)\n",
        "- Only compare items in the same bucket\n",
        "- High recall for truly similar items\n",
        "- Tunable precision/recall tradeoff\n",
        "\n",
        "**Mathematical Foundation:**\n",
        "For a hash family H to be (d₁, d₂, p₁, p₂)-sensitive:\n",
        "- If distance(x, y) ≤ d₁, then P[h(x) = h(y)] ≥ p₁\n",
        "- If distance(x, y) ≥ d₂, then P[h(x) = h(y)] ≤ p₂\n",
        "\n",
        "For MinHash with Jaccard similarity:\n",
        "- P[MinHash(A) = MinHash(B)] = Jaccard(A, B)"
      ],
      "metadata": {
        "id": "lsh_intro"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.3 Setting up the Environment\n",
        "\n",
        "Let's set up PySpark and all required libraries for this lab."
      ],
      "metadata": {
        "id": "setup_intro"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install required packages\n",
        "!pip install pyspark==3.5.0 -q\n",
        "!pip install sentence-transformers -q\n",
        "!pip install numpy pandas -q\n",
        "\n",
        "# Install Java (Spark requires Java)\n",
        "!apt-get install openjdk-11-jdk-headless -qq > /dev/null\n",
        "\n",
        "# Set Java environment variable\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\"\n",
        "\n",
        "print(\"All packages installed successfully!\")"
      ],
      "metadata": {
        "id": "install_packages"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import (\n",
        "    col, udf, explode, array, lit, collect_list, size,\n",
        "    lower, regexp_replace, split, monotonically_increasing_id,\n",
        "    struct, when, coalesce, broadcast\n",
        ")\n",
        "from pyspark.sql.types import (\n",
        "    ArrayType, StringType, IntegerType, FloatType,\n",
        "    StructType, StructField, DoubleType\n",
        ")\n",
        "from pyspark.ml.feature import (\n",
        "    HashingTF, CountVectorizer, MinHashLSH,\n",
        "    Tokenizer, StopWordsRemover, NGram\n",
        ")\n",
        "from pyspark.ml.linalg import Vectors, SparseVector, VectorUDT\n",
        "from pyspark.ml import Pipeline\n",
        "\n",
        "import numpy as np\n",
        "import hashlib\n",
        "from typing import List, Set, Tuple\n",
        "\n",
        "# Create a SparkSession configured for LSH operations\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"SCC454-LocalitySensitiveHashing\") \\\n",
        "    .config(\"spark.driver.memory\", \"4g\") \\\n",
        "    .config(\"spark.sql.shuffle.partitions\", \"8\") \\\n",
        "    .config(\"spark.ui.port\", \"4050\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "sc = spark.sparkContext\n",
        "\n",
        "print(f\"Spark Version: {spark.version}\")\n",
        "print(f\"Application Name: {spark.sparkContext.appName}\")\n",
        "print(\"\\nSpark Session ready for LSH operations!\")"
      ],
      "metadata": {
        "id": "create_session"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.4 Sample Dataset\n",
        "\n",
        "Let's create a sample document corpus that we'll use throughout this lab."
      ],
      "metadata": {
        "id": "sample_data_intro"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample document corpus with varying degrees of similarity\n",
        "documents = [\n",
        "    (0, \"Machine learning is a subset of artificial intelligence that enables systems to learn from data.\"),\n",
        "    (1, \"Artificial intelligence and machine learning allow computers to learn from data automatically.\"),\n",
        "    (2, \"Deep learning is a type of machine learning using neural networks with many layers.\"),\n",
        "    (3, \"The weather today is sunny with a high of 25 degrees celsius.\"),\n",
        "    (4, \"Today's weather forecast shows sunny skies and temperatures around 25 degrees.\"),\n",
        "    (5, \"Natural language processing helps computers understand human language.\"),\n",
        "    (6, \"NLP enables machines to process and understand natural human language.\"),\n",
        "    (7, \"Python is a popular programming language for data science and machine learning.\"),\n",
        "    (8, \"Data science often uses Python programming for machine learning applications.\"),\n",
        "    (9, \"The cat sat on the mat and watched the birds outside the window.\"),\n",
        "    (10, \"A small cat was sitting on a mat, watching birds through the window.\"),\n",
        "    (11, \"Apache Spark provides distributed computing for big data processing.\"),\n",
        "    (12, \"Big data processing is made efficient through distributed computing with Spark.\"),\n",
        "    (13, \"Locality sensitive hashing enables fast approximate nearest neighbor search.\"),\n",
        "    (14, \"LSH provides fast approximate nearest neighbor queries using hashing techniques.\"),\n",
        "    (15, \"The restaurant serves delicious Italian pasta and fresh salads daily.\"),\n",
        "]\n",
        "\n",
        "df_docs = spark.createDataFrame(documents, [\"id\", \"text\"])\n",
        "\n",
        "print(\"Sample Document Corpus:\")\n",
        "df_docs.show(truncate=60)"
      ],
      "metadata": {
        "id": "sample_data"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Note:** Documents 0-1, 3-4, 5-6, 7-8, 9-10, 11-12, and 13-14 are intentionally similar pairs. Document 2 is somewhat similar to 0-1, and document 15 is unrelated to others."
      ],
      "metadata": {
        "id": "sample_data_note"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "# Part 2: Document Shingling\n",
        "---\n",
        "\n",
        "## 2.1 Understanding Shingling\n",
        "\n",
        "**What is Shingling?**\n",
        "Shingling converts documents into sets of contiguous subsequences (shingles). This allows us to measure document similarity by comparing these sets.\n",
        "\n",
        "**Types of Shingles:**\n",
        "\n",
        "| Type | Description | Example (\"hello world\") |\n",
        "|------|-------------|-------------------------|\n",
        "| Character k-shingles | Contiguous k characters | {\"hel\", \"ell\", \"llo\", \"lo \", \"o w\", ...} |\n",
        "| Word n-grams | Contiguous n words | {\"hello world\"} for n=2 |\n",
        "\n",
        "**Choosing Shingle Size:**\n",
        "- Too small: High overlap even for dissimilar documents\n",
        "- Too large: Low overlap even for similar documents\n",
        "- Rule of thumb: k=5-9 for characters, n=2-4 for words"
      ],
      "metadata": {
        "id": "part2_intro"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.2 Character Shingles"
      ],
      "metadata": {
        "id": "char_shingles_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_char_shingles(text: str, k: int = 5) -> List[str]:\n",
        "    \"\"\"Generate character k-shingles from text.\"\"\"\n",
        "    # Normalize text: lowercase, remove extra whitespace\n",
        "    text = ' '.join(text.lower().split())\n",
        "    # Generate shingles\n",
        "    shingles = [text[i:i+k] for i in range(len(text) - k + 1)]\n",
        "    return shingles\n",
        "\n",
        "# Example\n",
        "sample_text = \"Hello World\"\n",
        "char_shingles = get_char_shingles(sample_text, k=5)\n",
        "print(f\"Text: '{sample_text}'\")\n",
        "print(f\"5-character shingles: {char_shingles}\")\n",
        "print(f\"Number of shingles: {len(char_shingles)}\")\n",
        "print(f\"Unique shingles: {len(set(char_shingles))}\")"
      ],
      "metadata": {
        "id": "char_shingles"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.3 Word Shingles (N-Grams)\n",
        "\n",
        "Word-level shingles (also called n-grams) are more meaningful for natural language documents. They capture semantic relationships better than character shingles."
      ],
      "metadata": {
        "id": "word_shingles_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_word_shingles(text: str, n: int = 3) -> List[str]:\n",
        "    \"\"\"Generate word n-gram shingles from text.\"\"\"\n",
        "    # Normalize and tokenize\n",
        "    words = text.lower().split()\n",
        "    # Generate n-grams\n",
        "    shingles = [' '.join(words[i:i+n]) for i in range(len(words) - n + 1)]\n",
        "    return shingles\n",
        "\n",
        "# Example\n",
        "sample_text = \"Machine learning is a subset of artificial intelligence\"\n",
        "word_shingles = get_word_shingles(sample_text, n=3)\n",
        "print(f\"Text: '{sample_text}'\")\n",
        "print(f\"\\n3-word shingles:\")\n",
        "for i, shingle in enumerate(word_shingles):\n",
        "    print(f\"  {i+1}. '{shingle}'\")"
      ],
      "metadata": {
        "id": "word_shingles"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.4 Implementing Shingling in Spark\n",
        "\n",
        "For large-scale processing, we use Spark's built-in transformers combined with custom UDFs."
      ],
      "metadata": {
        "id": "spark_shingling_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Method 1: Using Spark's NGram transformer\n",
        "def create_shingle_pipeline(n_values=[2, 3, 4]):\n",
        "    \"\"\"Create a pipeline for generating word n-grams of multiple sizes.\"\"\"\n",
        "    from pyspark.sql.functions import concat_ws, flatten\n",
        "\n",
        "    # Tokenizer\n",
        "    tokenizer = Tokenizer(inputCol=\"text_clean\", outputCol=\"words\")\n",
        "\n",
        "    # Create NGram transformers for each n\n",
        "    ngram_transformers = []\n",
        "    for n in n_values:\n",
        "        ngram = NGram(n=n, inputCol=\"words\", outputCol=f\"ngrams_{n}\")\n",
        "        ngram_transformers.append(ngram)\n",
        "\n",
        "    return tokenizer, ngram_transformers\n",
        "\n",
        "# Preprocess text\n",
        "df_clean = df_docs.withColumn(\n",
        "    \"text_clean\",\n",
        "    lower(regexp_replace(col(\"text\"), r\"[^a-zA-Z\\s]\", \"\"))\n",
        ")\n",
        "\n",
        "# Apply tokenizer\n",
        "tokenizer = Tokenizer(inputCol=\"text_clean\", outputCol=\"words\")\n",
        "df_tokenized = tokenizer.transform(df_clean)\n",
        "\n",
        "# Apply n-gram transformers for n=2, 3, 4\n",
        "ngram_2 = NGram(n=2, inputCol=\"words\", outputCol=\"ngrams_2\")\n",
        "ngram_3 = NGram(n=3, inputCol=\"words\", outputCol=\"ngrams_3\")\n",
        "ngram_4 = NGram(n=4, inputCol=\"words\", outputCol=\"ngrams_4\")\n",
        "\n",
        "df_shingles = ngram_2.transform(df_tokenized)\n",
        "df_shingles = ngram_3.transform(df_shingles)\n",
        "df_shingles = ngram_4.transform(df_shingles)\n",
        "\n",
        "print(\"Document with word shingles (n=2,3,4):\")\n",
        "df_shingles.select(\"id\", \"ngrams_2\", \"ngrams_3\").show(5, truncate=50)"
      ],
      "metadata": {
        "id": "spark_shingling"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Method 2: Combine all n-grams into a single shingle set\n",
        "from pyspark.sql.functions import concat, array_union, array_distinct\n",
        "\n",
        "# Combine 2-grams, 3-grams, and 4-grams into a single shingle set\n",
        "df_combined_shingles = df_shingles.withColumn(\n",
        "    \"all_shingles\",\n",
        "    array_distinct(\n",
        "        concat(col(\"ngrams_2\"), col(\"ngrams_3\"), col(\"ngrams_4\"))\n",
        "    )\n",
        ")\n",
        "\n",
        "print(\"Combined shingles (2-4 grams):\")\n",
        "df_combined_shingles.select(\"id\", \"all_shingles\").show(3, truncate=80)"
      ],
      "metadata": {
        "id": "combine_shingles"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.5 Practical Exercise: Analyzing Shingle Overlap\n",
        "\n",
        "Let's analyze how shingle overlap relates to document similarity."
      ],
      "metadata": {
        "id": "shingle_overlap_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate Jaccard similarity from shingle sets\n",
        "def jaccard_similarity(set1: set, set2: set) -> float:\n",
        "    \"\"\"Calculate Jaccard similarity between two sets.\"\"\"\n",
        "    if not set1 or not set2:\n",
        "        return 0.0\n",
        "    intersection = len(set1 & set2)\n",
        "    union = len(set1 | set2)\n",
        "    return intersection / union if union > 0 else 0.0\n",
        "\n",
        "# Get shingles for comparison\n",
        "shingle_data = df_combined_shingles.select(\"id\", \"text\", \"all_shingles\").collect()\n",
        "\n",
        "# Compare a few document pairs\n",
        "pairs_to_compare = [(0, 1), (0, 2), (0, 3), (3, 4), (9, 10), (0, 15)]\n",
        "\n",
        "print(\"Jaccard Similarity Analysis:\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "for i, j in pairs_to_compare:\n",
        "    doc_i = next(row for row in shingle_data if row.id == i)\n",
        "    doc_j = next(row for row in shingle_data if row.id == j)\n",
        "\n",
        "    set_i = set(doc_i.all_shingles)\n",
        "    set_j = set(doc_j.all_shingles)\n",
        "\n",
        "    jaccard = jaccard_similarity(set_i, set_j)\n",
        "\n",
        "    print(f\"\\nDocs {i} vs {j}: Jaccard = {jaccard:.4f}\")\n",
        "    print(f\"  Doc {i}: {doc_i.text[:60]}...\")\n",
        "    print(f\"  Doc {j}: {doc_j.text[:60]}...\")\n",
        "    print(f\"  Shingles: {len(set_i)} vs {len(set_j)}, Overlap: {len(set_i & set_j)}\")"
      ],
      "metadata": {
        "id": "shingle_overlap"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Self-Check Questions:**\n",
        "1. Why do documents 0 and 1 have higher Jaccard similarity than documents 0 and 3?\n",
        "2. What happens to Jaccard similarity if we use smaller n-grams (n=2 only)?\n",
        "3. How does the shingle set size affect similarity calculations?"
      ],
      "metadata": {
        "id": "shingle_selfcheck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "# Part 3: MinHash Fundamentals\n",
        "---\n",
        "\n",
        "## 3.1 The Jaccard Similarity Problem\n",
        "\n",
        "Computing Jaccard similarity directly has limitations:\n",
        "- Requires storing and comparing full sets\n",
        "- Memory-intensive for large vocabularies\n",
        "- O(n) per comparison where n is set size\n",
        "\n",
        "**MinHash Solution:**\n",
        "Create compact \"signatures\" that preserve similarity information.\n",
        "\n",
        "## 3.2 The MinHash Algorithm\n",
        "\n",
        "**Key Insight:**\n",
        "For any random permutation π of the universal set:\n",
        "```\n",
        "P[min(π(A)) = min(π(B))] = Jaccard(A, B)\n",
        "```\n",
        "\n",
        "**Algorithm:**\n",
        "1. Choose k random hash functions (simulating permutations)\n",
        "2. For each document, compute k minimum hash values\n",
        "3. The k values form the \"MinHash signature\"\n",
        "4. Signature similarity estimates Jaccard similarity"
      ],
      "metadata": {
        "id": "part3_intro"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# MinHash Implementation from Scratch\n",
        "\n",
        "class MinHasher:\n",
        "    \"\"\"MinHash signature generator.\"\"\"\n",
        "\n",
        "    def __init__(self, num_hashes: int = 100, seed: int = 42):\n",
        "        \"\"\"\n",
        "        Initialize MinHasher with num_hashes hash functions.\n",
        "\n",
        "        Args:\n",
        "            num_hashes: Number of hash functions (signature length)\n",
        "            seed: Random seed for reproducibility\n",
        "        \"\"\"\n",
        "        self.num_hashes = num_hashes\n",
        "        np.random.seed(seed)\n",
        "\n",
        "        # Generate random coefficients for hash functions\n",
        "        # h(x) = (a*x + b) mod c\n",
        "        self.max_hash = 2**32 - 1\n",
        "        self.a = np.random.randint(1, self.max_hash, size=num_hashes)\n",
        "        self.b = np.random.randint(0, self.max_hash, size=num_hashes)\n",
        "        self.c = 4294967311  # Large prime\n",
        "\n",
        "    def _hash_element(self, element: str) -> int:\n",
        "        \"\"\"Hash a string element to an integer.\"\"\"\n",
        "        return int(hashlib.md5(element.encode()).hexdigest(), 16) % self.max_hash\n",
        "\n",
        "    def get_signature(self, shingle_set: set) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Generate MinHash signature for a set of shingles.\n",
        "\n",
        "        Args:\n",
        "            shingle_set: Set of string shingles\n",
        "\n",
        "        Returns:\n",
        "            MinHash signature array of length num_hashes\n",
        "        \"\"\"\n",
        "        # Initialize signature with infinity\n",
        "        signature = np.full(self.num_hashes, np.inf)\n",
        "\n",
        "        for shingle in shingle_set:\n",
        "            # Hash the shingle to an integer\n",
        "            shingle_hash = self._hash_element(shingle)\n",
        "\n",
        "            # Apply all hash functions and keep minimum\n",
        "            hash_values = (self.a * shingle_hash + self.b) % self.c\n",
        "            signature = np.minimum(signature, hash_values)\n",
        "\n",
        "        return signature.astype(np.int64)\n",
        "\n",
        "    def estimate_similarity(self, sig1: np.ndarray, sig2: np.ndarray) -> float:\n",
        "        \"\"\"\n",
        "        Estimate Jaccard similarity from MinHash signatures.\n",
        "\n",
        "        Returns:\n",
        "            Estimated Jaccard similarity\n",
        "        \"\"\"\n",
        "        return np.mean(sig1 == sig2)\n",
        "\n",
        "# Create MinHasher\n",
        "minhasher = MinHasher(num_hashes=200)\n",
        "\n",
        "print(\"MinHasher created with 200 hash functions\")\n",
        "print(f\"Each signature will have 200 values\")"
      ],
      "metadata": {
        "id": "minhash_implementation"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.3 Computing and Comparing MinHash Signatures"
      ],
      "metadata": {
        "id": "minhash_compare_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute signatures for all documents\n",
        "signatures = {}\n",
        "for row in shingle_data:\n",
        "    shingle_set = set(row.all_shingles)\n",
        "    signatures[row.id] = minhasher.get_signature(shingle_set)\n",
        "\n",
        "# Compare MinHash estimates with actual Jaccard\n",
        "print(\"MinHash Similarity Estimation vs Actual Jaccard:\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "for i, j in pairs_to_compare:\n",
        "    doc_i = next(row for row in shingle_data if row.id == i)\n",
        "    doc_j = next(row for row in shingle_data if row.id == j)\n",
        "\n",
        "    # Actual Jaccard\n",
        "    set_i = set(doc_i.all_shingles)\n",
        "    set_j = set(doc_j.all_shingles)\n",
        "    actual_jaccard = jaccard_similarity(set_i, set_j)\n",
        "\n",
        "    # MinHash estimate\n",
        "    estimated_jaccard = minhasher.estimate_similarity(\n",
        "        signatures[i], signatures[j]\n",
        "    )\n",
        "\n",
        "    error = abs(actual_jaccard - estimated_jaccard)\n",
        "\n",
        "    print(f\"\\nDocs {i} vs {j}:\")\n",
        "    print(f\"  Actual Jaccard:    {actual_jaccard:.4f}\")\n",
        "    print(f\"  MinHash Estimate:  {estimated_jaccard:.4f}\")\n",
        "    print(f\"  Absolute Error:    {error:.4f}\")"
      ],
      "metadata": {
        "id": "minhash_compare"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.4 Understanding Signature Accuracy\n",
        "\n",
        "**Error Bounds:**\n",
        "The standard error of MinHash estimation is approximately:\n",
        "```\n",
        "SE ≈ √(J(1-J)/k)\n",
        "```\n",
        "where J is the true Jaccard and k is the number of hash functions.\n",
        "\n",
        "Let's verify this empirically:"
      ],
      "metadata": {
        "id": "minhash_accuracy_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Analyze accuracy vs number of hash functions\n",
        "doc_pair = (0, 1)  # A similar pair\n",
        "doc_i_shingles = set(shingle_data[0].all_shingles)\n",
        "doc_j_shingles = set(shingle_data[1].all_shingles)\n",
        "actual_jaccard = jaccard_similarity(doc_i_shingles, doc_j_shingles)\n",
        "\n",
        "print(f\"Analyzing MinHash accuracy for docs {doc_pair}\")\n",
        "print(f\"Actual Jaccard similarity: {actual_jaccard:.4f}\")\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(f\"{'Num Hashes':>12} | {'Estimated':>10} | {'Error':>8} | {'Theoretical SE':>14}\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "for num_hashes in [10, 25, 50, 100, 200, 500]:\n",
        "    mh = MinHasher(num_hashes=num_hashes)\n",
        "    sig_i = mh.get_signature(doc_i_shingles)\n",
        "    sig_j = mh.get_signature(doc_j_shingles)\n",
        "    estimated = mh.estimate_similarity(sig_i, sig_j)\n",
        "    error = abs(estimated - actual_jaccard)\n",
        "    theoretical_se = np.sqrt(actual_jaccard * (1 - actual_jaccard) / num_hashes)\n",
        "\n",
        "    print(f\"{num_hashes:>12} | {estimated:>10.4f} | {error:>8.4f} | {theoretical_se:>14.4f}\")"
      ],
      "metadata": {
        "id": "minhash_accuracy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Key Observations:**\n",
        "1. More hash functions → More accurate estimates\n",
        "2. Error decreases roughly as 1/√k\n",
        "3. Trade-off: accuracy vs. computation and storage"
      ],
      "metadata": {
        "id": "minhash_observations"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.5 Practical Exercise: MinHash at Scale\n",
        "\n",
        "Let's implement MinHash using Spark UDFs for distributed computation."
      ],
      "metadata": {
        "id": "minhash_spark_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Broadcast MinHasher parameters for use in UDF\n",
        "num_hashes = 128\n",
        "max_hash = 2**32 - 1\n",
        "np.random.seed(42)\n",
        "a_coeffs = sc.broadcast(np.random.randint(1, max_hash, size=num_hashes))\n",
        "b_coeffs = sc.broadcast(np.random.randint(0, max_hash, size=num_hashes))\n",
        "c_prime = 4294967311\n",
        "\n",
        "@udf(ArrayType(IntegerType()))\n",
        "def compute_minhash_signature(shingles):\n",
        "    \"\"\"Compute MinHash signature for a list of shingles.\"\"\"\n",
        "    if not shingles:\n",
        "        return [0] * num_hashes\n",
        "\n",
        "    import hashlib\n",
        "    import numpy as np\n",
        "\n",
        "    a = a_coeffs.value\n",
        "    b = b_coeffs.value\n",
        "\n",
        "    signature = np.full(num_hashes, np.iinfo(np.int64).max)\n",
        "\n",
        "    for shingle in shingles:\n",
        "        shingle_hash = int(hashlib.md5(shingle.encode()).hexdigest(), 16) % max_hash\n",
        "        hash_values = (a * shingle_hash + b) % c_prime\n",
        "        signature = np.minimum(signature, hash_values)\n",
        "\n",
        "    return signature.astype(int).tolist()\n",
        "\n",
        "# Compute signatures for all documents\n",
        "df_with_signatures = df_combined_shingles.withColumn(\n",
        "    \"minhash_signature\",\n",
        "    compute_minhash_signature(col(\"all_shingles\"))\n",
        ")\n",
        "\n",
        "print(\"Documents with MinHash signatures:\")\n",
        "df_with_signatures.select(\"id\", \"text\", \"minhash_signature\").show(5, truncate=50)"
      ],
      "metadata": {
        "id": "minhash_spark"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to estimate similarity between two signatures\n",
        "@udf(FloatType())\n",
        "def estimate_jaccard_from_signatures(sig1, sig2):\n",
        "    \"\"\"Estimate Jaccard similarity from two MinHash signatures.\"\"\"\n",
        "    if not sig1 or not sig2:\n",
        "        return 0.0\n",
        "    matches = sum(1 for s1, s2 in zip(sig1, sig2) if s1 == s2)\n",
        "    return float(matches) / len(sig1)\n",
        "\n",
        "# Compare a specific pair\n",
        "sig_0 = df_with_signatures.filter(col(\"id\") == 0).select(\"minhash_signature\").first()[0]\n",
        "sig_1 = df_with_signatures.filter(col(\"id\") == 1).select(\"minhash_signature\").first()[0]\n",
        "\n",
        "similarity = sum(1 for s1, s2 in zip(sig_0, sig_1) if s1 == s2) / len(sig_0)\n",
        "print(f\"MinHash similarity between docs 0 and 1: {similarity:.4f}\")"
      ],
      "metadata": {
        "id": "minhash_similarity"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "# Part 4: LSH with Spark's MinHashLSH\n",
        "---\n",
        "\n",
        "## 4.1 Understanding Spark ML's MinHashLSH\n",
        "\n",
        "Spark provides a built-in `MinHashLSH` transformer that:\n",
        "1. Computes MinHash signatures\n",
        "2. Organizes signatures into bands for LSH\n",
        "3. Provides methods for similarity joins and nearest neighbor queries\n",
        "\n",
        "**Banding Technique:**\n",
        "- Divide signature into b bands of r rows each\n",
        "- Two documents are candidates if they match in at least one band\n",
        "- P(candidate) = 1 - (1 - s^r)^b, where s is Jaccard similarity"
      ],
      "metadata": {
        "id": "part4_intro"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize the S-curve for different b and r values\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def prob_candidate(s, b, r):\n",
        "    \"\"\"Probability that two documents with Jaccard s become candidates.\"\"\"\n",
        "    return 1 - (1 - s**r)**b\n",
        "\n",
        "s_values = np.linspace(0, 1, 100)\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "\n",
        "# Different configurations (b bands, r rows)\n",
        "configs = [\n",
        "    (20, 5, 'b=20, r=5'),\n",
        "    (50, 2, 'b=50, r=2'),\n",
        "    (10, 10, 'b=10, r=10'),\n",
        "    (25, 4, 'b=25, r=4'),\n",
        "]\n",
        "\n",
        "for b, r, label in configs:\n",
        "    p_values = [prob_candidate(s, b, r) for s in s_values]\n",
        "    ax.plot(s_values, p_values, label=label, linewidth=2)\n",
        "\n",
        "ax.set_xlabel('Jaccard Similarity', fontsize=12)\n",
        "ax.set_ylabel('Probability of Becoming Candidate', fontsize=12)\n",
        "ax.set_title('LSH S-Curves for Different Band Configurations', fontsize=14)\n",
        "ax.legend()\n",
        "ax.grid(True, alpha=0.3)\n",
        "ax.axhline(y=0.5, color='gray', linestyle='--', alpha=0.5)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Calculate threshold (50% probability)\n",
        "for b, r, label in configs:\n",
        "    threshold = (1/b)**(1/r)\n",
        "    print(f\"{label}: Threshold ≈ {threshold:.3f}\")"
      ],
      "metadata": {
        "id": "scurve_visualization"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.2 Preparing Data for Spark's MinHashLSH\n",
        "\n",
        "Spark's MinHashLSH requires input as sparse vectors. We'll use CountVectorizer to convert shingles to vectors."
      ],
      "metadata": {
        "id": "prepare_minhash_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare data for Spark's MinHashLSH\n",
        "# First, we need to convert shingles to a sparse vector representation\n",
        "\n",
        "# Use CountVectorizer to create vocabulary and sparse vectors\n",
        "cv = CountVectorizer(inputCol=\"all_shingles\", outputCol=\"features\", binary=True)\n",
        "cv_model = cv.fit(df_combined_shingles)\n",
        "\n",
        "df_vectorized = cv_model.transform(df_combined_shingles)\n",
        "\n",
        "print(f\"Vocabulary size: {len(cv_model.vocabulary)}\")\n",
        "print(\"\\nVectorized documents:\")\n",
        "df_vectorized.select(\"id\", \"features\").show(5, truncate=80)"
      ],
      "metadata": {
        "id": "prepare_minhash"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.3 Using MinHashLSH for Approximate Similarity Join"
      ],
      "metadata": {
        "id": "minhash_lsh_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create and fit MinHashLSH model\n",
        "minhash_lsh = MinHashLSH(\n",
        "    inputCol=\"features\",\n",
        "    outputCol=\"hashes\",\n",
        "    numHashTables=5  # Number of hash tables (bands)\n",
        ")\n",
        "\n",
        "lsh_model = minhash_lsh.fit(df_vectorized)\n",
        "\n",
        "print(\"MinHashLSH model fitted successfully!\")\n",
        "print(f\"Number of hash tables: 5\")"
      ],
      "metadata": {
        "id": "fit_minhash_lsh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Approximate similarity join: find all pairs with Jaccard >= threshold\n",
        "threshold = 0.3\n",
        "\n",
        "similar_pairs = lsh_model.approxSimilarityJoin(\n",
        "    df_vectorized, df_vectorized,\n",
        "    threshold=threshold,\n",
        "    distCol=\"distance\"\n",
        ")\n",
        "\n",
        "# Filter out self-joins and duplicate pairs\n",
        "similar_pairs_filtered = similar_pairs.filter(\n",
        "    col(\"datasetA.id\") < col(\"datasetB.id\")\n",
        ").select(\n",
        "    col(\"datasetA.id\").alias(\"id_a\"),\n",
        "    col(\"datasetB.id\").alias(\"id_b\"),\n",
        "    col(\"datasetA.text\").alias(\"text_a\"),\n",
        "    col(\"datasetB.text\").alias(\"text_b\"),\n",
        "    (1 - col(\"distance\")).alias(\"jaccard_similarity\")  # Convert distance to similarity\n",
        ").orderBy(col(\"jaccard_similarity\").desc())\n",
        "\n",
        "print(f\"Similar document pairs (Jaccard >= {threshold}):\")\n",
        "similar_pairs_filtered.show(truncate=50)"
      ],
      "metadata": {
        "id": "similarity_join"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.4 Approximate Nearest Neighbors\n",
        "\n",
        "Find the k most similar documents to a query document."
      ],
      "metadata": {
        "id": "ann_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Find nearest neighbors for a specific document\n",
        "query_id = 0\n",
        "k = 5\n",
        "\n",
        "# Get the feature vector for query document\n",
        "query_vector = df_vectorized.filter(col(\"id\") == query_id).select(\"features\").first()[0]\n",
        "\n",
        "# Find approximate nearest neighbors\n",
        "neighbors = lsh_model.approxNearestNeighbors(\n",
        "    df_vectorized,\n",
        "    query_vector,\n",
        "    numNearestNeighbors=k+1  # +1 to account for the query itself\n",
        ")\n",
        "\n",
        "print(f\"Query document (id={query_id}):\")\n",
        "print(f\"  '{documents[query_id][1]}'\")\n",
        "print(f\"\\nTop {k} most similar documents:\")\n",
        "neighbors.filter(col(\"id\") != query_id).select(\n",
        "    \"id\", \"text\", (1 - col(\"distCol\")).alias(\"jaccard_similarity\")\n",
        ").show(truncate=60)"
      ],
      "metadata": {
        "id": "ann_query"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.5 Practical Exercise: Near-Duplicate Detection\n",
        "\n",
        "Build a complete near-duplicate detection system."
      ],
      "metadata": {
        "id": "duplicate_detection_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Extended document corpus with some near-duplicates\n",
        "extended_docs = [\n",
        "    (100, \"Apache Spark is a fast and general-purpose cluster computing system.\"),\n",
        "    (101, \"Spark is a fast general purpose cluster computing system for big data.\"),  # Near-dup of 100\n",
        "    (102, \"Apache Spark provides fast cluster computing for big data processing.\"),  # Near-dup of 100\n",
        "    (103, \"The Python programming language is widely used in data science.\"),\n",
        "    (104, \"Python is a widely used programming language for data science applications.\"),  # Near-dup of 103\n",
        "    (105, \"Machine learning algorithms can learn patterns from historical data.\"),\n",
        "    (106, \"ML algorithms learn patterns from historical data automatically.\"),  # Near-dup of 105\n",
        "    (107, \"The weather forecast predicts rain for the upcoming weekend.\"),\n",
        "    (108, \"Database systems store and manage large amounts of structured data.\"),\n",
        "    (109, \"Cloud computing provides on-demand access to computing resources.\"),\n",
        "]\n",
        "\n",
        "df_extended = spark.createDataFrame(extended_docs, [\"id\", \"text\"])\n",
        "\n",
        "# Full pipeline: preprocess -> shingle -> vectorize -> LSH\n",
        "def build_duplicate_detection_pipeline(df, num_hash_tables=10):\n",
        "    \"\"\"Build complete near-duplicate detection pipeline.\"\"\"\n",
        "\n",
        "    # 1. Preprocess\n",
        "    df_clean = df.withColumn(\n",
        "        \"text_clean\",\n",
        "        lower(regexp_replace(col(\"text\"), r\"[^a-zA-Z\\s]\", \"\"))\n",
        "    )\n",
        "\n",
        "    # 2. Tokenize\n",
        "    tokenizer = Tokenizer(inputCol=\"text_clean\", outputCol=\"words\")\n",
        "    df_tokenized = tokenizer.transform(df_clean)\n",
        "\n",
        "    # 3. Generate multiple n-gram sizes\n",
        "    for n in [2, 3]:\n",
        "        ngram = NGram(n=n, inputCol=\"words\", outputCol=f\"ngrams_{n}\")\n",
        "        df_tokenized = ngram.transform(df_tokenized)\n",
        "\n",
        "    # 4. Combine unigrams + bigrams + trigrams\n",
        "    df_shingles = df_tokenized.withColumn(\n",
        "        \"shingles\",\n",
        "        array_distinct(concat(col(\"words\"), col(\"ngrams_2\"), col(\"ngrams_3\")))\n",
        "    )\n",
        "\n",
        "    # 5. Vectorize\n",
        "    cv = CountVectorizer(inputCol=\"shingles\", outputCol=\"features\", binary=True)\n",
        "    cv_model = cv.fit(df_shingles)\n",
        "    df_vectorized = cv_model.transform(df_shingles)\n",
        "\n",
        "    # 6. MinHashLSH\n",
        "    minhash = MinHashLSH(\n",
        "        inputCol=\"features\",\n",
        "        outputCol=\"hashes\",\n",
        "        numHashTables=num_hash_tables\n",
        "    )\n",
        "    lsh_model = minhash.fit(df_vectorized)\n",
        "\n",
        "    return df_vectorized, lsh_model, cv_model\n",
        "\n",
        "# Build pipeline\n",
        "df_vec, dup_lsh_model, dup_cv_model = build_duplicate_detection_pipeline(df_extended)\n",
        "\n",
        "# Find near-duplicates (Jaccard distance <= 0.7, i.e. similarity >= 0.3)\n",
        "duplicates = dup_lsh_model.approxSimilarityJoin(\n",
        "    df_vec, df_vec,\n",
        "    threshold=0.7,\n",
        "    distCol=\"distance\"\n",
        ").filter(\n",
        "    col(\"datasetA.id\") < col(\"datasetB.id\")\n",
        ").select(\n",
        "    col(\"datasetA.id\").alias(\"doc1_id\"),\n",
        "    col(\"datasetB.id\").alias(\"doc2_id\"),\n",
        "    col(\"datasetA.text\").alias(\"doc1_text\"),\n",
        "    col(\"datasetB.text\").alias(\"doc2_text\"),\n",
        "    (1 - col(\"distance\")).alias(\"similarity\")\n",
        ").orderBy(col(\"similarity\").desc())\n",
        "\n",
        "print(\"Detected Near-Duplicates:\")\n",
        "print(\"=\" * 80)\n",
        "for row in duplicates.collect():\n",
        "    print(f\"\\nSimilarity: {row.similarity:.4f}\")\n",
        "    print(f\"  Doc {row.doc1_id}: {row.doc1_text}\")\n",
        "    print(f\"  Doc {row.doc2_id}: {row.doc2_text}\")"
      ],
      "metadata": {
        "id": "duplicate_detection"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "# Part 5: Weighted MinHash\n",
        "---\n",
        "\n",
        "## 5.1 Limitations of Classic MinHash\n",
        "\n",
        "**Classic MinHash treats shingles as binary:**\n",
        "- Present (1) or Absent (0)\n",
        "- Ignores term frequency information\n",
        "- All shingles weighted equally\n",
        "\n",
        "**The Problem:**\n",
        "```\n",
        "Doc A: \"the the the cat sat\"\n",
        "Doc B: \"the cat sat mat\"\n",
        "\n",
        "Classic MinHash: {the, cat, sat} vs {the, cat, sat, mat}\n",
        "Jaccard = 3/4 = 0.75\n",
        "\n",
        "But Doc A has 3x \"the\" - shouldn't this matter?\n",
        "```\n",
        "\n",
        "**Weighted MinHash Solution:**\n",
        "- Preserves term frequency (TF) weights\n",
        "- Better captures document characteristics\n",
        "- Approximates weighted Jaccard similarity"
      ],
      "metadata": {
        "id": "part5_intro"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5.2 Weighted Jaccard Similarity\n",
        "\n",
        "For weighted sets A and B with weights w_A(x) and w_B(x):\n",
        "\n",
        "```\n",
        "Weighted_Jaccard(A, B) = Σ min(w_A(x), w_B(x)) / Σ max(w_A(x), w_B(x))\n",
        "```\n",
        "\n",
        "This generalizes standard Jaccard (where weights are 0 or 1)."
      ],
      "metadata": {
        "id": "weighted_jaccard_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Weighted Jaccard similarity implementation\n",
        "from collections import Counter\n",
        "\n",
        "def weighted_jaccard(doc1_tokens: List[str], doc2_tokens: List[str]) -> float:\n",
        "    \"\"\"\n",
        "    Calculate weighted Jaccard similarity based on term frequencies.\n",
        "    \"\"\"\n",
        "    # Count term frequencies\n",
        "    counter1 = Counter(doc1_tokens)\n",
        "    counter2 = Counter(doc2_tokens)\n",
        "\n",
        "    # Get all unique terms\n",
        "    all_terms = set(counter1.keys()) | set(counter2.keys())\n",
        "\n",
        "    # Calculate weighted intersection and union\n",
        "    intersection = sum(min(counter1.get(t, 0), counter2.get(t, 0)) for t in all_terms)\n",
        "    union = sum(max(counter1.get(t, 0), counter2.get(t, 0)) for t in all_terms)\n",
        "\n",
        "    return intersection / union if union > 0 else 0.0\n",
        "\n",
        "# Example\n",
        "doc_a = \"the the the cat sat on the mat\".split()\n",
        "doc_b = \"the cat sat on a mat\".split()\n",
        "\n",
        "# Binary (standard) Jaccard\n",
        "binary_jaccard = len(set(doc_a) & set(doc_b)) / len(set(doc_a) | set(doc_b))\n",
        "\n",
        "# Weighted Jaccard\n",
        "w_jaccard = weighted_jaccard(doc_a, doc_b)\n",
        "\n",
        "print(\"Document A:\", ' '.join(doc_a))\n",
        "print(\"  Term frequencies:\", dict(Counter(doc_a)))\n",
        "print(\"\\nDocument B:\", ' '.join(doc_b))\n",
        "print(\"  Term frequencies:\", dict(Counter(doc_b)))\n",
        "print(f\"\\nBinary Jaccard:   {binary_jaccard:.4f}\")\n",
        "print(f\"Weighted Jaccard: {w_jaccard:.4f}\")"
      ],
      "metadata": {
        "id": "weighted_jaccard"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5.3 Weighted MinHash Algorithm\n",
        "\n",
        "**ICWS (Improved Consistent Weighted Sampling):**\n",
        "\n",
        "For each element x with weight w(x):\n",
        "1. Generate uniform random values r, ln_c, β for each hash function\n",
        "2. Compute: t = floor(ln(w(x))/r + β)\n",
        "3. Compute: y = exp(r * (t - β))\n",
        "4. Compute: a = ln_c - y*r\n",
        "5. MinHash signature = argmin over all elements\n",
        "\n",
        "This ensures P[h(A) = h(B)] = WeightedJaccard(A, B)"
      ],
      "metadata": {
        "id": "weighted_minhash_algo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class WeightedMinHasher:\n",
        "    \"\"\"\n",
        "    Weighted MinHash using ICWS (Improved Consistent Weighted Sampling).\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, num_hashes: int = 128, seed: int = 42):\n",
        "        self.num_hashes = num_hashes\n",
        "        np.random.seed(seed)\n",
        "\n",
        "        # Pre-generate random values for hash functions\n",
        "        self.r = np.random.uniform(0, 1, size=num_hashes)\n",
        "        self.ln_c = np.random.uniform(0, 1, size=num_hashes)\n",
        "        self.beta = np.random.uniform(0, 1, size=num_hashes)\n",
        "\n",
        "    def _hash_term(self, term: str) -> int:\n",
        "      \"\"\"Hash term to integer.\"\"\"\n",
        "      # Constrain to int64 range to avoid overflow\n",
        "      return int(hashlib.md5(term.encode()).hexdigest(), 16) % (2**63 - 1)\n",
        "\n",
        "    def get_signature(self, term_weights: dict) -> Tuple[np.ndarray, np.ndarray]:\n",
        "        \"\"\"\n",
        "        Compute weighted MinHash signature.\n",
        "\n",
        "        Args:\n",
        "            term_weights: Dictionary of {term: weight}\n",
        "\n",
        "        Returns:\n",
        "            Tuple of (k_values, y_values) forming the signature\n",
        "        \"\"\"\n",
        "        if not term_weights:\n",
        "            return np.zeros(self.num_hashes), np.zeros(self.num_hashes)\n",
        "\n",
        "        # Initialize with infinity\n",
        "        min_a = np.full(self.num_hashes, np.inf)\n",
        "        k_star = np.zeros(self.num_hashes, dtype=np.int64)\n",
        "        y_star = np.zeros(self.num_hashes)\n",
        "\n",
        "        for term, weight in term_weights.items():\n",
        "            if weight <= 0:\n",
        "                continue\n",
        "\n",
        "            term_hash = self._hash_term(term)\n",
        "            np.random.seed(term_hash % (2**31))\n",
        "\n",
        "            # Generate term-specific random values\n",
        "            r_term = np.random.uniform(0, 1, size=self.num_hashes)\n",
        "            c_term = np.random.uniform(0, 1, size=self.num_hashes)\n",
        "            beta_term = np.random.uniform(0, 1, size=self.num_hashes)\n",
        "\n",
        "            # ICWS computation\n",
        "            ln_weight = np.log(weight)\n",
        "            t = np.floor(ln_weight / r_term + beta_term)\n",
        "            y = np.exp(r_term * (t - beta_term))\n",
        "            a = np.log(c_term) - y * r_term\n",
        "\n",
        "            # Update minimum\n",
        "            mask = a < min_a\n",
        "            min_a = np.where(mask, a, min_a)\n",
        "            k_star = np.where(mask, term_hash, k_star)\n",
        "            y_star = np.where(mask, y, y_star)\n",
        "\n",
        "        return k_star, y_star\n",
        "\n",
        "    def estimate_similarity(self, sig1: Tuple, sig2: Tuple) -> float:\n",
        "        \"\"\"\n",
        "        Estimate weighted Jaccard from signatures.\n",
        "        \"\"\"\n",
        "        k1, y1 = sig1\n",
        "        k2, y2 = sig2\n",
        "        matches = np.sum((k1 == k2) & (y1 == y2))\n",
        "        return matches / self.num_hashes\n",
        "\n",
        "# Demonstrate weighted MinHash\n",
        "wmh = WeightedMinHasher(num_hashes=256)\n",
        "\n",
        "# Create weighted document representations\n",
        "weights_a = dict(Counter(doc_a))\n",
        "weights_b = dict(Counter(doc_b))\n",
        "\n",
        "sig_a = wmh.get_signature(weights_a)\n",
        "sig_b = wmh.get_signature(weights_b)\n",
        "\n",
        "estimated_weighted = wmh.estimate_similarity(sig_a, sig_b)\n",
        "actual_weighted = weighted_jaccard(doc_a, doc_b)\n",
        "\n",
        "print(\"Weighted MinHash Results:\")\n",
        "print(f\"  Actual Weighted Jaccard:    {actual_weighted:.4f}\")\n",
        "print(f\"  Estimated (Weighted MH):    {estimated_weighted:.4f}\")\n",
        "print(f\"  Binary Jaccard (for ref):   {binary_jaccard:.4f}\")"
      ],
      "metadata": {
        "id": "weighted_minhash_impl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5.4 Weighted MinHash in Spark\n",
        "\n",
        "Let's implement weighted MinHash for distributed processing."
      ],
      "metadata": {
        "id": "weighted_minhash_spark_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Simplified Weighted MinHash using TF-weighted vectors with Spark's MinHashLSH\n",
        "# Note: Spark's built-in MinHashLSH can work with weighted vectors\n",
        "\n",
        "# Create documents with term frequency vectors (not binary)\n",
        "df_for_weighted = df_docs.withColumn(\n",
        "    \"text_clean\",\n",
        "    lower(regexp_replace(col(\"text\"), r\"[^a-zA-Z\\s]\", \"\"))\n",
        ")\n",
        "\n",
        "tokenizer = Tokenizer(inputCol=\"text_clean\", outputCol=\"words\")\n",
        "df_tokenized = tokenizer.transform(df_for_weighted)\n",
        "\n",
        "# Use HashingTF instead of CountVectorizer for TF weights\n",
        "# This preserves term frequencies instead of binary presence\n",
        "hashing_tf = HashingTF(\n",
        "    inputCol=\"words\",\n",
        "    outputCol=\"tf_features\",\n",
        "    numFeatures=1000\n",
        ")\n",
        "\n",
        "df_tf = hashing_tf.transform(df_tokenized)\n",
        "\n",
        "# Also create binary version for comparison\n",
        "cv = CountVectorizer(\n",
        "    inputCol=\"words\",\n",
        "    outputCol=\"binary_features\",\n",
        "    binary=True\n",
        ")\n",
        "cv_model = cv.fit(df_tokenized)\n",
        "df_both = cv_model.transform(df_tf)\n",
        "\n",
        "print(\"Prepared documents with both TF and binary features\")\n",
        "df_both.select(\"id\", \"words\").show(3, truncate=60)"
      ],
      "metadata": {
        "id": "weighted_minhash_spark"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compare binary vs TF-weighted MinHashLSH\n",
        "\n",
        "# Binary MinHashLSH\n",
        "binary_minhash = MinHashLSH(\n",
        "    inputCol=\"binary_features\",\n",
        "    outputCol=\"binary_hashes\",\n",
        "    numHashTables=10\n",
        ")\n",
        "binary_model = binary_minhash.fit(df_both)\n",
        "\n",
        "# Compare similarities for a specific pair\n",
        "doc_pair = (0, 1)  # Similar documents\n",
        "\n",
        "# Use approxSimilarityJoin to compute distance between documents\n",
        "df_pair_a = df_both.filter(col(\"id\") == 0).select(col(\"id\").alias(\"id_a\"), \"binary_features\")\n",
        "df_pair_b = df_both.filter(col(\"id\") == 1).select(col(\"id\").alias(\"id_b\"), \"binary_features\")\n",
        "\n",
        "# Compute distance using similarity join with high threshold\n",
        "result = binary_model.approxSimilarityJoin(\n",
        "    df_pair_a, df_pair_b,\n",
        "    threshold=1.0,  # Accept any distance up to 1.0\n",
        "    distCol=\"distance\"\n",
        ")\n",
        "\n",
        "if result.count() > 0:\n",
        "    binary_dist = result.first().distance\n",
        "    print(f\"Comparison for documents {doc_pair}:\")\n",
        "    print(f\"  Binary MinHash Jaccard Distance: {binary_dist:.4f}\")\n",
        "    print(f\"  Binary MinHash Jaccard Similarity: {1-binary_dist:.4f}\")\n",
        "else:\n",
        "    print(\"No match found within threshold\")\n",
        "\n",
        "print(\"\\nNote: For true weighted MinHash in production, consider using\")\n",
        "print(\"specialized libraries like datasketch which provide ICWS implementation.\")"
      ],
      "metadata": {
        "id": "weighted_minhash_compare"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5.5 When to Use Weighted MinHash\n",
        "\n",
        "**Use Classic (Binary) MinHash when:**\n",
        "- Near-duplicate detection (exact or near-exact copies)\n",
        "- Set similarity is the main concern\n",
        "- Speed is critical and TF doesn't add value\n",
        "\n",
        "**Use Weighted MinHash when:**\n",
        "- Document similarity (topical similarity)\n",
        "- Term frequency carries meaning\n",
        "- Comparing documents of different lengths\n",
        "- Higher precision is needed\n",
        "\n",
        "**Best Practice:**\n",
        "Many production systems use both:\n",
        "1. Binary MinHash for fast initial filtering\n",
        "2. Weighted features for final ranking"
      ],
      "metadata": {
        "id": "weighted_minhash_when"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "# Part 6: Production Pipeline - MinHash + Transformer Reranking\n",
        "---\n",
        "\n",
        "## 6.1 Two-Stage Retrieval Architecture\n",
        "\n",
        "This is an extremely common pattern in production similarity search systems:\n",
        "\n",
        "```\n",
        "┌────────────────────────────────────────────────────────────┐\n",
        "│                  PRODUCTION PIPELINE                        │\n",
        "├────────────────────────────────────────────────────────────┤\n",
        "│                                                            │\n",
        "│   Query → [Shingle] → [MinHash+LSH] → Candidates (fast)   │\n",
        "│                            │                               │\n",
        "│                            ▼                               │\n",
        "│            [Sentence Transformer] → Reranked (precise)    │\n",
        "│                            │                               │\n",
        "│                            ▼                               │\n",
        "│                      Final Results                         │\n",
        "│                                                            │\n",
        "└────────────────────────────────────────────────────────────┘\n",
        "```\n",
        "\n",
        "**Why Two Stages?**\n",
        "\n",
        "| Stage | Method | Properties |\n",
        "|-------|--------|------------|\n",
        "| Stage 1 | MinHash + LSH | Cheap, scalable, high recall |\n",
        "| Stage 2 | Transformers | Expensive, precise, semantic |\n",
        "\n",
        "**Key Insight:** We avoid computing expensive embeddings for every document pair."
      ],
      "metadata": {
        "id": "part6_intro"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6.2 Setting Up the Sentence Transformer\n",
        "\n",
        "We'll use `all-MiniLM-L6-v2`, a small, fast model that works well on CPUs."
      ],
      "metadata": {
        "id": "transformer_setup_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "import torch\n",
        "\n",
        "# Load a small, CPU-friendly sentence transformer\n",
        "# all-MiniLM-L6-v2: 22M parameters, 384 dimensions, fast inference\n",
        "print(\"Loading sentence transformer model...\")\n",
        "model_name = 'all-MiniLM-L6-v2'\n",
        "sentence_model = SentenceTransformer(model_name)\n",
        "\n",
        "# Check device\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(f\"Model: {model_name}\")\n",
        "print(f\"Device: {device}\")\n",
        "print(f\"Embedding dimension: {sentence_model.get_sentence_embedding_dimension()}\")"
      ],
      "metadata": {
        "id": "transformer_setup"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test the transformer\n",
        "test_sentences = [\n",
        "    \"Machine learning is a subset of artificial intelligence.\",\n",
        "    \"AI and ML allow computers to learn from data.\",\n",
        "    \"The weather is sunny today.\"\n",
        "]\n",
        "\n",
        "embeddings = sentence_model.encode(test_sentences)\n",
        "\n",
        "# Compute cosine similarities\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "similarities = cosine_similarity(embeddings)\n",
        "\n",
        "print(\"Sentence Transformer Cosine Similarities:\")\n",
        "print(\"\\nSentences:\")\n",
        "for i, s in enumerate(test_sentences):\n",
        "    print(f\"  [{i}] {s}\")\n",
        "\n",
        "print(\"\\nSimilarity Matrix:\")\n",
        "print(\"      [0]    [1]    [2]\")\n",
        "for i, row in enumerate(similarities):\n",
        "    print(f\"[{i}]  {row[0]:.3f}  {row[1]:.3f}  {row[2]:.3f}\")"
      ],
      "metadata": {
        "id": "transformer_test"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6.3 Stage 1: MinHash + LSH for Candidate Retrieval\n",
        "\n",
        "First stage: fast, approximate retrieval using MinHash."
      ],
      "metadata": {
        "id": "stage1_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a larger corpus for the production pipeline demo\n",
        "corpus = [\n",
        "    # ML/AI cluster\n",
        "    (0, \"Machine learning algorithms can identify patterns in large datasets.\"),\n",
        "    (1, \"Deep learning neural networks achieve state-of-the-art results in computer vision.\"),\n",
        "    (2, \"Artificial intelligence systems are transforming healthcare diagnostics.\"),\n",
        "    (3, \"Supervised learning requires labeled training data for model development.\"),\n",
        "    (4, \"Natural language processing enables machines to understand human text.\"),\n",
        "\n",
        "    # Big Data cluster\n",
        "    (5, \"Apache Spark provides distributed computing for big data analytics.\"),\n",
        "    (6, \"Hadoop ecosystem tools process massive amounts of unstructured data.\"),\n",
        "    (7, \"Data lakes store raw data in its native format for later processing.\"),\n",
        "    (8, \"Stream processing systems handle real-time data ingestion and analysis.\"),\n",
        "\n",
        "    # Weather cluster\n",
        "    (9, \"Today's weather forecast predicts sunny skies with mild temperatures.\"),\n",
        "    (10, \"The meteorological service issued a severe thunderstorm warning.\"),\n",
        "    (11, \"Climate change affects global weather patterns and ecosystems.\"),\n",
        "\n",
        "    # Food cluster\n",
        "    (12, \"The restaurant serves authentic Italian pasta and wood-fired pizza.\"),\n",
        "    (13, \"Fresh ingredients are essential for preparing delicious meals.\"),\n",
        "    (14, \"The chef specializes in Mediterranean cuisine with local produce.\"),\n",
        "\n",
        "    # More ML/AI\n",
        "    (15, \"Transformer models revolutionized natural language understanding tasks.\"),\n",
        "    (16, \"Reinforcement learning agents learn through trial and error interactions.\"),\n",
        "    (17, \"Feature engineering is crucial for traditional machine learning models.\"),\n",
        "]\n",
        "\n",
        "df_corpus = spark.createDataFrame(corpus, [\"id\", \"text\"])\n",
        "\n",
        "# Build MinHash LSH index\n",
        "def build_minhash_index(df, num_hash_tables=10):\n",
        "    \"\"\"Build MinHash LSH index with unigrams + n-grams.\"\"\"\n",
        "    df = df.withColumn(\n",
        "        \"text_clean\",\n",
        "        lower(regexp_replace(col(\"text\"), r\"[^a-zA-Z\\s]\", \"\"))\n",
        "    )\n",
        "    tokenizer = Tokenizer(inputCol=\"text_clean\", outputCol=\"words\")\n",
        "    df = tokenizer.transform(df)\n",
        "\n",
        "    for n in [2, 3, 4]:\n",
        "        ngram = NGram(n=n, inputCol=\"words\", outputCol=f\"ngrams_{n}\")\n",
        "        df = ngram.transform(df)\n",
        "\n",
        "    # Include unigrams (words) for better overlap with queries\n",
        "    df = df.withColumn(\n",
        "        \"all_shingles\",\n",
        "        array_distinct(concat(col(\"words\"), col(\"ngrams_2\"), col(\"ngrams_3\"), col(\"ngrams_4\")))\n",
        "    )\n",
        "\n",
        "    cv = CountVectorizer(inputCol=\"all_shingles\", outputCol=\"features\", binary=True)\n",
        "    cv_model = cv.fit(df)\n",
        "    df = cv_model.transform(df)\n",
        "\n",
        "    minhash = MinHashLSH(inputCol=\"features\", outputCol=\"hashes\", numHashTables=num_hash_tables)\n",
        "    lsh_model = minhash.fit(df)\n",
        "    return df, lsh_model, cv_model\n",
        "\n",
        "df_indexed, lsh_model, cv_model = build_minhash_index(df_corpus)\n",
        "print(f\"Indexed {df_indexed.count()} documents with MinHash LSH\")"
      ],
      "metadata": {
        "id": "stage1_build"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_lsh_candidates(query_text, df_indexed, lsh_model, cv_model, num_candidates=10):\n",
        "    \"\"\"Get candidates using unigrams + n-grams.\"\"\"\n",
        "    import re\n",
        "    text_clean = re.sub(r\"[^a-zA-Z\\s]\", \"\", query_text.lower())\n",
        "    words = text_clean.split()\n",
        "\n",
        "    # Include unigrams alongside n-grams\n",
        "    shingles = list(words)\n",
        "    for n in [2, 3, 4]:\n",
        "        shingles.extend([' '.join(words[i:i+n]) for i in range(len(words)-n+1)])\n",
        "    shingles = list(set(shingles))\n",
        "\n",
        "    if not shingles:\n",
        "        return []\n",
        "\n",
        "    query_df = spark.createDataFrame([(shingles,)], [\"all_shingles\"])\n",
        "    query_vec = cv_model.transform(query_df).first().features\n",
        "\n",
        "    if query_vec.numNonzeros() == 0:\n",
        "        print(\"Warning: Query has no matching terms in vocabulary\")\n",
        "        return []\n",
        "\n",
        "    candidates = lsh_model.approxNearestNeighbors(\n",
        "        df_indexed, query_vec, numNearestNeighbors=num_candidates\n",
        "    )\n",
        "    return candidates.select(\n",
        "        \"id\", \"text\", col(\"distCol\").alias(\"jaccard_distance\")\n",
        "    ).collect()"
      ],
      "metadata": {
        "id": "stage1_query"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6.4 Stage 2: Transformer Reranking with Spark\n",
        "\n",
        "Now we use the sentence transformer to rerank the candidates based on semantic similarity."
      ],
      "metadata": {
        "id": "stage2_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "def rerank_with_transformer(query_text, candidates, sentence_model):\n",
        "    if not candidates:\n",
        "        return []\n",
        "    candidate_texts = [c.text for c in candidates]\n",
        "    query_embedding = sentence_model.encode([query_text])\n",
        "    candidate_embeddings = sentence_model.encode(candidate_texts)\n",
        "    similarities = cosine_similarity(query_embedding, candidate_embeddings)[0]\n",
        "    reranked = []\n",
        "    for c, sim in zip(candidates, similarities):\n",
        "        reranked.append({\n",
        "            'id': c.id, 'text': c.text,\n",
        "            'jaccard_distance': c.jaccard_distance,\n",
        "            'cosine_similarity': float(sim)\n",
        "        })\n",
        "    reranked.sort(key=lambda x: x['cosine_similarity'], reverse=True)\n",
        "    return reranked\n",
        "\n",
        "\n",
        "query = \"How do neural networks learn patterns from data?\"\n",
        "candidates = get_lsh_candidates(query, df_indexed, lsh_model, cv_model, num_candidates=8)\n",
        "reranked = rerank_with_transformer(query, candidates, sentence_model)\n",
        "\n",
        "print(f\"Query: '{query}'\")\n",
        "print(f\"\\nStage 2 - Transformer Reranked Results:\")\n",
        "print(\"=\" * 70)\n",
        "for i, r in enumerate(reranked):\n",
        "    print(f\"\\n  Rank {i+1}: ID {r['id']}\")\n",
        "    print(f\"    Text: {r['text'][:60]}...\")\n",
        "    print(f\"    Jaccard Distance: {r['jaccard_distance']:.4f}\")\n",
        "    print(f\"    Cosine Similarity: {r['cosine_similarity']:.4f}\")"
      ],
      "metadata": {
        "id": "stage2_rerank"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6.5 Distributed Transformer Inference with Spark\n",
        "\n",
        "For large-scale reranking, we can parallelize transformer inference across Spark workers."
      ],
      "metadata": {
        "id": "distributed_transformer_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Approach: Use Spark's mapPartitions for batch processing\n",
        "\n",
        "# Pre-compute embeddings for all corpus documents\n",
        "# This is done once and stored\n",
        "\n",
        "def compute_corpus_embeddings(df_corpus, sentence_model):\n",
        "    \"\"\"\n",
        "    Pre-compute embeddings for all documents in corpus.\n",
        "    Returns a dictionary mapping id -> embedding\n",
        "    \"\"\"\n",
        "    docs = df_corpus.select(\"id\", \"text\").collect()\n",
        "    texts = [doc.text for doc in docs]\n",
        "    ids = [doc.id for doc in docs]\n",
        "\n",
        "    # Batch encode all documents\n",
        "    embeddings = sentence_model.encode(texts, show_progress_bar=True)\n",
        "\n",
        "    # Create lookup dictionary\n",
        "    embedding_dict = {id_: emb for id_, emb in zip(ids, embeddings)}\n",
        "\n",
        "    return embedding_dict\n",
        "\n",
        "print(\"Computing corpus embeddings...\")\n",
        "corpus_embeddings = compute_corpus_embeddings(df_corpus, sentence_model)\n",
        "print(f\"Computed embeddings for {len(corpus_embeddings)} documents\")"
      ],
      "metadata": {
        "id": "corpus_embeddings"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a Spark UDF for cosine similarity using pre-computed embeddings\n",
        "# Broadcast the embeddings to all workers\n",
        "\n",
        "embeddings_broadcast = sc.broadcast(corpus_embeddings)\n",
        "\n",
        "@udf(FloatType())\n",
        "def compute_cosine_similarity(doc_id, query_embedding_list):\n",
        "    \"\"\"\n",
        "    Compute cosine similarity between query and document.\n",
        "    Uses pre-computed document embeddings.\n",
        "    \"\"\"\n",
        "    import numpy as np\n",
        "\n",
        "    doc_embedding = embeddings_broadcast.value.get(doc_id)\n",
        "    if doc_embedding is None:\n",
        "        return 0.0\n",
        "\n",
        "    query_embedding = np.array(query_embedding_list)\n",
        "\n",
        "    # Cosine similarity\n",
        "    dot_product = np.dot(query_embedding, doc_embedding)\n",
        "    norm_q = np.linalg.norm(query_embedding)\n",
        "    norm_d = np.linalg.norm(doc_embedding)\n",
        "\n",
        "    if norm_q == 0 or norm_d == 0:\n",
        "        return 0.0\n",
        "\n",
        "    return float(dot_product / (norm_q * norm_d))\n",
        "\n",
        "print(\"Cosine similarity UDF created with broadcast embeddings\")"
      ],
      "metadata": {
        "id": "cosine_udf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Complete production pipeline as a single function\n",
        "\n",
        "def production_similarity_search(query_text: str,\n",
        "                                 df_indexed,\n",
        "                                 lsh_model,\n",
        "                                 cv_model,\n",
        "                                 sentence_model,\n",
        "                                 num_candidates: int = 10,\n",
        "                                 top_k: int = 5):\n",
        "    \"\"\"\n",
        "    Production two-stage similarity search pipeline.\n",
        "\n",
        "    Stage 1: MinHash + LSH for fast candidate retrieval\n",
        "    Stage 2: Sentence transformer for precise reranking\n",
        "\n",
        "    Args:\n",
        "        query_text: Query document text\n",
        "        df_indexed: Indexed corpus DataFrame\n",
        "        lsh_model: Fitted MinHashLSH model\n",
        "        cv_model: Fitted CountVectorizer model\n",
        "        sentence_model: Sentence transformer model\n",
        "        num_candidates: Number of candidates from Stage 1\n",
        "        top_k: Final number of results to return\n",
        "\n",
        "    Returns:\n",
        "        List of top_k most similar documents\n",
        "    \"\"\"\n",
        "    import time\n",
        "\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"Query: '{query_text}'\")\n",
        "    print(f\"{'='*60}\")\n",
        "\n",
        "    # Stage 1: MinHash LSH\n",
        "    start_time = time.time()\n",
        "    candidates = get_lsh_candidates(\n",
        "        query_text, df_indexed, lsh_model, cv_model, num_candidates\n",
        "    )\n",
        "    stage1_time = time.time() - start_time\n",
        "    print(f\"\\n[Stage 1] MinHash LSH: {len(candidates)} candidates in {stage1_time:.3f}s\")\n",
        "\n",
        "    if not candidates:\n",
        "        print(\"No candidates found!\")\n",
        "        return []\n",
        "\n",
        "    # Stage 2: Transformer reranking\n",
        "    start_time = time.time()\n",
        "    reranked = rerank_with_transformer(query_text, candidates, sentence_model)\n",
        "    stage2_time = time.time() - start_time\n",
        "    print(f\"[Stage 2] Transformer Rerank: {len(reranked)} docs in {stage2_time:.3f}s\")\n",
        "\n",
        "    # Return top_k results\n",
        "    results = reranked[:top_k]\n",
        "\n",
        "    print(f\"\\nTop {top_k} Results:\")\n",
        "    print(\"-\" * 60)\n",
        "    for i, r in enumerate(results):\n",
        "        print(f\"  {i+1}. [ID {r['id']}] Cosine: {r['cosine_similarity']:.4f}\")\n",
        "        print(f\"     {r['text'][:70]}...\")\n",
        "\n",
        "    return results\n",
        "\n",
        "# Test the complete pipeline\n",
        "queries = [\n",
        "    \"How do neural networks learn from data?\",\n",
        "    \"What tools are used for processing big data?\",\n",
        "    \"Is it going to rain tomorrow?\"\n",
        "]\n",
        "\n",
        "for q in queries:\n",
        "    results = production_similarity_search(\n",
        "        q, df_indexed, lsh_model, cv_model, sentence_model,\n",
        "        num_candidates=8, top_k=3\n",
        "    )"
      ],
      "metadata": {
        "id": "complete_pipeline"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6.6 Performance Analysis\n",
        "\n",
        "Let's analyze the computational savings of the two-stage approach."
      ],
      "metadata": {
        "id": "performance_analysis_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "# Simulate larger corpus for performance comparison\n",
        "def benchmark_approaches(corpus_size: int, query: str):\n",
        "    \"\"\"\n",
        "    Compare brute force vs two-stage approach.\n",
        "    \"\"\"\n",
        "    print(f\"\\nBenchmark with {corpus_size} documents\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    # Simulate corpus texts\n",
        "    corpus_texts = [doc.text for doc in df_corpus.collect()]\n",
        "    simulated_corpus = corpus_texts * (corpus_size // len(corpus_texts) + 1)\n",
        "    simulated_corpus = simulated_corpus[:corpus_size]\n",
        "\n",
        "    # Brute force: encode all documents and compute similarities\n",
        "    start = time.time()\n",
        "    all_embeddings = sentence_model.encode(simulated_corpus[:100])  # Limit for demo\n",
        "    query_emb = sentence_model.encode([query])\n",
        "    similarities = cosine_similarity(query_emb, all_embeddings)\n",
        "    brute_force_time = time.time() - start\n",
        "    brute_force_estimated = brute_force_time * (corpus_size / 100)\n",
        "\n",
        "    # Two-stage: MinHash (simulated) + limited transformer calls\n",
        "    num_candidates = min(20, corpus_size)\n",
        "    start = time.time()\n",
        "    # MinHash retrieval is O(1) per query after index built\n",
        "    minhash_time = 0.001 * corpus_size / 1000  # Simulated: ~1ms per 1000 docs\n",
        "    # Transformer only on candidates\n",
        "    candidate_texts = simulated_corpus[:num_candidates]\n",
        "    candidate_embs = sentence_model.encode(candidate_texts)\n",
        "    similarities = cosine_similarity(query_emb, candidate_embs)\n",
        "    two_stage_time = time.time() - start + minhash_time\n",
        "\n",
        "    print(f\"  Brute Force (estimated): {brute_force_estimated:.3f}s\")\n",
        "    print(f\"  Two-Stage:               {two_stage_time:.3f}s\")\n",
        "    print(f\"  Speedup:                 {brute_force_estimated/two_stage_time:.1f}x\")\n",
        "\n",
        "    return brute_force_estimated, two_stage_time\n",
        "\n",
        "# Run benchmarks\n",
        "query = \"How do machine learning algorithms work?\"\n",
        "results = []\n",
        "\n",
        "for size in [100, 1000, 10000, 100000]:\n",
        "    bf, ts = benchmark_approaches(size, query)\n",
        "    results.append((size, bf, ts))\n",
        "\n",
        "print(\"\\n\" + \"=\" * 50)\n",
        "print(\"Summary: Time Complexity\")\n",
        "print(\"=\" * 50)\n",
        "print(f\"{'Corpus Size':<15} {'Brute Force':<15} {'Two-Stage':<15} {'Speedup':<10}\")\n",
        "print(\"-\" * 55)\n",
        "for size, bf, ts in results:\n",
        "    print(f\"{size:<15} {bf:<15.3f} {ts:<15.3f} {bf/ts:<10.1f}x\")"
      ],
      "metadata": {
        "id": "performance_analysis"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6.7 Key Takeaways\n",
        "\n",
        "**Production Pipeline Benefits:**\n",
        "1. **Scalability**: MinHash + LSH scales to billions of documents\n",
        "2. **Precision**: Transformer reranking provides semantic understanding\n",
        "3. **Efficiency**: Only expensive operations on small candidate set\n",
        "4. **Flexibility**: Easy to tune recall/precision trade-off\n",
        "\n",
        "**When to Use This Pattern:**\n",
        "- Large document collections (>10,000 documents)\n",
        "- Real-time similarity search requirements\n",
        "- Semantic similarity is important (not just lexical)\n",
        "- Resource constraints on transformer inference"
      ],
      "metadata": {
        "id": "takeaways"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "# Part 7: Final Challenge\n",
        "---\n",
        "\n",
        "## End-to-End Document Similarity System\n",
        "\n",
        "Build a complete document similarity system that incorporates everything you've learned.\n",
        "\n",
        "**Dataset:** News articles from different categories\n",
        "\n",
        "**Tasks:**\n",
        "1. Implement shingling with configurable n-gram sizes (2-4)\n",
        "2. Build a MinHash LSH index with tunable parameters\n",
        "3. Implement both binary and weighted similarity measures\n",
        "4. Create a two-stage retrieval pipeline with transformer reranking\n",
        "5. Evaluate the system's recall and precision\n",
        "6. Compare performance with brute-force approach"
      ],
      "metadata": {
        "id": "final_challenge"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Challenge Dataset: News articles\n",
        "news_articles = [\n",
        "    # Technology\n",
        "    (0, \"tech\", \"Apple announces new iPhone with revolutionary AI features and improved camera system.\"),\n",
        "    (1, \"tech\", \"Google introduces advanced language model that surpasses human performance on benchmarks.\"),\n",
        "    (2, \"tech\", \"Microsoft integrates AI assistants into Office productivity suite for enterprise users.\"),\n",
        "    (3, \"tech\", \"New smartphone from Apple features cutting-edge AI capabilities and enhanced photography.\"),\n",
        "\n",
        "    # Sports\n",
        "    (4, \"sports\", \"Manchester United secures victory in Champions League quarter-final match.\"),\n",
        "    (5, \"sports\", \"Tennis star wins Wimbledon after intense five-set final against rival.\"),\n",
        "    (6, \"sports\", \"Champions League sees dramatic finish as Manchester United advances to semi-finals.\"),\n",
        "    (7, \"sports\", \"Olympic committee announces new sports for upcoming summer games.\"),\n",
        "\n",
        "    # Finance\n",
        "    (8, \"finance\", \"Federal Reserve raises interest rates amid inflation concerns in economy.\"),\n",
        "    (9, \"finance\", \"Stock market reaches all-time high as tech sector leads gains.\"),\n",
        "    (10, \"finance\", \"Central bank increases rates to combat rising inflation pressures.\"),\n",
        "    (11, \"finance\", \"Cryptocurrency market sees volatility as regulations tighten globally.\"),\n",
        "\n",
        "    # Science\n",
        "    (12, \"science\", \"NASA discovers water on Mars surface, raising possibilities for life.\"),\n",
        "    (13, \"science\", \"Breakthrough in quantum computing achieved by research team.\"),\n",
        "    (14, \"science\", \"Mars exploration reveals signs of ancient water presence on planet.\"),\n",
        "    (15, \"science\", \"Climate scientists warn of accelerating ice sheet melting rates.\"),\n",
        "\n",
        "    # Health\n",
        "    (16, \"health\", \"New vaccine shows promising results in clinical trials for disease prevention.\"),\n",
        "    (17, \"health\", \"Medical researchers develop innovative treatment for rare genetic disorder.\"),\n",
        "    (18, \"health\", \"Clinical trials demonstrate vaccine effectiveness against emerging variants.\"),\n",
        "    (19, \"health\", \"WHO releases updated guidelines for pandemic preparedness strategies.\"),\n",
        "]\n",
        "\n",
        "df_news = spark.createDataFrame(news_articles, [\"id\", \"category\", \"text\"])\n",
        "\n",
        "print(\"Challenge Dataset: News Articles\")\n",
        "df_news.groupBy(\"category\").count().show()"
      ],
      "metadata": {
        "id": "challenge_data"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Your code here: Build complete document similarity system\n",
        "#\n",
        "# Suggested structure:\n",
        "# 1. Create a DocumentSimilaritySystem class that encapsulates:\n",
        "#    - Shingling\n",
        "#    - MinHash LSH indexing\n",
        "#    - Transformer embeddings\n",
        "#    - Two-stage search\n",
        "#\n",
        "# 2. Implement methods:\n",
        "#    - index_documents(df)\n",
        "#    - search(query, top_k)\n",
        "#    - evaluate(test_queries, ground_truth)\n",
        "#\n",
        "# 3. Test with various queries and analyze:\n",
        "#    - Recall: Did we find all similar documents?\n",
        "#    - Precision: Are the returned documents actually similar?\n",
        "#    - Latency: How fast is the search?\n",
        "\n",
        "class DocumentSimilaritySystem:\n",
        "    def __init__(self, num_hash_tables=10, shingle_sizes=[2, 3, 4]):\n",
        "        # Your implementation here\n",
        "        pass\n",
        "\n",
        "    def index_documents(self, df):\n",
        "        # Your implementation here\n",
        "        pass\n",
        "\n",
        "    def search(self, query, num_candidates=10, top_k=5):\n",
        "        # Your implementation here\n",
        "        pass\n",
        "\n",
        "    def evaluate(self, test_queries, expected_categories):\n",
        "        # Your implementation here\n",
        "        pass\n",
        "\n",
        "# Test queries\n",
        "test_queries = [\n",
        "    (\"Latest AI developments in mobile phones\", \"tech\"),\n",
        "    (\"Football championship results\", \"sports\"),\n",
        "    (\"Economic policy and monetary decisions\", \"finance\"),\n",
        "    (\"Space exploration discoveries\", \"science\"),\n",
        "    (\"Medical treatment breakthroughs\", \"health\"),\n",
        "]\n",
        "\n",
        "# Your evaluation code here\n"
      ],
      "metadata": {
        "id": "challenge_code"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "# Cleanup\n",
        "---\n",
        "\n",
        "Always remember to stop your Spark session when you're done."
      ],
      "metadata": {
        "id": "cleanup"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Stop the Spark session\n",
        "spark.stop()\n",
        "print(\"Spark session stopped.\")"
      ],
      "metadata": {
        "id": "stop_spark"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "# Summary\n",
        "---\n",
        "\n",
        "In this lab, you learned:\n",
        "\n",
        "1. **Locality-Sensitive Hashing Fundamentals**\n",
        "   - The challenge of similarity search at scale\n",
        "   - How LSH reduces O(N²) to approximately O(N)\n",
        "   - The trade-off between precision and recall\n",
        "\n",
        "2. **Document Shingling**\n",
        "   - Character vs word shingles\n",
        "   - Choosing appropriate shingle sizes\n",
        "   - Implementing shingling in Spark\n",
        "\n",
        "3. **MinHash Algorithm**\n",
        "   - Jaccard similarity and its limitations\n",
        "   - MinHash signature generation\n",
        "   - Estimating similarity from signatures\n",
        "   - Error bounds and accuracy analysis\n",
        "\n",
        "4. **Spark's MinHashLSH**\n",
        "   - Banding technique and S-curves\n",
        "   - Approximate similarity joins\n",
        "   - Approximate nearest neighbors\n",
        "   - Near-duplicate detection\n",
        "\n",
        "5. **Weighted MinHash**\n",
        "   - Limitations of binary MinHash\n",
        "   - Weighted Jaccard similarity\n",
        "   - ICWS algorithm\n",
        "   - When to use weighted vs binary\n",
        "\n",
        "6. **Production Pipeline**\n",
        "   - Two-stage retrieval architecture\n",
        "   - MinHash + LSH for candidate retrieval\n",
        "   - Sentence transformer reranking\n",
        "   - Performance benefits and trade-offs"
      ],
      "metadata": {
        "id": "summary"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "# Additional Resources\n",
        "---\n",
        "\n",
        "**Core Concepts:**\n",
        "- [Mining of Massive Datasets - Chapter 3: Finding Similar Items](http://www.mmds.org/)\n",
        "\n",
        "\n",
        "**Spark Documentation:**\n",
        "- [Spark ML - MinHashLSH](https://spark.apache.org/docs/latest/ml-features.html#minhash-for-jaccard-distance)\n",
        "- [Spark ML - Feature Extractors](https://spark.apache.org/docs/latest/ml-features.html)\n",
        "\n",
        "**Libraries:**\n",
        "- [datasketch - Probabilistic Data Structures](https://github.com/ekzhu/datasketch)\n",
        "- [sentence-transformers](https://www.sbert.net/)\n",
        "\n",
        "**Papers:**\n",
        "- Broder, A. Z. (1997). On the resemblance and containment of documents\n",
        "- Ioffe, S. (2010). Improved Consistent Sampling, Weighted Minhash and L1 Sketching\n",
        "- Indyk, P., & Motwani, R. (1998). Approximate nearest neighbors: towards removing the curse of dimensionality"
      ],
      "metadata": {
        "id": "resources"
      }
    }
  ]
}