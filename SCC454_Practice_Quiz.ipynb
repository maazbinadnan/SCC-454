{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![Lancaster University](https://www.lancaster.ac.uk/media/lancaster-university/content-assets/images/fst/logos/SCC-Logo.svg)\n",
        "\n",
        "# SCC.454: Large Scale Platforms for AI and Data Analysis\n",
        "## Practice Quiz\n",
        "\n",
        "**Duration:** 1 Hour  \n",
        "**Total Marks:** 100  \n",
        "\n",
        "---\n",
        "\n",
        "### Instructions\n",
        "\n",
        "1. This is a **practice quiz** to help you prepare for the actual assessment.\n",
        "2. **Write your code** in the designated code cells below each question.\n",
        "3. **All questions are independent** — if you cannot answer one question, move on to the next.\n",
        "4. **Run your code** to verify correctness.\n",
        "\n",
        "### API Documentation\n",
        "\n",
        "- **NumPy:** [https://numpy.org/doc/stable/reference/](https://numpy.org/doc/stable/reference/)\n",
        "- **Pandas:** [https://pandas.pydata.org/docs/reference/](https://pandas.pydata.org/docs/reference/)\n",
        "- **Scikit-learn:** [https://scikit-learn.org/stable/api/](https://scikit-learn.org/stable/api/)\n",
        "- **PySpark SQL Functions:** [https://spark.apache.org/docs/3.5.0/api/python/reference/pyspark.sql/functions.html](https://spark.apache.org/docs/3.5.0/api/python/reference/pyspark.sql/functions.html)\n",
        "- **PySpark DataFrame:** [https://spark.apache.org/docs/3.5.0/api/python/reference/pyspark.sql/dataframe.html](https://spark.apache.org/docs/3.5.0/api/python/reference/pyspark.sql/dataframe.html)\n",
        "- **PySpark ML Feature:** [https://spark.apache.org/docs/3.5.0/api/python/reference/pyspark.ml.html](https://spark.apache.org/docs/3.5.0/api/python/reference/pyspark.ml.html)\n",
        "\n",
        "---\n",
        "\n",
        "| Section | Topic | Marks |\n",
        "|---------|-------|-------|\n",
        "| **A** | Python, NumPy, Pandas & Scikit-learn | **30** |\n",
        "| **B** | Apache Spark (RDDs, DataFrames, SQL) | **35** |\n",
        "| **C** | Data Preprocessing & Similarity Search | **35** |\n",
        "| | **Total** | **100** |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "# Section A: Python, NumPy, Pandas & Scikit-learn (30 marks)\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Question 1 — NumPy Array Operations [10 marks]\n",
        "\n",
        "Consider the following 3×4 matrix **M**:\n",
        "\n",
        "```\n",
        "      Col0  Col1  Col2  Col3\n",
        "Row0    4    12     7     3\n",
        "Row1    8     5    14    10\n",
        "Row2    6    11     2     9\n",
        "```\n",
        "\n",
        "**API Reference:** [numpy.org/doc/stable/reference](https://numpy.org/doc/stable/reference/)\n",
        "\n",
        "**(a)** Create the matrix `M` as a NumPy array exactly as shown above. Print its shape and data type. **[2 marks]**\n",
        "\n",
        "**(b)** Extract and print: (i) the second row, (ii) the third column, and (iii) the element at row 1, column 2. **[2 marks]**\n",
        "\n",
        "**(c)** Compute and print the **sum** of each row and the **mean** of each column. **[3 marks]**\n",
        "\n",
        "**(d)** Using boolean indexing, find and print all elements in `M` that are **greater than 7**. Then, create a copy of `M` and replace all elements greater than 7 with `0`. Print the modified matrix. **[3 marks]**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(3, 4)\n",
            "int64\n",
            "[ 8  5 14 10]\n",
            "[ 7 14  2]\n",
            "14\n",
            "[26 37 28]\n",
            "[6.         9.33333333 7.66666667 7.33333333]\n",
            "[12  8 14 10 11  9]\n",
            "[[4 0 7 3]\n",
            " [0 5 0 0]\n",
            " [6 0 2 0]]\n"
          ]
        }
      ],
      "source": [
        "# Q1 — Write your code here\n",
        "import numpy as np\n",
        "\n",
        "# (a) Create matrix M, print shape and dtype\n",
        "matrix = np.array([[4,12,7,3],[8,5,14,10],[6,11,2,9]])\n",
        "print(matrix.shape)\n",
        "print(matrix.dtype)\n",
        "\n",
        "# (b) Extract second row, third column, element at [1,2]\n",
        "print(matrix[1])\n",
        "print(matrix[:,2])\n",
        "print(matrix[1,2])\n",
        "\n",
        "# (c) Sum of each row, mean of each column\n",
        "print(np.sum(matrix,axis=1))\n",
        "print(np.mean(matrix,axis =0))\n",
        "\n",
        "# (d) Elements > 7, then replace > 7 with 0\n",
        "elements = matrix[matrix > 7]\n",
        "print(elements)\n",
        "\n",
        "matrixcopy = matrix.copy()\n",
        "matrixcopy[matrixcopy > 7] =0\n",
        "print(matrixcopy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Question 2 — Pandas Data Manipulation [10 marks]\n",
        "\n",
        "A shop has recorded the following sales data:\n",
        "\n",
        "```\n",
        "order_id  product      category     price   quantity  date\n",
        "1001      Laptop       Electronics  999.99  1         2025-03-01\n",
        "1002      Mouse        Electronics  29.99   3         2025-03-01\n",
        "1003      Notebook     Stationery   5.99    10        2025-03-02\n",
        "1004      Keyboard     Electronics  79.99   2         2025-03-02\n",
        "1005      Pen Set      Stationery   12.99   5         2025-03-03\n",
        "1006      Monitor      Electronics  349.99  1         2025-03-03\n",
        "1007      Stapler      Stationery   8.99    NaN       2025-03-04\n",
        "1008      Headphones   Electronics  149.99  2         2025-03-04\n",
        "```\n",
        "\n",
        "**API Reference:** [pandas.pydata.org/docs/reference](https://pandas.pydata.org/docs/reference/)\n",
        "\n",
        "**(a)** Create this DataFrame in pandas exactly as shown (use `np.nan` for the missing value). Print the DataFrame and its info. **[2 marks]**\n",
        "\n",
        "**(b)** Fill the missing `quantity` value with the **median** quantity of all products. Print the updated DataFrame. **[2 marks]**\n",
        "\n",
        "**(c)** Add a new column called `total` computed as `price × quantity`. Then filter and display only rows where `total > 100`. **[3 marks]**\n",
        "\n",
        "**(d)** Using `groupby`, calculate the **total revenue** (sum of `total`) and the **number of orders** per category. Sort by total revenue descending. **[3 marks]**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   order_id     product     category   price  quantity        date\n",
            "0      1001      Laptop  Electronics  999.99       1.0  2025-03-01\n",
            "1      1002       Mouse  Electronics   29.99       3.0  2025-03-01\n",
            "2      1003    Notebook   Stationery    5.99      10.0  2025-03-02\n",
            "3      1004    Keyboard  Electronics   79.99       2.0  2025-03-02\n",
            "4      1005     Pen Set   Stationery   12.99       5.0  2025-03-03\n",
            "5      1006     Monitor  Electronics  349.99       1.0  2025-03-03\n",
            "6      1007     Stapler   Stationery    8.99       NaN  2025-03-04\n",
            "7      1008  Headphones  Electronics  149.99       2.0  2025-03-04\n",
            "   order_id     product     category   price  quantity        date\n",
            "0      1001      Laptop  Electronics  999.99       1.0  2025-03-01\n",
            "1      1002       Mouse  Electronics   29.99       3.0  2025-03-01\n",
            "2      1003    Notebook   Stationery    5.99      10.0  2025-03-02\n",
            "3      1004    Keyboard  Electronics   79.99       2.0  2025-03-02\n",
            "4      1005     Pen Set   Stationery   12.99       5.0  2025-03-03\n",
            "5      1006     Monitor  Electronics  349.99       1.0  2025-03-03\n",
            "6      1007     Stapler   Stationery    8.99       2.0  2025-03-04\n",
            "7      1008  Headphones  Electronics  149.99       2.0  2025-03-04\n",
            "   order_id     product     category   price  quantity        date   total\n",
            "0      1001      Laptop  Electronics  999.99       1.0  2025-03-01  999.99\n",
            "3      1004    Keyboard  Electronics   79.99       2.0  2025-03-02  159.98\n",
            "5      1006     Monitor  Electronics  349.99       1.0  2025-03-03  349.99\n",
            "7      1008  Headphones  Electronics  149.99       2.0  2025-03-04  299.98\n",
            "             total_revenue  order_count\n",
            "category                               \n",
            "Electronics        1899.91         5021\n",
            "Stationery          142.83         3015\n"
          ]
        }
      ],
      "source": [
        "# Q2 — Write your code here\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# (a) Create DataFrame and print info\n",
        "test_df = pd.DataFrame({\n",
        "    'order_id': [1001, 1002, 1003, 1004, 1005, 1006, 1007, 1008],\n",
        "    'product': ['Laptop', 'Mouse', 'Notebook', 'Keyboard', 'Pen Set', 'Monitor', 'Stapler', 'Headphones'],\n",
        "    'category': ['Electronics', 'Electronics', 'Stationery', 'Electronics', 'Stationery', 'Electronics', 'Stationery', 'Electronics'],\n",
        "    'price': [999.99, 29.99, 5.99, 79.99, 12.99, 349.99, 8.99, 149.99],\n",
        "    'quantity': [1, 3, 10, 2, 5, 1, np.nan, 2],\n",
        "    'date': ['2025-03-01', '2025-03-01', '2025-03-02', '2025-03-02', '2025-03-03', '2025-03-03', '2025-03-04', '2025-03-04']\n",
        "}\n",
        ")\n",
        "print(test_df)\n",
        "\n",
        "# (b) Fill missing quantity with median\n",
        "quantity_median = test_df['quantity'].median()\n",
        "test_filled_df = test_df.copy()\n",
        "test_filled_df['quantity']=test_df['quantity'].fillna(quantity_median)\n",
        "print(test_filled_df)\n",
        "# (c) Add total column, filter where total > 100\n",
        "test_df_total = test_filled_df.copy()\n",
        "test_df_total['total'] =test_df_total['price'] * test_df_total['quantity']\n",
        "print(test_df_total[test_df_total['total']>100])\n",
        "\n",
        "# (d) Groupby category: total revenue and order count\n",
        "print(test_df_total.groupby('category').agg(\n",
        "    total_revenue = ('total',\"sum\"),\n",
        "    order_count = ('order_id',\"sum\"),\n",
        "))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Question 3 — Scikit-learn Classification [10 marks]\n",
        "\n",
        "You will use the Iris dataset for this question. **Run the setup cell first.**\n",
        "\n",
        "**API Reference:** [scikit-learn.org/stable/api](https://scikit-learn.org/stable/api/)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# === RUN THIS CELL FIRST ===\n",
        "from sklearn.datasets import load_iris\n",
        "import pandas as pd\n",
        "\n",
        "iris = load_iris()\n",
        "df_iris = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
        "df_iris['target'] = iris.target\n",
        "\n",
        "print(f\"Dataset shape: {df_iris.shape}\")\n",
        "print(f\"Target classes: {list(iris.target_names)}\")\n",
        "print(f\"Class distribution:\\n{df_iris['target'].value_counts().sort_index()}\")\n",
        "df_iris.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**(a)** Split the data into training (80%) and testing (20%) sets with `random_state=42` and stratified sampling. Print the shapes. **[2 marks]**\n",
        "\n",
        "**(b)** Apply `StandardScaler` to the features. Fit on training data only, then transform both sets. **[2 marks]**\n",
        "\n",
        "**(c)** Train a **K-Nearest Neighbours** classifier with `n_neighbors=3`. Print the accuracy on the test set. **[3 marks]**\n",
        "\n",
        "**(d)** Print the **confusion matrix** and the **classification report** for the KNN model. **[3 marks]**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Q3 — Write your code here\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "\n",
        "# (a) Train-test split with stratification\n",
        "\n",
        "\n",
        "# (b) StandardScaler - fit on train, transform both\n",
        "\n",
        "\n",
        "# (c) Train KNN with n_neighbors=3, print accuracy\n",
        "\n",
        "\n",
        "# (d) Confusion matrix and classification report\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "# Section B: Apache Spark — RDDs, DataFrames & SQL (35 marks)\n",
        "---\n",
        "\n",
        "### ⚙️ Spark Setup\n",
        "\n",
        "Run the two setup cells below before attempting the Spark questions.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# === SETUP CELL 1: Install PySpark and Java ===\n",
        "!pip install pyspark==3.5.0 -q\n",
        "!apt-get install openjdk-11-jdk-headless -qq > /dev/null 2>&1\n",
        "\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\"\n",
        "print(\"PySpark and Java installed successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Spark version: 4.0.2\n"
          ]
        }
      ],
      "source": [
        "# === SETUP CELL 2: Create SparkSession ===\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"SCC454-Practice\") \\\n",
        "    .config(\"spark.driver.memory\", \"4g\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "sc = spark.sparkContext\n",
        "print(f\"Spark version: {spark.version}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Question 4 — RDD Transformations and Actions [12 marks]\n",
        "\n",
        "**Run the setup cell first**, then answer the questions below.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "RDD created with 5 sentences\n"
          ]
        }
      ],
      "source": [
        "# === RUN THIS CELL FIRST ===\n",
        "sentences = [\n",
        "    \"Apache Spark is fast\",\n",
        "    \"Spark is used for big data\",\n",
        "    \"Big data processing is important\",\n",
        "    \"Spark and Hadoop are popular\",\n",
        "    \"Data science uses Spark\",\n",
        "]\n",
        "\n",
        "sentences_rdd = sc.parallelize(sentences, 2)\n",
        "print(f\"RDD created with {sentences_rdd.count()} sentences\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**(a)** Using `flatMap`, split each sentence into words (lowercase) and collect all words as a list. Print the total number of words. **[3 marks]**\n",
        "\n",
        "**(b)** Using `map` and `reduceByKey`, count the occurrences of each word. Print all word counts. **[3 marks]**\n",
        "\n",
        "**(c)** Find the **top 5 most frequent words** using `sortBy`. Print them with their counts. **[3 marks]**\n",
        "\n",
        "**(d)** Using `filter`, find all words that contain the letter `'a'`. Print the count and the list of words. **[3 marks]**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "24\n",
            "[('spark', 4), ('is', 3), ('data', 3), ('big', 2), ('apache', 1)]\n",
            "[('apache', 1), ('fast', 1), ('important', 1), ('and', 1), ('hadoop', 1), ('are', 1), ('popular', 1), ('spark', 4), ('data', 3)]\n"
          ]
        }
      ],
      "source": [
        "from operator import add\n",
        "# Q4 — Write your code here\n",
        "\n",
        "# (a) Split sentences into words, count total words\n",
        "sentences_flat_mapped = sentences_rdd.flatMap(lambda s:s.lower().split())\n",
        "sentences_flat_mapped.collect()\n",
        "print(sentences_flat_mapped.count())\n",
        "\n",
        "# (b) Word count using map and reduceByKey\n",
        "sentences_mapped = sentences_flat_mapped.map(lambda word:(word,1))\n",
        "sentences = sentences_mapped.reduceByKey(add)\n",
        "# print(sentences.collect())\n",
        "# (c) Top 5 most frequent words\n",
        "sorted_S = sentences.sortBy(lambda x: -x[1]).take(5)\n",
        "print(sorted_S)\n",
        "# (d) Words containing letter 'a'\n",
        "sentences_with_a = sentences.filter(lambda x: 'a' in x[0])\n",
        "print(sentences_with_a.collect())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Question 5 — Spark DataFrame Operations [12 marks]\n",
        "\n",
        "Consider the following student grades data:\n",
        "\n",
        "```\n",
        "student_id  name      subject     score   semester\n",
        "S001        Alice     Maths       85      Fall\n",
        "S001        Alice     Physics     78      Fall\n",
        "S002        Bob       Maths       92      Fall\n",
        "S002        Bob       Physics     88      Fall\n",
        "S003        Carol     Maths       76      Fall\n",
        "S003        Carol     Physics     82      Fall\n",
        "S001        Alice     Maths       88      Spring\n",
        "S001        Alice     Physics     84      Spring\n",
        "S002        Bob       Maths       90      Spring\n",
        "S002        Bob       Physics     91      Spring\n",
        "```\n",
        "\n",
        "**Run the setup cell first.**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Grades DataFrame created:\n",
            "+----------+-----+-------+-----+--------+\n",
            "|student_id| name|subject|score|semester|\n",
            "+----------+-----+-------+-----+--------+\n",
            "|      S001|Alice|  Maths|   85|    Fall|\n",
            "|      S001|Alice|Physics|   78|    Fall|\n",
            "|      S002|  Bob|  Maths|   92|    Fall|\n",
            "|      S002|  Bob|Physics|   88|    Fall|\n",
            "|      S003|Carol|  Maths|   76|    Fall|\n",
            "|      S003|Carol|Physics|   82|    Fall|\n",
            "|      S001|Alice|  Maths|   88|  Spring|\n",
            "|      S001|Alice|Physics|   84|  Spring|\n",
            "|      S002|  Bob|  Maths|   90|  Spring|\n",
            "|      S002|  Bob|Physics|   91|  Spring|\n",
            "+----------+-----+-------+-----+--------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# === RUN THIS CELL FIRST ===\n",
        "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
        "\n",
        "grades_data = [\n",
        "    (\"S001\", \"Alice\", \"Maths\", 85, \"Fall\"),\n",
        "    (\"S001\", \"Alice\", \"Physics\", 78, \"Fall\"),\n",
        "    (\"S002\", \"Bob\", \"Maths\", 92, \"Fall\"),\n",
        "    (\"S002\", \"Bob\", \"Physics\", 88, \"Fall\"),\n",
        "    (\"S003\", \"Carol\", \"Maths\", 76, \"Fall\"),\n",
        "    (\"S003\", \"Carol\", \"Physics\", 82, \"Fall\"),\n",
        "    (\"S001\", \"Alice\", \"Maths\", 88, \"Spring\"),\n",
        "    (\"S001\", \"Alice\", \"Physics\", 84, \"Spring\"),\n",
        "    (\"S002\", \"Bob\", \"Maths\", 90, \"Spring\"),\n",
        "    (\"S002\", \"Bob\", \"Physics\", 91, \"Spring\"),\n",
        "]\n",
        "\n",
        "grades_schema = StructType([\n",
        "    StructField(\"student_id\", StringType(), True),\n",
        "    StructField(\"name\", StringType(), True),\n",
        "    StructField(\"subject\", StringType(), True),\n",
        "    StructField(\"score\", IntegerType(), True),\n",
        "    StructField(\"semester\", StringType(), True),\n",
        "])\n",
        "\n",
        "grades_df = spark.createDataFrame(grades_data, grades_schema)\n",
        "print(\"Grades DataFrame created:\")\n",
        "grades_df.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**(a)** Select only `name`, `subject`, and `score` columns. Then filter to show only rows where `score >= 85`. **[3 marks]**\n",
        "\n",
        "**(b)** Add a new column `grade` based on score: `'A'` if score >= 90, `'B'` if score >= 80, `'C'` otherwise. Show the result. **[3 marks]**\n",
        "\n",
        "**(c)** Using `groupBy`, calculate the **average score** per student (by `name`). Order by average score descending. **[3 marks]**\n",
        "\n",
        "**(d)** Using `groupBy`, calculate the **average score** per subject per semester. Show the result ordered by semester then subject. **[3 marks]**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----+----------+\n",
            "| name|avg(score)|\n",
            "+-----+----------+\n",
            "|Carol|      79.0|\n",
            "|  Bob|     90.25|\n",
            "|Alice|     83.75|\n",
            "+-----+----------+\n",
            "\n",
            "None\n",
            "+--------+-------+-----------------+\n",
            "|semester|subject|       avg(score)|\n",
            "+--------+-------+-----------------+\n",
            "|    Fall|  Maths|84.33333333333333|\n",
            "|    Fall|Physics|82.66666666666667|\n",
            "|  Spring|  Maths|             89.0|\n",
            "|  Spring|Physics|             87.5|\n",
            "+--------+-------+-----------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Q5 — Write your code here\n",
        "from pyspark.sql.functions import col, when, avg, round as spark_round\n",
        "from pyspark.sql import functions as f\n",
        "# (a) Select columns and filter score >= 85\n",
        "filtered_grades = grades_df.select(\"name\",\"subject\",\"score\").filter(col(\"score\")>=85)\n",
        "# print(filtered_grades.show())\n",
        "\n",
        "# (b) Add grade column (A/B/C based on score)\n",
        "letter_score = grades_df.withColumn(\n",
        "    \"grade\",\n",
        "    when(col(\"score\") >= 90,'A').when(col(\"score\") >= 80,'B').otherwise('C')\n",
        ")\n",
        "# print(letter_score.show())\n",
        "# (c) Average score per student\n",
        "avg_score = grades_df.groupby(\"name\").agg(\n",
        "    avg(col(\"score\"))\n",
        ")\n",
        "print(avg_score.show())\n",
        "# (d) Average score per subject per semester\n",
        "avg_score_subject = grades_df.groupBy(\"semester\",\"subject\").agg(avg(\"score\"))\n",
        "avg_score_subject.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Question 6 — Spark SQL [11 marks]\n",
        "\n",
        "Register the grades DataFrame as a temporary view and answer using **Spark SQL**.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "View 'grades' registered.\n"
          ]
        }
      ],
      "source": [
        "# Register the DataFrame as a temp view\n",
        "grades_df.createOrReplaceTempView(\"grades\")\n",
        "print(\"View 'grades' registered.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**(a)** Write a SQL query to find all students who scored **above 85** in **Maths**. Return: `name`, `score`, `semester`. **[3 marks]**\n",
        "\n",
        "**(b)** Write a SQL query to calculate the **average score per subject**. Return: `subject`, `avg_score` (rounded to 2 decimals). **[3 marks]**\n",
        "\n",
        "**(c)** Write a SQL query to find the **highest score** achieved by each student across all subjects and semesters. Return: `name`, `max_score`. Order by `max_score` descending. **[5 marks]**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----+-----+--------+\n",
            "| name|score|semester|\n",
            "+-----+-----+--------+\n",
            "|Alice|   85|    Fall|\n",
            "|  Bob|   92|    Fall|\n",
            "|Alice|   88|  Spring|\n",
            "|  Bob|   90|  Spring|\n",
            "+-----+-----+--------+\n",
            "\n",
            "+-------+---------+\n",
            "|subject|avg_score|\n",
            "+-------+---------+\n",
            "|Physics|     84.6|\n",
            "|  Maths|     86.2|\n",
            "+-------+---------+\n",
            "\n",
            "+-----+---------+\n",
            "| name|max_score|\n",
            "+-----+---------+\n",
            "|  Bob|       92|\n",
            "|Alice|       88|\n",
            "|Carol|       82|\n",
            "+-----+---------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Q6 — Write your SQL queries here\n",
        "\n",
        "# (a) Students scoring above 85 in Maths\n",
        "result_a = spark.sql(\"\"\"select name,score,semester from grades where score >= 85 and subject ='Maths'\n",
        "    \n",
        "\"\"\")\n",
        "result_a.show()\n",
        "\n",
        "\n",
        "# (b) Average score per subject\n",
        "result_b = spark.sql(\"\"\"\n",
        "    select subject, round(avg(score),2) as avg_score from grades group by subject\n",
        "\"\"\")\n",
        "result_b.show()\n",
        "\n",
        "\n",
        "# (c) Highest score per student\n",
        "result_c = spark.sql(\"\"\"\n",
        "    select name, max(score) as max_score from grades group by name order by max_score desc\n",
        "\"\"\")\n",
        "result_c.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "# Section C: Data Preprocessing & Similarity Search (35 marks)\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Question 7 — Text Preprocessing & Regular Expressions [12 marks]\n",
        "\n",
        "Consider the following product data with messy text:\n",
        "\n",
        "```\n",
        "id   raw_text\n",
        "1    \"Product: LAPTOP-2025 | Price: $999.99 | Stock: 50\"\n",
        "2    \"Product: mouse-2024 | Price: $29.50 | Stock: 200\"\n",
        "3    \"Product: KEYBOARD-2025 | Price: $79.00 | Stock: 75\"\n",
        "4    \"Product: Monitor-2023 | Price: $349.99 | Stock: 30\"\n",
        "5    \"Product: HEADSET-2025 | Price: $149.00 | Stock: 100\"\n",
        "```\n",
        "\n",
        "**Run the setup cell first.**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Products DataFrame created:\n",
            "+---+---------------------------------------------------+\n",
            "|id |raw_text                                           |\n",
            "+---+---------------------------------------------------+\n",
            "|1  |Product: LAPTOP-2025 | Price: $999.99 | Stock: 50  |\n",
            "|2  |Product: mouse-2024 | Price: $29.50 | Stock: 200   |\n",
            "|3  |Product: KEYBOARD-2025 | Price: $79.00 | Stock: 75 |\n",
            "|4  |Product: Monitor-2023 | Price: $349.99 | Stock: 30 |\n",
            "|5  |Product: HEADSET-2025 | Price: $149.00 | Stock: 100|\n",
            "+---+---------------------------------------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# === RUN THIS CELL FIRST ===\n",
        "product_data = [\n",
        "    (1, \"Product: LAPTOP-2025 | Price: $999.99 | Stock: 50\"),\n",
        "    (2, \"Product: mouse-2024 | Price: $29.50 | Stock: 200\"),\n",
        "    (3, \"Product: KEYBOARD-2025 | Price: $79.00 | Stock: 75\"),\n",
        "    (4, \"Product: Monitor-2023 | Price: $349.99 | Stock: 30\"),\n",
        "    (5, \"Product: HEADSET-2025 | Price: $149.00 | Stock: 100\"),\n",
        "]\n",
        "\n",
        "products_df = spark.createDataFrame(product_data, [\"id\", \"raw_text\"])\n",
        "print(\"Products DataFrame created:\")\n",
        "products_df.show(truncate=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**(a)** Using `regexp_extract`, extract the **product name** (e.g., \"LAPTOP-2025\") into a new column called `product_name`. Show the result. **[3 marks]**\n",
        "\n",
        "**(b)** Using `regexp_extract`, extract the **price** (the numeric value after $, e.g., \"999.99\") into a column called `price`. Cast it to `DoubleType`. **[3 marks]**\n",
        "\n",
        "**(c)** Using `lower()`, convert the `product_name` to lowercase. Then use `regexp_replace` to remove the year part (e.g., \"-2025\") from the product name. **[3 marks]**\n",
        "\n",
        "**(d)** Using `rlike`, filter to show only products from year **2025** (i.e., product name contains \"2025\"). **[3 marks]**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---+--------------------+-------------+\n",
            "| id|            raw_text| product_name|\n",
            "+---+--------------------+-------------+\n",
            "|  1|Product: LAPTOP-2...|  LAPTOP-2025|\n",
            "|  3|Product: KEYBOARD...|KEYBOARD-2025|\n",
            "|  5|Product: HEADSET-...| HEADSET-2025|\n",
            "+---+--------------------+-------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Q7 — Write your code here\n",
        "from pyspark.sql.functions import regexp_extract, regexp_replace, lower, col\n",
        "from pyspark.sql.types import DoubleType\n",
        "\n",
        "# (a) Extract product name\n",
        "product_name = products_df.withColumn(\n",
        "    \"product_name\",\n",
        "    regexp_extract(col(\"raw_text\"),r'Product:\\s+([^\\s|]+)',idx=1)\n",
        ")\n",
        "# product_name.show()\n",
        "# (b) Extract price and cast to Double\n",
        "product_price = products_df.withColumn(\n",
        "    \"price\",\n",
        "    regexp_extract(col(\"raw_text\"),r'Price:\\s+\\$([^\\s|]+)',idx=1).cast(DoubleType())\n",
        ")\n",
        "# product_price.show()\n",
        "\n",
        "# (c) Lowercase product name and remove year\n",
        "lower_cased = product_name.withColumn(\n",
        "    \"product_name\",\n",
        "    lower(regexp_replace(\"product_name\",r\"\\-\\d+\",\"\"))\n",
        ")\n",
        "# lower_cased.show()\n",
        "# (d) Filter products from 2025\n",
        "productss_2025 = product_name.filter(col(\"product_name\").rlike(r'2025'))\n",
        "productss_2025.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Question 8 — Shingling & Jaccard Similarity [12 marks]\n",
        "\n",
        "Consider these three short documents:\n",
        "\n",
        "```\n",
        "Doc A: \"the cat sat on the mat\"\n",
        "Doc B: \"the cat sat on the hat\"\n",
        "Doc C: \"the dog ran in the park\"\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**(a)** Write a Python function `word_shingles(text, n)` that returns a **set** of word n-grams. Apply it to all three documents with `n=2`. Print the shingle sets for each document. **[3 marks]**\n",
        "\n",
        "**(b)** Write a function `jaccard_similarity(set_a, set_b)` that computes Jaccard similarity. Calculate and print the similarity between: (A, B), (A, C), and (B, C). **[3 marks]**\n",
        "\n",
        "**(c)** Based on your results, which pair of documents is **most similar**? Which is **least similar**? **[2 marks]**\n",
        "\n",
        "**(d)** Write a simple `MinHash` function that takes a set and `num_hashes` parameter, and returns a signature (list of minimum hash values). Use Python's built-in `hash()` function with different salts. Compare the estimated Jaccard (from signatures) with the true Jaccard for documents A and B using `num_hashes=50`. **[4 marks]**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Q8 — Write your code here\n",
        "\n",
        "# Documents\n",
        "doc_a = \"the cat sat on the mat\"\n",
        "doc_b = \"the cat sat on the hat\"\n",
        "doc_c = \"the dog ran in the park\"\n",
        "\n",
        "# (a) Word shingles function, apply with n=2\n",
        "\n",
        "\n",
        "# (b) Jaccard similarity function and compute for all pairs\n",
        "\n",
        "\n",
        "# (c) Most similar and least similar pairs (print as comments or text)\n",
        "\n",
        "\n",
        "# (d) Simple MinHash function and comparison\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Question 9 — LSH with Spark ML [11 marks]\n",
        "\n",
        "Using the same three documents from Question 8, build an LSH pipeline with Spark ML.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**(a)** Create a Spark DataFrame with columns `id` and `text` for the three documents. Use `Tokenizer` to split into words, then `CountVectorizer` with `binary=True` to create feature vectors. Show the schema. **[3 marks]**\n",
        "\n",
        "**(b)** Fit a `MinHashLSH` model with `numHashTables=3`. Transform the data and show the hash values. **[3 marks]**\n",
        "\n",
        "**(c)** Use `approxSimilarityJoin` with threshold `0.6` to find similar document pairs. Display the results. **[3 marks]**\n",
        "\n",
        "**(d)** Use `approxNearestNeighbors` to find the 2 nearest neighbours of document A. Print their IDs and distances. **[2 marks]**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Q9 — Write your code here\n",
        "from pyspark.ml.feature import Tokenizer, CountVectorizer, MinHashLSH\n",
        "\n",
        "# (a) Create DataFrame, tokenize, vectorize\n",
        "\n",
        "\n",
        "# (b) Fit MinHashLSH and show hashes\n",
        "\n",
        "\n",
        "# (c) approxSimilarityJoin with threshold 0.6\n",
        "\n",
        "\n",
        "# (d) approxNearestNeighbors for document A\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Cleanup\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Stop Spark session\n",
        "spark.stop()\n",
        "print(\"Spark session stopped. Practice quiz complete!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "### End of Practice Quiz\n",
        "\n",
        "**Review your answers and check:**\n",
        "- [ ] All code cells execute without errors\n",
        "- [ ] Outputs match what you expect\n",
        "- [ ] You understand the concepts tested\n",
        "\n",
        "---\n",
        "*SCC.454: Large Scale Platforms for AI and Data Analysis — Lancaster University*\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
