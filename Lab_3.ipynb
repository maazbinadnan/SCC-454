{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "intro_header"
      },
      "source": [
        "![Lancaster University](https://www.lancaster.ac.uk/media/lancaster-university/content-assets/images/fst/logos/SCC-Logo.svg)\n",
        "\n",
        "# SCC.454: Large Scale Platforms for AI and Data Analysis\n",
        "## Lab 3: Data Preprocessing with Apache Spark\n",
        "\n",
        "**Duration:** 2 hours\n",
        "\n",
        "**Learning Objectives:**\n",
        "- Understand the importance of data preprocessing in big data pipelines\n",
        "- Master techniques for handling missing values in Spark DataFrames\n",
        "- Learn data type conversions and schema management\n",
        "- Apply text preprocessing techniques at scale\n",
        "- Use regular expressions (regex) for pattern matching and data extraction\n",
        "- Build complete data cleaning pipelines in PySpark"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "toc"
      },
      "source": [
        "## Table of Contents\n",
        "\n",
        "1. **Part 1: Introduction to Data Preprocessing** (15 minutes)\n",
        "   - Why Data Preprocessing Matters\n",
        "   - Setting up the Environment\n",
        "   - Overview of Spark's Data Cleaning Capabilities\n",
        "\n",
        "2. **Part 2: Handling Missing Values** (30 minutes)\n",
        "   - Identifying Missing Data\n",
        "   - Strategies for Missing Value Treatment\n",
        "   - Dropping Null Values\n",
        "   - Filling Missing Values (Imputation)\n",
        "   - Advanced Imputation Techniques\n",
        "   - Practical Exercise: Customer Data Cleaning\n",
        "\n",
        "3. **Part 3: Data Type Handling and Schema Management** (20 minutes)\n",
        "   - Understanding Spark Data Types\n",
        "   - Type Casting and Conversion\n",
        "   - Handling Date and Timestamp Fields\n",
        "   - Schema Validation and Enforcement\n",
        "\n",
        "4. **Part 4: Text Preprocessing** (25 minutes)\n",
        "   - String Functions in Spark\n",
        "   - Case Normalization and Trimming\n",
        "   - Tokenization and Text Splitting\n",
        "   - Stop Word Removal\n",
        "   - Text Cleaning Pipeline\n",
        "   - Practical Exercise: Product Review Cleaning\n",
        "\n",
        "5. **Part 5: Regular Expressions in Spark** (30 minutes)\n",
        "   - Introduction to Regex in PySpark\n",
        "   - Pattern Matching with rlike\n",
        "   - Extracting Data with regexp_extract\n",
        "   - Replacing Patterns with regexp_replace\n",
        "   - Complex Pattern Matching\n",
        "   - Practical Exercise: Log File Parsing\n",
        "\n",
        "6. **Part 6: Building Complete Preprocessing Pipelines** (15 minutes)\n",
        "   - Combining Multiple Preprocessing Steps\n",
        "   - Creating Reusable Cleaning Functions\n",
        "   - Data Quality Reporting\n",
        "   - Final Challenge: End-to-End Data Cleaning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "part1_intro"
      },
      "source": [
        "---\n",
        "# Part 1: Introduction to Data Preprocessing\n",
        "---\n",
        "\n",
        "## 1.1 Why Data Preprocessing Matters\n",
        "\n",
        "Data preprocessing is the foundation of any successful data analysis or machine learning project. In the context of big data, preprocessing becomes even more critical due to:\n",
        "\n",
        "**The Scale Challenge:**\n",
        "- Real-world datasets often contain millions or billions of records\n",
        "- Traditional preprocessing tools (like pandas) may not scale effectively\n",
        "- Distributed processing is essential for handling large volumes\n",
        "\n",
        "**Common Data Quality Issues:**\n",
        "- Missing values (null, NaN, empty strings)\n",
        "- Inconsistent formatting (dates, currencies, units)\n",
        "- Duplicate records\n",
        "- Invalid or out-of-range values\n",
        "- Inconsistent text data (case, whitespace, special characters)\n",
        "- Unstructured or semi-structured data requiring parsing\n",
        "\n",
        "**The Garbage In, Garbage Out Principle:**\n",
        "```\n",
        "Raw Data (Messy) --> Preprocessing Pipeline --> Clean Data (Analysis-Ready)\n",
        "```\n",
        "\n",
        "**Apache Spark's Advantages for Preprocessing:**\n",
        "- Distributed processing across clusters\n",
        "- Lazy evaluation for optimization\n",
        "- Rich built-in functions for data manipulation\n",
        "- SQL interface for familiar syntax\n",
        "- Seamless integration with ML pipelines"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "setup_intro"
      },
      "source": [
        "## 1.2 Setting up the Environment\n",
        "\n",
        "Let's set up PySpark in Google Colab. If you're running this on a different system, ensure Java is installed and JAVA_HOME is configured correctly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "install_spark"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "E: Could not open lock file /var/lib/dpkg/lock-frontend - open (13: Permission denied)\n",
            "E: Unable to acquire the dpkg frontend lock (/var/lib/dpkg/lock-frontend), are you root?\n",
            "PySpark and Java installed successfully!\n"
          ]
        }
      ],
      "source": [
        "# Install PySpark\n",
        "!pip install pyspark==3.5.0 -q\n",
        "\n",
        "# Install Java (Spark requires Java)\n",
        "!apt-get install openjdk-11-jdk-headless -qq > /dev/null\n",
        "\n",
        "# Set Java environment variable\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\"\n",
        "\n",
        "print(\"PySpark and Java installed successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "create_session"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Spark Version: 4.0.1\n",
            "Application Name: SCC454-DataPreprocessing\n",
            "Master: local[*]\n",
            "\n",
            "Spark Session ready for data preprocessing!\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark import SparkConf\n",
        "\n",
        "# Create a SparkSession configured for data preprocessing\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"SCC454-DataPreprocessing\") \\\n",
        "    .config(\"spark.driver.memory\", \"4g\") \\\n",
        "    .config(\"spark.sql.legacy.timeParserPolicy\", \"LEGACY\") \\\n",
        "    .config(\"spark.ui.port\", \"4050\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Get the underlying SparkContext\n",
        "sc = spark.sparkContext\n",
        "\n",
        "print(f\"Spark Version: {spark.version}\")\n",
        "print(f\"Application Name: {spark.sparkContext.appName}\")\n",
        "print(f\"Master: {spark.sparkContext.master}\")\n",
        "print(\"\\nSpark Session ready for data preprocessing!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "capabilities_overview"
      },
      "source": [
        "## 1.3 Overview of Spark's Data Cleaning Capabilities\n",
        "\n",
        "Spark provides several modules and functions for data preprocessing:\n",
        "\n",
        "**Key Modules:**\n",
        "- `pyspark.sql.functions`: Built-in functions for transformations\n",
        "- `pyspark.sql.types`: Data type definitions\n",
        "- `pyspark.ml.feature`: Feature engineering tools\n",
        "\n",
        "**Important Function Categories:**\n",
        "\n",
        "| Category | Functions |\n",
        "|----------|----------|\n",
        "| Null Handling | `isNull()`, `isNotNull()`, `na.drop()`, `na.fill()` |\n",
        "| String Functions | `lower()`, `upper()`, `trim()`, `split()`, `concat()` |\n",
        "| Regex Functions | `regexp_extract()`, `regexp_replace()`, `rlike()` |\n",
        "| Type Conversion | `cast()`, `to_date()`, `to_timestamp()` |\n",
        "| Aggregation | `count()`, `avg()`, `sum()`, `mean()` |\n",
        "| Conditional | `when()`, `otherwise()`, `coalesce()` |"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pyspark.sql import functions as f"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "import_functions"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "All preprocessing functions imported successfully!\n"
          ]
        }
      ],
      "source": [
        "# Import commonly used functions for data preprocessing\n",
        "from pyspark.sql.functions import (\n",
        "    col, lit, isnan, isnull, coalesce,\n",
        "    lower, upper, trim, ltrim, rtrim, length,\n",
        "    split, concat, concat_ws, substring, replace,\n",
        "    regexp_extract, regexp_replace,\n",
        "    to_date, to_timestamp,\n",
        "    count, avg, sum as spark_sum, mean, stddev,\n",
        "    min as spark_min, max as spark_max,\n",
        "    when, expr,\n",
        "    year, month, dayofmonth, datediff, date_format,\n",
        "    monotonically_increasing_id, round as spark_round\n",
        ")\n",
        "\n",
        "from pyspark.sql.types import (\n",
        "    StructType, StructField, StringType, IntegerType,\n",
        "    FloatType, DoubleType, DateType, TimestampType, BooleanType\n",
        ")\n",
        "\n",
        "print(\"All preprocessing functions imported successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "part2_intro"
      },
      "source": [
        "---\n",
        "# Part 2: Handling Missing Values\n",
        "---\n",
        "\n",
        "## 2.1 Identifying Missing Data\n",
        "\n",
        "Missing data is one of the most common data quality issues. In Spark, missing values can appear as:\n",
        "- `null`: The absence of a value\n",
        "- `NaN` (Not a Number): Result of undefined numerical operations\n",
        "- Empty strings: For string columns\n",
        "- Placeholder values (e.g., -999, \"N/A\", \"Unknown\")\n",
        "\n",
        "Let's create a sample dataset with various types of missing values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "create_missing_data"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Customer Data with Missing Values:\n",
            "+-----------+------------+----+-----------------+-------+-----------+-----------+\n",
            "|customer_id|name        |age |email            |salary |city       |signup_date|\n",
            "+-----------+------------+----+-----------------+-------+-----------+-----------+\n",
            "|1          |John Smith  |28  |john@email.com   |75000.0|New York   |2023-01-15 |\n",
            "|2          |Jane Doe    |NULL|jane@email.com   |82000.0|Los Angeles|2023-02-20 |\n",
            "|3          |Bob Wilson  |35  |NULL             |65000.0|Chicago    |2023-03-10 |\n",
            "|4          |Alice Brown |42  |alice@email.com  |NULL   |Houston    |NULL       |\n",
            "|5          |NULL        |31  |unknown@email.com|70000.0|Phoenix    |2023-05-01 |\n",
            "|6          |Charlie Lee |29  |                 |58000.0|           |2023-06-15 |\n",
            "|7          |Diana Prince|NULL|diana@email.com  |95000.0|NULL       |2023-07-20 |\n",
            "|8          |Edward Kim  |38  |N/A              |NaN    |Boston     |2023-08-25 |\n",
            "|9          |Fiona Garcia|45  |fiona@email.com  |88000.0|Seattle    |2023-09-30 |\n",
            "|10         |George Hall |NULL|NULL             |NULL   |NULL       |NULL       |\n",
            "|11         |Hannah White|33  |hannah@email.com |72000.0|Denver     |2023-11-10 |\n",
            "|12         |Ivan Torres |27  |ivan@email.com   |63000.0|Miami      |2023-12-05 |\n",
            "+-----------+------------+----+-----------------+-------+-----------+-----------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Create a DataFrame with various missing value patterns\n",
        "customer_data = [\n",
        "    (1, \"John Smith\", 28, \"john@email.com\", 75000.0, \"New York\", \"2023-01-15\"),\n",
        "    (2, \"Jane Doe\", None, \"jane@email.com\", 82000.0, \"Los Angeles\", \"2023-02-20\"),\n",
        "    (3, \"Bob Wilson\", 35, None, 65000.0, \"Chicago\", \"2023-03-10\"),\n",
        "    (4, \"Alice Brown\", 42, \"alice@email.com\", None, \"Houston\", None),\n",
        "    (5, None, 31, \"unknown@email.com\", 70000.0, \"Phoenix\", \"2023-05-01\"),\n",
        "    (6, \"Charlie Lee\", 29, \"\", 58000.0, \"\", \"2023-06-15\"),\n",
        "    (7, \"Diana Prince\", None, \"diana@email.com\", 95000.0, None, \"2023-07-20\"),\n",
        "    (8, \"Edward Kim\", 38, \"N/A\", float('nan'), \"Boston\", \"2023-08-25\"),\n",
        "    (9, \"Fiona Garcia\", 45, \"fiona@email.com\", 88000.0, \"Seattle\", \"2023-09-30\"),\n",
        "    (10, \"George Hall\", None, None, None, None, None),\n",
        "    (11, \"Hannah White\", 33, \"hannah@email.com\", 72000.0, \"Denver\", \"2023-11-10\"),\n",
        "    (12, \"Ivan Torres\", 27, \"ivan@email.com\", 63000.0, \"Miami\", \"2023-12-05\")\n",
        "]\n",
        "\n",
        "columns = [\"customer_id\", \"name\", \"age\", \"email\", \"salary\", \"city\", \"signup_date\"]\n",
        "df_customers = spark.createDataFrame(customer_data, columns)\n",
        "\n",
        "print(\"Customer Data with Missing Values:\")\n",
        "df_customers.show(truncate=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "check_schema"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Schema:\n",
            "root\n",
            " |-- customer_id: long (nullable = true)\n",
            " |-- name: string (nullable = true)\n",
            " |-- age: long (nullable = true)\n",
            " |-- email: string (nullable = true)\n",
            " |-- salary: double (nullable = true)\n",
            " |-- city: string (nullable = true)\n",
            " |-- signup_date: string (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Check the schema\n",
        "print(\"Schema:\")\n",
        "df_customers.printSchema()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "count_missing_intro"
      },
      "source": [
        "### Counting Missing Values\n",
        "\n",
        "The first step in handling missing data is to understand its extent."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "count_nulls_basic"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Null counts per column:\n",
            "+-----------+----+---+-----+------+----+-----------+\n",
            "|customer_id|name|age|email|salary|city|signup_date|\n",
            "+-----------+----+---+-----+------+----+-----------+\n",
            "|          0|   1|  3|    2|     2|   2|          2|\n",
            "+-----------+----+---+-----+------+----+-----------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Method 1: Count nulls per column\n",
        "print(\"Null counts per column:\")\n",
        "null_counts = df_customers.select([\n",
        "    count(when(col(c).isNull(), c)).alias(c)\n",
        "    for c in df_customers.columns\n",
        "])\n",
        "null_counts.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "count_nulls_nan"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Null + NaN counts for numeric columns:\n",
            "+--------------+\n",
            "|salary_missing|\n",
            "+--------------+\n",
            "|             3|\n",
            "+--------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Method 2: Count nulls AND NaN values (important for numeric columns)\n",
        "print(\"Null + NaN counts for numeric columns:\")\n",
        "df_customers.select([\n",
        "    f.count(f.when(f.col(\"salary\").isNull() | f.isnan(f.col(\"salary\")), 1)).alias(\"salary_missing\")\n",
        "]).show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----------+------------+----+-----------------+-------+-----------+-----------+\n",
            "|customer_id|        name| age|            email| salary|       city|signup_date|\n",
            "+-----------+------------+----+-----------------+-------+-----------+-----------+\n",
            "|          1|  John Smith|  28|   john@email.com|75000.0|   New York| 2023-01-15|\n",
            "|          2|    Jane Doe|NULL|   jane@email.com|82000.0|Los Angeles| 2023-02-20|\n",
            "|          3|  Bob Wilson|  35|             NULL|65000.0|    Chicago| 2023-03-10|\n",
            "|          4| Alice Brown|  42|  alice@email.com|   NULL|    Houston|       NULL|\n",
            "|          5|        NULL|  31|unknown@email.com|70000.0|    Phoenix| 2023-05-01|\n",
            "|          6| Charlie Lee|  29|                 |58000.0|           | 2023-06-15|\n",
            "|          7|Diana Prince|NULL|  diana@email.com|95000.0|       NULL| 2023-07-20|\n",
            "|          8|  Edward Kim|  38|              N/A|    NaN|     Boston| 2023-08-25|\n",
            "|          9|Fiona Garcia|  45|  fiona@email.com|88000.0|    Seattle| 2023-09-30|\n",
            "|         10| George Hall|NULL|             NULL|   NULL|       NULL|       NULL|\n",
            "|         11|Hannah White|  33| hannah@email.com|72000.0|     Denver| 2023-11-10|\n",
            "|         12| Ivan Torres|  27|   ivan@email.com|63000.0|      Miami| 2023-12-05|\n",
            "+-----------+------------+----+-----------------+-------+-----------+-----------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df_customers.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "missing_report"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "COMPREHENSIVE MISSING VALUE REPORT\n",
            "============================================================\n",
            "+-----------+------------+-----+-----+---+-------------+---------+\n",
            "|Column     |Type        |Nulls|Empty|NaN|Total_Missing|Missing_%|\n",
            "+-----------+------------+-----+-----+---+-------------+---------+\n",
            "|customer_id|LongType()  |0    |0    |0  |0            |0.0      |\n",
            "|name       |StringType()|1    |0    |0  |1            |8.33     |\n",
            "|age        |LongType()  |3    |0    |0  |3            |25.0     |\n",
            "|email      |StringType()|2    |2    |0  |4            |33.33    |\n",
            "|salary     |DoubleType()|2    |0    |1  |3            |25.0     |\n",
            "|city       |StringType()|2    |1    |0  |3            |25.0     |\n",
            "|signup_date|StringType()|2    |0    |0  |2            |16.67    |\n",
            "+-----------+------------+-----+-----+---+-------------+---------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Method 3: Comprehensive missing value report\n",
        "def missing_value_report(df):\n",
        "    total_rows = df.count()\n",
        "    report_data = []\n",
        "    for col_name in df.columns:\n",
        "        col_type = str(df.schema[col_name].dataType)\n",
        "        null_count = df.filter(col(col_name).isNull()).count()\n",
        "        empty_count = 0\n",
        "        if \"String\" in col_type:\n",
        "            empty_count = df.filter(\n",
        "                (col(col_name) == \"\") | (col(col_name) == \"N/A\") | (col(col_name) == \"Unknown\")\n",
        "            ).count()\n",
        "        nan_count = 0\n",
        "        if \"Double\" in col_type or \"Float\" in col_type:\n",
        "            nan_count = df.filter(isnan(col(col_name))).count()\n",
        "        total_missing = null_count + empty_count + nan_count\n",
        "        missing_pct = (total_missing / total_rows) * 100\n",
        "        report_data.append((col_name, col_type, null_count, empty_count, nan_count, total_missing, round(missing_pct, 2)))\n",
        "    report_columns = [\"Column\", \"Type\", \"Nulls\", \"Empty\", \"NaN\", \"Total_Missing\", \"Missing_%\"]\n",
        "    return spark.createDataFrame(report_data, report_columns)\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"COMPREHENSIVE MISSING VALUE REPORT\")\n",
        "print(\"=\"*60)\n",
        "report = missing_value_report(df_customers)\n",
        "report.show(truncate=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "strategies_intro"
      },
      "source": [
        "## 2.2 Strategies for Missing Value Treatment\n",
        "\n",
        "There are several strategies for handling missing values:\n",
        "\n",
        "1. **Deletion**: Remove rows or columns with missing values\n",
        "2. **Imputation**: Fill missing values with estimated values (mean, median, mode)\n",
        "3. **Flagging**: Create indicator variables for missingness\n",
        "4. **Keep as-is**: Leave missing values for algorithms that can handle them"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dropping_nulls"
      },
      "source": [
        "## 2.3 Dropping Null Values\n",
        "\n",
        "Spark's `na.drop()` method provides flexible options for removing rows with missing values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "drop_any"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original row count: 12\n",
            "After dropping rows with ANY null: 5\n",
            "+-----------+------------+---+----------------+-------+--------+-----------+\n",
            "|customer_id|        name|age|           email| salary|    city|signup_date|\n",
            "+-----------+------------+---+----------------+-------+--------+-----------+\n",
            "|          1|  John Smith| 28|  john@email.com|75000.0|New York| 2023-01-15|\n",
            "|          6| Charlie Lee| 29|                |58000.0|        | 2023-06-15|\n",
            "|          9|Fiona Garcia| 45| fiona@email.com|88000.0| Seattle| 2023-09-30|\n",
            "|         11|Hannah White| 33|hannah@email.com|72000.0|  Denver| 2023-11-10|\n",
            "|         12| Ivan Torres| 27|  ivan@email.com|63000.0|   Miami| 2023-12-05|\n",
            "+-----------+------------+---+----------------+-------+--------+-----------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Original count\n",
        "print(f\"Original row count: {df_customers.count()}\")\n",
        "\n",
        "# Drop rows with ANY null value (most aggressive)\n",
        "df_drop_any = df_customers.na.drop(how=\"any\")\n",
        "print(f\"After dropping rows with ANY null: {df_drop_any.count()}\")\n",
        "df_drop_any.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "drop_all"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "After dropping rows where ALL are null: 12\n"
          ]
        }
      ],
      "source": [
        "# Drop rows where ALL values are null (least aggressive)\n",
        "df_drop_all = df_customers.na.drop(how=\"all\")\n",
        "print(f\"After dropping rows where ALL are null: {df_drop_all.count()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "drop_subset"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "After dropping rows where name OR email is null: 9\n",
            "+-----------+------------+----+----------------+-------+-----------+-----------+\n",
            "|customer_id|        name| age|           email| salary|       city|signup_date|\n",
            "+-----------+------------+----+----------------+-------+-----------+-----------+\n",
            "|          1|  John Smith|  28|  john@email.com|75000.0|   New York| 2023-01-15|\n",
            "|          2|    Jane Doe|NULL|  jane@email.com|82000.0|Los Angeles| 2023-02-20|\n",
            "|          4| Alice Brown|  42| alice@email.com|   NULL|    Houston|       NULL|\n",
            "|          6| Charlie Lee|  29|                |58000.0|           | 2023-06-15|\n",
            "|          7|Diana Prince|NULL| diana@email.com|95000.0|       NULL| 2023-07-20|\n",
            "|          8|  Edward Kim|  38|             N/A|    NaN|     Boston| 2023-08-25|\n",
            "|          9|Fiona Garcia|  45| fiona@email.com|88000.0|    Seattle| 2023-09-30|\n",
            "|         11|Hannah White|  33|hannah@email.com|72000.0|     Denver| 2023-11-10|\n",
            "|         12| Ivan Torres|  27|  ivan@email.com|63000.0|      Miami| 2023-12-05|\n",
            "+-----------+------------+----+----------------+-------+-----------+-----------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Drop rows where specific columns are null (most practical)\n",
        "df_drop_subset = df_customers.na.drop(subset=[\"name\", \"email\"])\n",
        "print(f\"After dropping rows where name OR email is null: {df_drop_subset.count()}\")\n",
        "df_drop_subset.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "drop_thresh"
      },
      "outputs": [],
      "source": [
        "# Drop rows with fewer than a threshold of non-null values\n",
        "df_drop_thresh = df_customers.na.drop(thresh=5)\n",
        "print(f\"After dropping rows with < 5 non-null values: {df_drop_thresh.count()}\")\n",
        "df_drop_thresh.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "filling_nulls"
      },
      "source": [
        "## 2.4 Filling Missing Values (Imputation)\n",
        "\n",
        "Spark's `na.fill()` method allows filling missing values with specified values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "fill_single"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fill all numeric nulls with 0:\n",
            "+-----------+------------+---+-----------------+-------+-----------+-----------+\n",
            "|customer_id|        name|age|            email| salary|       city|signup_date|\n",
            "+-----------+------------+---+-----------------+-------+-----------+-----------+\n",
            "|          1|  John Smith| 28|   john@email.com|75000.0|   New York| 2023-01-15|\n",
            "|          2|    Jane Doe|  0|   jane@email.com|82000.0|Los Angeles| 2023-02-20|\n",
            "|          3|  Bob Wilson| 35|             NULL|65000.0|    Chicago| 2023-03-10|\n",
            "|          4| Alice Brown| 42|  alice@email.com|    0.0|    Houston|       NULL|\n",
            "|          5|        NULL| 31|unknown@email.com|70000.0|    Phoenix| 2023-05-01|\n",
            "|          6| Charlie Lee| 29|                 |58000.0|           | 2023-06-15|\n",
            "|          7|Diana Prince|  0|  diana@email.com|95000.0|       NULL| 2023-07-20|\n",
            "|          8|  Edward Kim| 38|              N/A|    0.0|     Boston| 2023-08-25|\n",
            "|          9|Fiona Garcia| 45|  fiona@email.com|88000.0|    Seattle| 2023-09-30|\n",
            "|         10| George Hall|  0|             NULL|    0.0|       NULL|       NULL|\n",
            "|         11|Hannah White| 33| hannah@email.com|72000.0|     Denver| 2023-11-10|\n",
            "|         12| Ivan Torres| 27|   ivan@email.com|63000.0|      Miami| 2023-12-05|\n",
            "+-----------+------------+---+-----------------+-------+-----------+-----------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Fill all numeric columns with a single value\n",
        "print(\"Fill all numeric nulls with 0:\")\n",
        "df_customers.na.fill(0).show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "fill_dict"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fill with specific values per column:\n",
            "+-----------+------------+---+------------------------+-------+-----------+-----------+\n",
            "|customer_id|name        |age|email                   |salary |city       |signup_date|\n",
            "+-----------+------------+---+------------------------+-------+-----------+-----------+\n",
            "|1          |John Smith  |28 |john@email.com          |75000.0|New York   |2023-01-15 |\n",
            "|2          |Jane Doe    |30 |jane@email.com          |82000.0|Los Angeles|2023-02-20 |\n",
            "|3          |Bob Wilson  |35 |no_email@placeholder.com|65000.0|Chicago    |2023-03-10 |\n",
            "|4          |Alice Brown |42 |alice@email.com         |0.0    |Houston    |1970-01-01 |\n",
            "|5          |Unknown     |31 |unknown@email.com       |70000.0|Phoenix    |2023-05-01 |\n",
            "|6          |Charlie Lee |29 |                        |58000.0|           |2023-06-15 |\n",
            "|7          |Diana Prince|30 |diana@email.com         |95000.0|Unknown    |2023-07-20 |\n",
            "|8          |Edward Kim  |38 |N/A                     |0.0    |Boston     |2023-08-25 |\n",
            "|9          |Fiona Garcia|45 |fiona@email.com         |88000.0|Seattle    |2023-09-30 |\n",
            "|10         |George Hall |30 |no_email@placeholder.com|0.0    |Unknown    |1970-01-01 |\n",
            "|11         |Hannah White|33 |hannah@email.com        |72000.0|Denver     |2023-11-10 |\n",
            "|12         |Ivan Torres |27 |ivan@email.com          |63000.0|Miami      |2023-12-05 |\n",
            "+-----------+------------+---+------------------------+-------+-----------+-----------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Fill specific columns with specific values (dictionary approach)\n",
        "fill_values = {\n",
        "    \"name\": \"Unknown\",\n",
        "    \"age\": 30,\n",
        "    \"email\": \"no_email@placeholder.com\",\n",
        "    \"salary\": 0.0,\n",
        "    \"city\": \"Unknown\",\n",
        "    \"signup_date\": \"1970-01-01\"\n",
        "}\n",
        "\n",
        "print(\"Fill with specific values per column:\")\n",
        "df_filled = df_customers.na.fill(fill_values)\n",
        "df_filled.show(truncate=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "statistical_imputation"
      },
      "source": [
        "### Statistical Imputation\n",
        "\n",
        "For numerical columns, it's often better to use statistical measures like mean or median."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "mean_imputation"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mean age: 34.22\n",
            "Median age: 33\n",
            "\n",
            "Age filled with mean:\n",
            "+-----------+------------+---+\n",
            "|customer_id|        name|age|\n",
            "+-----------+------------+---+\n",
            "|          1|  John Smith| 28|\n",
            "|          2|    Jane Doe| 34|\n",
            "|          3|  Bob Wilson| 35|\n",
            "|          4| Alice Brown| 42|\n",
            "|          5|        NULL| 31|\n",
            "|          6| Charlie Lee| 29|\n",
            "|          7|Diana Prince| 34|\n",
            "|          8|  Edward Kim| 38|\n",
            "|          9|Fiona Garcia| 45|\n",
            "|         10| George Hall| 34|\n",
            "|         11|Hannah White| 33|\n",
            "|         12| Ivan Torres| 27|\n",
            "+-----------+------------+---+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Calculate mean for age column (excluding nulls)\n",
        "age_stats = df_customers.select(\n",
        "    mean(col(\"age\")).alias(\"mean_age\"),\n",
        "    expr(\"percentile_approx(age, 0.5)\").alias(\"median_age\")\n",
        ").collect()[0]\n",
        "\n",
        "mean_age = age_stats[\"mean_age\"]\n",
        "median_age = age_stats[\"median_age\"]\n",
        "\n",
        "print(f\"Mean age: {mean_age:.2f}\")\n",
        "print(f\"Median age: {median_age}\")\n",
        "\n",
        "# Fill with mean\n",
        "df_mean_imputed = df_customers.na.fill({\"age\": int(mean_age)})\n",
        "print(\"\\nAge filled with mean:\")\n",
        "df_mean_imputed.select(\"customer_id\", \"name\", \"age\").show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "median_imputation"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mean salary: $74,222.22\n",
            "Median salary: $72,000.00\n"
          ]
        }
      ],
      "source": [
        "# Calculate and impute with median for salary\n",
        "salary_stats = df_customers.filter(\n",
        "    col(\"salary\").isNotNull() & ~isnan(col(\"salary\"))\n",
        ").select(\n",
        "    mean(col(\"salary\")).alias(\"mean_salary\"),\n",
        "    expr(\"percentile_approx(salary, 0.5)\").alias(\"median_salary\")\n",
        ").collect()[0]\n",
        "\n",
        "print(f\"Mean salary: ${salary_stats['mean_salary']:,.2f}\")\n",
        "print(f\"Median salary: ${salary_stats['median_salary']:,.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "advanced_imputation"
      },
      "source": [
        "## 2.5 Advanced Imputation Techniques"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "group_imputation_prep"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Average salary per city:\n",
            "+-----------+---------------+\n",
            "|       city|city_avg_salary|\n",
            "+-----------+---------------+\n",
            "|    Phoenix|        70000.0|\n",
            "|Los Angeles|        82000.0|\n",
            "|    Chicago|        65000.0|\n",
            "|           |        58000.0|\n",
            "|   New York|        75000.0|\n",
            "|    Seattle|        88000.0|\n",
            "|      Miami|        63000.0|\n",
            "|     Denver|        72000.0|\n",
            "+-----------+---------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# # Group-wise imputation: Fill missing values based on group statistics\n",
        "from pyspark.sql.window import Window\n",
        "\n",
        "# Calculate average salary per city\n",
        "city_avg_salary = df_customers.filter(\n",
        "    col(\"salary\").isNotNull() & ~isnan(col(\"salary\")) & col(\"city\").isNotNull()\n",
        ").groupBy(\"city\").agg(\n",
        "    spark_round(mean(\"salary\"), 2).alias(\"city_avg_salary\")\n",
        ")\n",
        "\n",
        "print(\"Average salary per city:\")\n",
        "city_avg_salary.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "group_imputation_apply"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Smart imputation using city averages:\n",
            "+-----------+------------+-----------+-------+---------------+-----------------+\n",
            "|customer_id|        name|       city| salary|city_avg_salary|   salary_imputed|\n",
            "+-----------+------------+-----------+-------+---------------+-----------------+\n",
            "|          2|    Jane Doe|Los Angeles|82000.0|        82000.0|          82000.0|\n",
            "|          3|  Bob Wilson|    Chicago|65000.0|        65000.0|          65000.0|\n",
            "|          1|  John Smith|   New York|75000.0|        75000.0|          75000.0|\n",
            "|          5|        NULL|    Phoenix|70000.0|        70000.0|          70000.0|\n",
            "|          4| Alice Brown|    Houston|   NULL|           NULL|74222.22222222222|\n",
            "|          6| Charlie Lee|           |58000.0|        58000.0|          58000.0|\n",
            "|          7|Diana Prince|       NULL|95000.0|           NULL|          95000.0|\n",
            "|          9|Fiona Garcia|    Seattle|88000.0|        88000.0|          88000.0|\n",
            "|          8|  Edward Kim|     Boston|    NaN|           NULL|74222.22222222222|\n",
            "|         10| George Hall|       NULL|   NULL|           NULL|74222.22222222222|\n",
            "|         12| Ivan Torres|      Miami|63000.0|        63000.0|          63000.0|\n",
            "|         11|Hannah White|     Denver|72000.0|        72000.0|          72000.0|\n",
            "+-----------+------------+-----------+-------+---------------+-----------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Using coalesce for conditional filling\n",
        "overall_avg_salary = salary_stats['mean_salary']\n",
        "\n",
        "df_smart_imputed = df_customers.join(\n",
        "    city_avg_salary, on=\"city\", how=\"left\"\n",
        ").withColumn(\n",
        "    \"salary_imputed\",\n",
        "    coalesce(\n",
        "        when(~isnan(col(\"salary\")), col(\"salary\")),\n",
        "        col(\"city_avg_salary\"),\n",
        "        lit(overall_avg_salary)\n",
        "    )\n",
        ").select(\n",
        "    \"customer_id\", \"name\", \"city\", \"salary\", \"city_avg_salary\", \"salary_imputed\"\n",
        ")\n",
        "\n",
        "print(\"Smart imputation using city averages:\")\n",
        "df_smart_imputed.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "forward_fill"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Time series with missing values:\n",
            "+----------+-----+\n",
            "|      date|value|\n",
            "+----------+-----+\n",
            "|2024-01-01|100.0|\n",
            "|2024-01-02| NULL|\n",
            "|2024-01-03| NULL|\n",
            "|2024-01-04|105.0|\n",
            "|2024-01-05| NULL|\n",
            "|2024-01-06|108.0|\n",
            "|2024-01-07| NULL|\n",
            "+----------+-----+\n",
            "\n",
            "After forward fill:\n",
            "+----------+-----+-----------+\n",
            "|      date|value|value_ffill|\n",
            "+----------+-----+-----------+\n",
            "|2024-01-01|100.0|      100.0|\n",
            "|2024-01-02| NULL|      100.0|\n",
            "|2024-01-03| NULL|      100.0|\n",
            "|2024-01-04|105.0|      105.0|\n",
            "|2024-01-05| NULL|      105.0|\n",
            "|2024-01-06|108.0|      108.0|\n",
            "|2024-01-07| NULL|      108.0|\n",
            "+----------+-----+-----------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Forward Fill using Window functions (useful for time-series)\n",
        "from pyspark.sql.functions import last, first\n",
        "\n",
        "ts_data = [\n",
        "    (\"2024-01-01\", 100.0),\n",
        "    (\"2024-01-02\", None),\n",
        "    (\"2024-01-03\", None),\n",
        "    (\"2024-01-04\", 105.0),\n",
        "    (\"2024-01-05\", None),\n",
        "    (\"2024-01-06\", 108.0),\n",
        "    (\"2024-01-07\", None),\n",
        "]\n",
        "\n",
        "df_ts = spark.createDataFrame(ts_data, [\"date\", \"value\"])\n",
        "print(\"Time series with missing values:\")\n",
        "df_ts.show()\n",
        "\n",
        "# Forward fill\n",
        "window_forward = Window.orderBy(\"date\").rowsBetween(Window.unboundedPreceding, 0)\n",
        "\n",
        "df_forward_fill = df_ts.withColumn(\n",
        "    \"value_ffill\",\n",
        "    last(col(\"value\"), ignorenulls=True).over(window_forward)\n",
        ")\n",
        "\n",
        "print(\"After forward fill:\")\n",
        "df_forward_fill.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "missing_indicators"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data with missing value indicators:\n",
            "+-----------+------------+----+---------------+-------+------------------+\n",
            "|customer_id|        name| age|age_was_missing| salary|salary_was_missing|\n",
            "+-----------+------------+----+---------------+-------+------------------+\n",
            "|          1|  John Smith|  28|              0|75000.0|                 0|\n",
            "|          2|    Jane Doe|NULL|              1|82000.0|                 0|\n",
            "|          3|  Bob Wilson|  35|              0|65000.0|                 0|\n",
            "|          4| Alice Brown|  42|              0|   NULL|                 1|\n",
            "|          5|        NULL|  31|              0|70000.0|                 0|\n",
            "|          6| Charlie Lee|  29|              0|58000.0|                 0|\n",
            "|          7|Diana Prince|NULL|              1|95000.0|                 0|\n",
            "|          8|  Edward Kim|  38|              0|    NaN|                 1|\n",
            "|          9|Fiona Garcia|  45|              0|88000.0|                 0|\n",
            "|         10| George Hall|NULL|              1|   NULL|                 1|\n",
            "|         11|Hannah White|  33|              0|72000.0|                 0|\n",
            "|         12| Ivan Torres|  27|              0|63000.0|                 0|\n",
            "+-----------+------------+----+---------------+-------+------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Creating missing value indicator columns\n",
        "df_with_indicators = df_customers.withColumn(\n",
        "    \"age_was_missing\",\n",
        "    when(col(\"age\").isNull(), 1).otherwise(0)\n",
        ").withColumn(\n",
        "    \"salary_was_missing\",\n",
        "    when(col(\"salary\").isNull() | isnan(col(\"salary\")), 1).otherwise(0)\n",
        ")\n",
        "\n",
        "print(\"Data with missing value indicators:\")\n",
        "df_with_indicators.select(\n",
        "    \"customer_id\", \"name\", \"age\", \"age_was_missing\", \"salary\", \"salary_was_missing\"\n",
        ").show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "exercise1"
      },
      "source": [
        "## 2.6 Practical Exercise: Customer Data Cleaning\n",
        "\n",
        "Now it's your turn! Clean the customer dataset by applying appropriate imputation strategies.\n",
        "\n",
        "**Tasks:**\n",
        "1. Handle missing names (fill with \"Customer_[ID]\")\n",
        "2. Impute missing ages with the median age\n",
        "3. Replace empty/placeholder emails with a standardized format\n",
        "4. Impute missing salaries with the median salary\n",
        "5. Fill missing cities with \"Unknown\"\n",
        "6. Handle missing signup dates appropriately\n",
        "7. Remove the NaN value from salary column\n",
        "8. Create a data quality score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "exercise1_code"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----------+------------+----+-----------------+-------+-----------+-----------+\n",
            "|customer_id|        name| age|            email| salary|       city|signup_date|\n",
            "+-----------+------------+----+-----------------+-------+-----------+-----------+\n",
            "|          1|  John Smith|  28|   john@email.com|75000.0|   New York| 2023-01-15|\n",
            "|          2|    Jane Doe|NULL|   jane@email.com|82000.0|Los Angeles| 2023-02-20|\n",
            "|          3|  Bob Wilson|  35|             NULL|65000.0|    Chicago| 2023-03-10|\n",
            "|          4| Alice Brown|  42|  alice@email.com|   NULL|    Houston|       NULL|\n",
            "|          5|        NULL|  31|unknown@email.com|70000.0|    Phoenix| 2023-05-01|\n",
            "|          6| Charlie Lee|  29|                 |58000.0|           | 2023-06-15|\n",
            "|          7|Diana Prince|NULL|  diana@email.com|95000.0|       NULL| 2023-07-20|\n",
            "|          8|  Edward Kim|  38|              N/A|    NaN|     Boston| 2023-08-25|\n",
            "|          9|Fiona Garcia|  45|  fiona@email.com|88000.0|    Seattle| 2023-09-30|\n",
            "|         10| George Hall|NULL|             NULL|   NULL|       NULL|       NULL|\n",
            "|         11|Hannah White|  33| hannah@email.com|72000.0|     Denver| 2023-11-10|\n",
            "|         12| Ivan Torres|  27|   ivan@email.com|63000.0|      Miami| 2023-12-05|\n",
            "+-----------+------------+----+-----------------+-------+-----------+-----------+\n",
            "\n",
            "['customer_id', 'name', 'age', 'email', 'salary', 'city', 'signup_date']\n"
          ]
        }
      ],
      "source": [
        "# Your code here\n",
        "df_customers.show()\n",
        "print(df_customers.columns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----------+----+---+-----+------+----+-----------+\n",
            "|customer_id|name|age|email|salary|city|signup_date|\n",
            "+-----------+----+---+-----+------+----+-----------+\n",
            "|          0|   1|  3|    4|     3|   3|          2|\n",
            "+-----------+----+---+-----+------+----+-----------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "##find amount of missing values in the dataframe\n",
        "from pyspark.sql.functions import isnan,when,count,col,isnull\n",
        "##first find the number only\n",
        "\n",
        "df_customers.select(\n",
        "   [\n",
        "    count(\n",
        "        when(\n",
        "            isnull(c) | \n",
        "            (isnan(col(c)) if t in (\"double\", \"float\", \"int\") else lit(False)) |\n",
        "            ((col(c) == \"\") if t == \"string\" else lit(False)) |\n",
        "            ((col(c).contains(\"N/A\")) if t == \"string\" else lit(False)), \n",
        "            c\n",
        "        )\n",
        "    ).alias(c) \n",
        "    for c, t in df_customers.dtypes\n",
        "]).show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "replace names with customer ID"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----------+------------+----+-----------------+-------+-----------+-----------+\n",
            "|customer_id|        name| age|            email| salary|       city|signup_date|\n",
            "+-----------+------------+----+-----------------+-------+-----------+-----------+\n",
            "|          1|  John Smith|  28|   john@email.com|75000.0|   New York| 2023-01-15|\n",
            "|          2|    Jane Doe|NULL|   jane@email.com|82000.0|Los Angeles| 2023-02-20|\n",
            "|          3|  Bob Wilson|  35|             NULL|65000.0|    Chicago| 2023-03-10|\n",
            "|          4| Alice Brown|  42|  alice@email.com|   NULL|    Houston|       NULL|\n",
            "|          5|           5|  31|unknown@email.com|70000.0|    Phoenix| 2023-05-01|\n",
            "|          6| Charlie Lee|  29|                 |58000.0|           | 2023-06-15|\n",
            "|          7|Diana Prince|NULL|  diana@email.com|95000.0|       NULL| 2023-07-20|\n",
            "|          8|  Edward Kim|  38|              N/A|    NaN|     Boston| 2023-08-25|\n",
            "|          9|Fiona Garcia|  45|  fiona@email.com|88000.0|    Seattle| 2023-09-30|\n",
            "|         10| George Hall|NULL|             NULL|   NULL|       NULL|       NULL|\n",
            "|         11|Hannah White|  33| hannah@email.com|72000.0|     Denver| 2023-11-10|\n",
            "|         12| Ivan Torres|  27|   ivan@email.com|63000.0|      Miami| 2023-12-05|\n",
            "+-----------+------------+----+-----------------+-------+-----------+-----------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df_customers_cleaned = df_customers.withColumn(\n",
        "    'name',\n",
        "    when(col('name').isNull(),col('customer_id').cast(\"string\")).otherwise(col('name'))\n",
        ")\n",
        "\n",
        "df_customers_cleaned.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### IMPUTE MISSING AGES WITH MEDIAN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import median"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----------+------------+----+-----------------+-------+-----------+-----------+\n",
            "|customer_id|        name| age|            email| salary|       city|signup_date|\n",
            "+-----------+------------+----+-----------------+-------+-----------+-----------+\n",
            "|          1|  John Smith|28.0|   john@email.com|75000.0|   New York| 2023-01-15|\n",
            "|          2|    Jane Doe|33.0|   jane@email.com|82000.0|Los Angeles| 2023-02-20|\n",
            "|          3|  Bob Wilson|35.0|             NULL|65000.0|    Chicago| 2023-03-10|\n",
            "|          4| Alice Brown|42.0|  alice@email.com|   NULL|    Houston|       NULL|\n",
            "|          5|           5|31.0|unknown@email.com|70000.0|    Phoenix| 2023-05-01|\n",
            "|          6| Charlie Lee|29.0|                 |58000.0|           | 2023-06-15|\n",
            "|          7|Diana Prince|33.0|  diana@email.com|95000.0|       NULL| 2023-07-20|\n",
            "|          8|  Edward Kim|38.0|              N/A|    NaN|     Boston| 2023-08-25|\n",
            "|          9|Fiona Garcia|45.0|  fiona@email.com|88000.0|    Seattle| 2023-09-30|\n",
            "|         10| George Hall|33.0|             NULL|   NULL|       NULL|       NULL|\n",
            "|         11|Hannah White|33.0| hannah@email.com|72000.0|     Denver| 2023-11-10|\n",
            "|         12| Ivan Torres|27.0|   ivan@email.com|63000.0|      Miami| 2023-12-05|\n",
            "+-----------+------------+----+-----------------+-------+-----------+-----------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df_customers_cleaned = df_customers_cleaned.withColumn(\n",
        "    'age',\n",
        "    when(col('age').isNull(),df_customers.approxQuantile(\"age\", [0.5], 0.01)[0]).otherwise(col(\"age\")))\n",
        "\n",
        "df_customers_cleaned.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "replace empty emails with a standardized format"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----------+------------+----+-----------------+-------+-----------+-----------+\n",
            "|customer_id|        name| age|            email| salary|       city|signup_date|\n",
            "+-----------+------------+----+-----------------+-------+-----------+-----------+\n",
            "|          1|  John Smith|28.0|   john@email.com|75000.0|   New York| 2023-01-15|\n",
            "|          2|    Jane Doe|33.0|   jane@email.com|82000.0|Los Angeles| 2023-02-20|\n",
            "|          3|  Bob Wilson|35.0| random@email.com|65000.0|    Chicago| 2023-03-10|\n",
            "|          4| Alice Brown|42.0|  alice@email.com|72000.0|    Houston|       NULL|\n",
            "|          5|           5|31.0|unknown@email.com|70000.0|    Phoenix| 2023-05-01|\n",
            "|          6| Charlie Lee|29.0| random@email.com|58000.0|           | 2023-06-15|\n",
            "|          7|Diana Prince|33.0|  diana@email.com|95000.0|       NULL| 2023-07-20|\n",
            "|          8|  Edward Kim|38.0| random@email.com|72000.0|     Boston| 2023-08-25|\n",
            "|          9|Fiona Garcia|45.0|  fiona@email.com|88000.0|    Seattle| 2023-09-30|\n",
            "|         10| George Hall|33.0| random@email.com|72000.0|       NULL|       NULL|\n",
            "|         11|Hannah White|33.0| hannah@email.com|72000.0|     Denver| 2023-11-10|\n",
            "|         12| Ivan Torres|27.0|   ivan@email.com|63000.0|      Miami| 2023-12-05|\n",
            "+-----------+------------+----+-----------------+-------+-----------+-----------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df_customers_cleaned = df_customers_cleaned.withColumn(\n",
        "    'email', when((col('email').isNull()) | (col('email') == \"\") | (col('email').contains(\"N/A\")),\"random@email.com\").otherwise(col('email')))\n",
        "\n",
        "df_customers_cleaned.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Replacing Salary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----------+------------+----+-----------------+-------+-----------+-----------+\n",
            "|customer_id|        name| age|            email| salary|       city|signup_date|\n",
            "+-----------+------------+----+-----------------+-------+-----------+-----------+\n",
            "|          1|  John Smith|28.0|   john@email.com|75000.0|   New York| 2023-01-15|\n",
            "|          2|    Jane Doe|33.0|   jane@email.com|82000.0|Los Angeles| 2023-02-20|\n",
            "|          3|  Bob Wilson|35.0| random@email.com|65000.0|    Chicago| 2023-03-10|\n",
            "|          4| Alice Brown|42.0|  alice@email.com|72000.0|    Houston|       NULL|\n",
            "|          5|           5|31.0|unknown@email.com|70000.0|    Phoenix| 2023-05-01|\n",
            "|          6| Charlie Lee|29.0| random@email.com|58000.0|           | 2023-06-15|\n",
            "|          7|Diana Prince|33.0|  diana@email.com|95000.0|       NULL| 2023-07-20|\n",
            "|          8|  Edward Kim|38.0| random@email.com|72000.0|     Boston| 2023-08-25|\n",
            "|          9|Fiona Garcia|45.0|  fiona@email.com|88000.0|    Seattle| 2023-09-30|\n",
            "|         10| George Hall|33.0| random@email.com|72000.0|       NULL|       NULL|\n",
            "|         11|Hannah White|33.0| hannah@email.com|72000.0|     Denver| 2023-11-10|\n",
            "|         12| Ivan Torres|27.0|   ivan@email.com|63000.0|      Miami| 2023-12-05|\n",
            "+-----------+------------+----+-----------------+-------+-----------+-----------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df_customers_cleaned = df_customers_cleaned.withColumn(\n",
        "    'salary',\n",
        "    when((col('salary').isNull() ) | (isnan(col=\"salary\")),df_customers.approxQuantile(\"salary\", [0.5], 0.01)[0]).otherwise(col(\"salary\")))\n",
        "\n",
        "df_customers_cleaned.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Handling missing city"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_customers_cleaned = df_customers_cleaned.withColumn(\n",
        "    'city',\n",
        "    when((col('city').isNull()) | (col('city') == \"\"), \"Unknown\").otherwise(col(\"city\")))\n",
        "\n",
        "df_customers_cleaned.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "part3_intro"
      },
      "source": [
        "---\n",
        "# Part 3: Data Type Handling and Schema Management\n",
        "---\n",
        "\n",
        "## 3.1 Understanding Spark Data Types\n",
        "\n",
        "Proper data types are essential for efficient processing.\n",
        "\n",
        "**Numeric Types:** ByteType, ShortType, IntegerType, LongType, FloatType, DoubleType, DecimalType\n",
        "\n",
        "**String and Binary:** StringType, BinaryType\n",
        "\n",
        "**Date/Time:** DateType, TimestampType\n",
        "\n",
        "**Complex Types:** ArrayType, MapType, StructType"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "messy_types"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Messy data with type issues:\n",
            "+---+------+--------+--------------+------+\n",
            "| id|   age|  salary|          date|active|\n",
            "+---+------+--------+--------------+------+\n",
            "|  1|    25|50000.50|    2024-01-15|  true|\n",
            "|  2|thirty|   60000|    15/02/2024|   yes|\n",
            "|  3|    35| $75,000|    2024-03-20|     1|\n",
            "|  4|    40|80000.00|March 25, 2024| false|\n",
            "|  5|   N/A| invalid|    2024-04-30|    no|\n",
            "+---+------+--------+--------------+------+\n",
            "\n",
            "root\n",
            " |-- id: string (nullable = true)\n",
            " |-- age: string (nullable = true)\n",
            " |-- salary: string (nullable = true)\n",
            " |-- date: string (nullable = true)\n",
            " |-- active: string (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Create a DataFrame with mixed type issues\n",
        "messy_data = [\n",
        "    (\"1\", \"25\", \"50000.50\", \"2024-01-15\", \"true\"),\n",
        "    (\"2\", \"thirty\", \"60000\", \"15/02/2024\", \"yes\"),\n",
        "    (\"3\", \"35\", \"$75,000\", \"2024-03-20\", \"1\"),\n",
        "    (\"4\", \"40\", \"80000.00\", \"March 25, 2024\", \"false\"),\n",
        "    (\"5\", \"N/A\", \"invalid\", \"2024-04-30\", \"no\"),\n",
        "]\n",
        "\n",
        "df_messy = spark.createDataFrame(messy_data, [\"id\", \"age\", \"salary\", \"date\", \"active\"])\n",
        "\n",
        "print(\"Messy data with type issues:\")\n",
        "df_messy.show()\n",
        "df_messy.printSchema()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "type_casting"
      },
      "source": [
        "## 3.2 Type Casting and Conversion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "simple_cast"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ID cast to integer:\n",
            "+---+------+\n",
            "| id|id_int|\n",
            "+---+------+\n",
            "|  1|     1|\n",
            "|  2|     2|\n",
            "|  3|     3|\n",
            "|  4|     4|\n",
            "|  5|     5|\n",
            "+---+------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Simple casting\n",
        "df_cast = df_messy.withColumn(\"id_int\", col(\"id\").cast(IntegerType()))\n",
        "print(\"ID cast to integer:\")\n",
        "df_cast.select(\"id\", \"id_int\").show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "cast_with_nulls"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Age cast to integer (note nulls for invalid values):\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "{\"ts\": \"2026-02-06 18:27:40.989\", \"level\": \"ERROR\", \"logger\": \"DataFrameQueryContextLogger\", \"msg\": \"[CAST_INVALID_INPUT] The value 'thirty' of the type \\\"STRING\\\" cannot be cast to \\\"INT\\\" because it is malformed. Correct the value as per the syntax, or change its target type. Use `try_cast` to tolerate malformed input and return NULL instead. SQLSTATE: 22018\", \"context\": {\"file\": \"line 2 in cell [41]\", \"line\": \"\", \"fragment\": \"cast\", \"errorClass\": \"CAST_INVALID_INPUT\"}, \"exception\": {\"class\": \"Py4JJavaError\", \"msg\": \"An error occurred while calling o997.showString.\\n: org.apache.spark.SparkNumberFormatException: [CAST_INVALID_INPUT] The value 'thirty' of the type \\\"STRING\\\" cannot be cast to \\\"INT\\\" because it is malformed. Correct the value as per the syntax, or change its target type. Use `try_cast` to tolerate malformed input and return NULL instead. SQLSTATE: 22018\\n== DataFrame ==\\n\\\"cast\\\" was called from\\nline 2 in cell [41]\\n\\n\\tat org.apache.spark.sql.errors.QueryExecutionErrors$.invalidInputInCastToNumberError(QueryExecutionErrors.scala:145)\\n\\tat org.apache.spark.sql.catalyst.util.UTF8StringUtils$.withException(UTF8StringUtils.scala:51)\\n\\tat org.apache.spark.sql.catalyst.util.UTF8StringUtils$.toIntExact(UTF8StringUtils.scala:34)\\n\\tat org.apache.spark.sql.catalyst.util.UTF8StringUtils.toIntExact(UTF8StringUtils.scala)\\n\\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\\n\\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\\n\\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\\n\\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:402)\\n\\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:901)\\n\\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:901)\\n\\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\\n\\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\\n\\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\\n\\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\\n\\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\\n\\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\\n\\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\\n\\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\\n\\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\\n\\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)\\n\\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)\\n\\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\\n\\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\\n\\tat java.base/java.lang.Thread.run(Thread.java:840)\\n\\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:1009)\\n\\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2484)\\n\\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2505)\\n\\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2524)\\n\\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:544)\\n\\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:497)\\n\\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:58)\\n\\tat org.apache.spark.sql.classic.Dataset.collectFromPlan(Dataset.scala:2244)\\n\\tat org.apache.spark.sql.classic.Dataset.$anonfun$head$1(Dataset.scala:1379)\\n\\tat org.apache.spark.sql.classic.Dataset.$anonfun$withAction$2(Dataset.scala:2234)\\n\\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)\\n\\tat org.apache.spark.sql.classic.Dataset.$anonfun$withAction$1(Dataset.scala:2232)\\n\\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$8(SQLExecution.scala:163)\\n\\tat org.apache.spark.sql.execution.SQLExecution$.withSessionTagsApplied(SQLExecution.scala:272)\\n\\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$7(SQLExecution.scala:125)\\n\\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\\n\\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:112)\\n\\tat org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:106)\\n\\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:111)\\n\\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$6(SQLExecution.scala:125)\\n\\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:295)\\n\\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:124)\\n\\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\\n\\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:78)\\n\\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:237)\\n\\tat org.apache.spark.sql.classic.Dataset.withAction(Dataset.scala:2232)\\n\\tat org.apache.spark.sql.classic.Dataset.head(Dataset.scala:1379)\\n\\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2810)\\n\\tat org.apache.spark.sql.classic.Dataset.getRows(Dataset.scala:339)\\n\\tat org.apache.spark.sql.classic.Dataset.showString(Dataset.scala:375)\\n\\tat jdk.internal.reflect.GeneratedMethodAccessor89.invoke(Unknown Source)\\n\\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\\n\\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\\n\\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\\n\\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\\n\\tat py4j.Gateway.invoke(Gateway.java:282)\\n\\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\\n\\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\\n\\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\\n\\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)\\n\\tat java.base/java.lang.Thread.run(Thread.java:840)\\n\", \"stacktrace\": [{\"class\": null, \"method\": \"deco\", \"file\": \"/usr/local/lib/python3.12/dist-packages/pyspark/errors/exceptions/captured.py\", \"line\": \"282\"}, {\"class\": null, \"method\": \"get_return_value\", \"file\": \"/usr/local/lib/python3.12/dist-packages/py4j/protocol.py\", \"line\": \"327\"}]}}\n"
          ]
        },
        {
          "ename": "NumberFormatException",
          "evalue": "[CAST_INVALID_INPUT] The value 'thirty' of the type \"STRING\" cannot be cast to \"INT\" because it is malformed. Correct the value as per the syntax, or change its target type. Use `try_cast` to tolerate malformed input and return NULL instead. SQLSTATE: 22018\n== DataFrame ==\n\"cast\" was called from\nline 2 in cell [41]\n",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNumberFormatException\u001b[0m                     Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-4096589249.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mdf_age_cast\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_messy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"age_int\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"age\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mIntegerType\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Age cast to integer (note nulls for invalid values):\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mdf_age_cast\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"age\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"age_int\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pyspark/sql/classic/dataframe.py\u001b[0m in \u001b[0;36mshow\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    283\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 285\u001b[0;31m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_show_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    286\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m     def _show_string(\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pyspark/sql/classic/dataframe.py\u001b[0m in \u001b[0;36m_show_string\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    301\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 303\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    304\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    305\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1360\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1361\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1362\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1363\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pyspark/errors/exceptions/captured.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    286\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 288\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    289\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    290\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNumberFormatException\u001b[0m: [CAST_INVALID_INPUT] The value 'thirty' of the type \"STRING\" cannot be cast to \"INT\" because it is malformed. Correct the value as per the syntax, or change its target type. Use `try_cast` to tolerate malformed input and return NULL instead. SQLSTATE: 22018\n== DataFrame ==\n\"cast\" was called from\nline 2 in cell [41]\n"
          ]
        }
      ],
      "source": [
        "# Casting with potential failures - non-numeric strings become null\n",
        "df_age_cast = df_messy.withColumn(\"age_int\", col(\"age\").cast(IntegerType()))\n",
        "print(\"Age cast to integer (note nulls for invalid values):\")\n",
        "df_age_cast.select(\"age\", \"age_int\").show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "clean_before_cast"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Salary cleaning and casting:\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "{\"ts\": \"2026-02-06 18:28:58.482\", \"level\": \"ERROR\", \"logger\": \"DataFrameQueryContextLogger\", \"msg\": \"[CAST_INVALID_INPUT] The value 'invalid' of the type \\\"STRING\\\" cannot be cast to \\\"DOUBLE\\\" because it is malformed. Correct the value as per the syntax, or change its target type. Use `try_cast` to tolerate malformed input and return NULL instead. SQLSTATE: 22018\", \"context\": {\"file\": \"line 7 in cell [42]\", \"line\": \"\", \"fragment\": \"cast\", \"errorClass\": \"CAST_INVALID_INPUT\"}, \"exception\": {\"class\": \"Py4JJavaError\", \"msg\": \"An error occurred while calling o1024.showString.\\n: org.apache.spark.SparkNumberFormatException: [CAST_INVALID_INPUT] The value 'invalid' of the type \\\"STRING\\\" cannot be cast to \\\"DOUBLE\\\" because it is malformed. Correct the value as per the syntax, or change its target type. Use `try_cast` to tolerate malformed input and return NULL instead. SQLSTATE: 22018\\n== DataFrame ==\\n\\\"cast\\\" was called from\\nline 7 in cell [42]\\n\\n\\tat org.apache.spark.sql.errors.QueryExecutionErrors$.invalidInputInCastToNumberError(QueryExecutionErrors.scala:145)\\n\\tat org.apache.spark.sql.errors.QueryExecutionErrors.invalidInputInCastToNumberError(QueryExecutionErrors.scala)\\n\\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\\n\\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\\n\\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\\n\\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:402)\\n\\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:901)\\n\\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:901)\\n\\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\\n\\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\\n\\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\\n\\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\\n\\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\\n\\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\\n\\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\\n\\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\\n\\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\\n\\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)\\n\\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)\\n\\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\\n\\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\\n\\tat java.base/java.lang.Thread.run(Thread.java:840)\\n\\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:1009)\\n\\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2484)\\n\\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2505)\\n\\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2524)\\n\\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:544)\\n\\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:497)\\n\\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:58)\\n\\tat org.apache.spark.sql.classic.Dataset.collectFromPlan(Dataset.scala:2244)\\n\\tat org.apache.spark.sql.classic.Dataset.$anonfun$head$1(Dataset.scala:1379)\\n\\tat org.apache.spark.sql.classic.Dataset.$anonfun$withAction$2(Dataset.scala:2234)\\n\\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)\\n\\tat org.apache.spark.sql.classic.Dataset.$anonfun$withAction$1(Dataset.scala:2232)\\n\\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$8(SQLExecution.scala:163)\\n\\tat org.apache.spark.sql.execution.SQLExecution$.withSessionTagsApplied(SQLExecution.scala:272)\\n\\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$7(SQLExecution.scala:125)\\n\\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\\n\\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:112)\\n\\tat org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:106)\\n\\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:111)\\n\\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$6(SQLExecution.scala:125)\\n\\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:295)\\n\\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:124)\\n\\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\\n\\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:78)\\n\\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:237)\\n\\tat org.apache.spark.sql.classic.Dataset.withAction(Dataset.scala:2232)\\n\\tat org.apache.spark.sql.classic.Dataset.head(Dataset.scala:1379)\\n\\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2810)\\n\\tat org.apache.spark.sql.classic.Dataset.getRows(Dataset.scala:339)\\n\\tat org.apache.spark.sql.classic.Dataset.showString(Dataset.scala:375)\\n\\tat jdk.internal.reflect.GeneratedMethodAccessor89.invoke(Unknown Source)\\n\\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\\n\\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\\n\\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\\n\\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\\n\\tat py4j.Gateway.invoke(Gateway.java:282)\\n\\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\\n\\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\\n\\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\\n\\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)\\n\\tat java.base/java.lang.Thread.run(Thread.java:840)\\n\", \"stacktrace\": [{\"class\": null, \"method\": \"deco\", \"file\": \"/usr/local/lib/python3.12/dist-packages/pyspark/errors/exceptions/captured.py\", \"line\": \"282\"}, {\"class\": null, \"method\": \"get_return_value\", \"file\": \"/usr/local/lib/python3.12/dist-packages/py4j/protocol.py\", \"line\": \"327\"}]}}\n"
          ]
        },
        {
          "ename": "NumberFormatException",
          "evalue": "[CAST_INVALID_INPUT] The value 'invalid' of the type \"STRING\" cannot be cast to \"DOUBLE\" because it is malformed. Correct the value as per the syntax, or change its target type. Use `try_cast` to tolerate malformed input and return NULL instead. SQLSTATE: 22018\n== DataFrame ==\n\"cast\" was called from\nline 7 in cell [42]\n",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNumberFormatException\u001b[0m                     Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2574316115.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Salary cleaning and casting:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mdf_salary_clean\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"salary\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"salary_cleaned\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"salary_double\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pyspark/sql/classic/dataframe.py\u001b[0m in \u001b[0;36mshow\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    283\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 285\u001b[0;31m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_show_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    286\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m     def _show_string(\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pyspark/sql/classic/dataframe.py\u001b[0m in \u001b[0;36m_show_string\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    301\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 303\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    304\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    305\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1360\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1361\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1362\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1363\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pyspark/errors/exceptions/captured.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    286\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 288\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    289\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    290\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNumberFormatException\u001b[0m: [CAST_INVALID_INPUT] The value 'invalid' of the type \"STRING\" cannot be cast to \"DOUBLE\" because it is malformed. Correct the value as per the syntax, or change its target type. Use `try_cast` to tolerate malformed input and return NULL instead. SQLSTATE: 22018\n== DataFrame ==\n\"cast\" was called from\nline 7 in cell [42]\n"
          ]
        }
      ],
      "source": [
        "# Clean salary string before casting\n",
        "df_salary_clean = df_messy.withColumn(\n",
        "    \"salary_cleaned\",\n",
        "    regexp_replace(col(\"salary\"), \"[$,]\", \"\")\n",
        ").withColumn(\n",
        "    \"salary_double\",\n",
        "    col(\"salary_cleaned\").cast(DoubleType())\n",
        ")\n",
        "\n",
        "print(\"Salary cleaning and casting:\")\n",
        "df_salary_clean.select(\"salary\", \"salary_cleaned\", \"salary_double\").show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "boolean_convert"
      },
      "outputs": [],
      "source": [
        "# Boolean conversion with multiple representations\n",
        "df_bool = df_messy.withColumn(\n",
        "    \"active_bool\",\n",
        "    when(lower(col(\"active\")).isin(\"true\", \"yes\", \"1\", \"t\", \"y\"), True)\n",
        "    .when(lower(col(\"active\")).isin(\"false\", \"no\", \"0\", \"f\", \"n\"), False)\n",
        "    .otherwise(None)\n",
        ")\n",
        "\n",
        "print(\"Boolean conversion:\")\n",
        "df_bool.select(\"active\", \"active_bool\").show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "date_handling"
      },
      "source": [
        "## 3.3 Handling Date and Timestamp Fields"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "date_examples"
      },
      "outputs": [],
      "source": [
        "# Common date formats\n",
        "date_examples = [\n",
        "    (\"2024-01-15\", \"ISO format\"),\n",
        "    (\"15/02/2024\", \"European DD/MM/YYYY\"),\n",
        "    (\"03/20/2024\", \"American MM/DD/YYYY\"),\n",
        "    (\"March 25, 2024\", \"Written format\"),\n",
        "    (\"2024.04.30\", \"Dot separator\"),\n",
        "]\n",
        "\n",
        "df_dates = spark.createDataFrame(date_examples, [\"date_string\", \"format_name\"])\n",
        "df_dates.show(truncate=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "parse_dates"
      },
      "outputs": [],
      "source": [
        "# Parse dates with multiple format patterns using coalesce\n",
        "df_parsed = df_dates.withColumn(\n",
        "    \"parsed_date\",\n",
        "    coalesce(\n",
        "        to_date(col(\"date_string\"), \"yyyy-MM-dd\"),\n",
        "        to_date(col(\"date_string\"), \"dd/MM/yyyy\"),\n",
        "        to_date(col(\"date_string\"), \"MM/dd/yyyy\"),\n",
        "        to_date(col(\"date_string\"), \"MMMM dd, yyyy\"),\n",
        "        to_date(col(\"date_string\"), \"yyyy.MM.dd\")\n",
        "    )\n",
        ")\n",
        "\n",
        "print(\"Dates parsed from multiple formats:\")\n",
        "df_parsed.show(truncate=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "date_components"
      },
      "outputs": [],
      "source": [
        "# Extract components from dates\n",
        "from pyspark.sql.functions import year, month, dayofmonth, dayofweek, quarter, date_format\n",
        "\n",
        "df_date_parts = df_parsed.filter(col(\"parsed_date\").isNotNull()).select(\n",
        "    col(\"date_string\"),\n",
        "    col(\"parsed_date\"),\n",
        "    year(col(\"parsed_date\")).alias(\"year\"),\n",
        "    month(col(\"parsed_date\")).alias(\"month\"),\n",
        "    dayofmonth(col(\"parsed_date\")).alias(\"day\"),\n",
        "    quarter(col(\"parsed_date\")).alias(\"quarter\"),\n",
        "    date_format(col(\"parsed_date\"), \"EEEE\").alias(\"day_name\")\n",
        ")\n",
        "\n",
        "print(\"Date components extracted:\")\n",
        "df_date_parts.show(truncate=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "schema_validation"
      },
      "source": [
        "## 3.4 Schema Validation and Enforcement"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "define_schema"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Expected Schema:\n",
            "  customer_id: IntegerType() (nullable=False)\n",
            "  name: StringType() (nullable=True)\n",
            "  age: IntegerType() (nullable=True)\n",
            "  email: StringType() (nullable=True)\n",
            "  salary: DoubleType() (nullable=True)\n",
            "  city: StringType() (nullable=True)\n",
            "  signup_date: DateType() (nullable=True)\n",
            "  testing2: StringType() (nullable=True)\n"
          ]
        }
      ],
      "source": [
        "# Define an expected schema\n",
        "expected_schema = StructType([\n",
        "    StructField(\"customer_id\", IntegerType(), nullable=False),\n",
        "    StructField(\"name\", StringType(), nullable=True),\n",
        "    StructField(\"age\", IntegerType(), nullable=True),\n",
        "    StructField(\"email\", StringType(), nullable=True),\n",
        "    StructField(\"salary\", DoubleType(), nullable=True),\n",
        "    StructField(\"city\", StringType(), nullable=True),\n",
        "    StructField(\"signup_date\", DateType(), nullable=True),\n",
        "    StructField(\"testing2\", StringType(), nullable=True)\n",
        "])\n",
        "\n",
        "print(\"Expected Schema:\")\n",
        "for field in expected_schema.fields:\n",
        "    print(f\"  {field.name}: {field.dataType} (nullable={field.nullable})\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----------+------------+----+-----------------+-------+-----------+-----------+\n",
            "|customer_id|        name| age|            email| salary|       city|signup_date|\n",
            "+-----------+------------+----+-----------------+-------+-----------+-----------+\n",
            "|          1|  John Smith|  28|   john@email.com|75000.0|   New York| 2023-01-15|\n",
            "|          2|    Jane Doe|NULL|   jane@email.com|82000.0|Los Angeles| 2023-02-20|\n",
            "|          3|  Bob Wilson|  35|             NULL|65000.0|    Chicago| 2023-03-10|\n",
            "|          4| Alice Brown|  42|  alice@email.com|   NULL|    Houston|       NULL|\n",
            "|          5|        NULL|  31|unknown@email.com|70000.0|    Phoenix| 2023-05-01|\n",
            "|          6| Charlie Lee|  29|                 |58000.0|           | 2023-06-15|\n",
            "|          7|Diana Prince|NULL|  diana@email.com|95000.0|       NULL| 2023-07-20|\n",
            "|          8|  Edward Kim|  38|              N/A|    NaN|     Boston| 2023-08-25|\n",
            "|          9|Fiona Garcia|  45|  fiona@email.com|88000.0|    Seattle| 2023-09-30|\n",
            "|         10| George Hall|NULL|             NULL|   NULL|       NULL|       NULL|\n",
            "|         11|Hannah White|  33| hannah@email.com|72000.0|     Denver| 2023-11-10|\n",
            "|         12| Ivan Torres|  27|   ivan@email.com|63000.0|      Miami| 2023-12-05|\n",
            "+-----------+------------+----+-----------------+-------+-----------+-----------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df_customers.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "enforce_schema"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Schema-enforced DataFrame:\n",
            "root\n",
            " |-- customer_id: integer (nullable = true)\n",
            " |-- name: string (nullable = true)\n",
            " |-- age: integer (nullable = true)\n",
            " |-- email: string (nullable = true)\n",
            " |-- salary: double (nullable = true)\n",
            " |-- city: string (nullable = true)\n",
            " |-- signup_date: date (nullable = true)\n",
            " |-- testing2: string (nullable = false)\n",
            "\n",
            "+-----------+-----------+----+-----------------+-------+-----------+-----------+--------+\n",
            "|customer_id|       name| age|            email| salary|       city|signup_date|testing2|\n",
            "+-----------+-----------+----+-----------------+-------+-----------+-----------+--------+\n",
            "|          1| John Smith|  28|   john@email.com|75000.0|   New York| 2023-01-15|    Maaz|\n",
            "|          2|   Jane Doe|NULL|   jane@email.com|82000.0|Los Angeles| 2023-02-20|    Maaz|\n",
            "|          3| Bob Wilson|  35|             NULL|65000.0|    Chicago| 2023-03-10|    Maaz|\n",
            "|          4|Alice Brown|  42|  alice@email.com|   NULL|    Houston|       NULL|    Maaz|\n",
            "|          5|       NULL|  31|unknown@email.com|70000.0|    Phoenix| 2023-05-01|    Maaz|\n",
            "+-----------+-----------+----+-----------------+-------+-----------+-----------+--------+\n",
            "only showing top 5 rows\n"
          ]
        }
      ],
      "source": [
        "# Function to validate and transform data to match expected schema\n",
        "def enforce_schema(df, target_schema):\n",
        "    result_df = df\n",
        "    for field in target_schema.fields:\n",
        "        col_name = field.name\n",
        "        col_type = field.dataType\n",
        "        if col_name in df.columns:\n",
        "            result_df = result_df.withColumn(col_name, col(col_name).cast(col_type))\n",
        "        else:\n",
        "            result_df = result_df.withColumn(col_name, lit(\"Maaz\").cast(col_type))\n",
        "    return result_df.select([field.name for field in target_schema.fields])\n",
        "\n",
        "df_enforced = enforce_schema(df_customers, expected_schema)\n",
        "print(\"Schema-enforced DataFrame:\")\n",
        "df_enforced.printSchema()\n",
        "df_enforced.show(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "part4_intro"
      },
      "source": [
        "---\n",
        "# Part 4: Text Preprocessing\n",
        "---\n",
        "\n",
        "## 4.1 String Functions in Spark\n",
        "\n",
        "Text data often requires extensive preprocessing before analysis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "sample_text_data"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sample text data:\n",
            "+---+--------------------+------------------------+\n",
            "|id |text                |email                   |\n",
            "+---+--------------------+------------------------+\n",
            "|1  |   Hello World      |john.doe@email.com      |\n",
            "|2  |APACHE SPARK        |JANE.DOE@EMAIL.COM      |\n",
            "|3  |data science        |bob_wilson@company.co.uk|\n",
            "|4  |  Machine Learning  |alice-brown@domain.org  |\n",
            "|5  |NLP is Great!!!     |charlie.lee123@test.com |\n",
            "+---+--------------------+------------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Create sample text data\n",
        "text_data = [\n",
        "    (1, \"   Hello World   \", \"john.doe@email.com\"),\n",
        "    (2, \"APACHE SPARK\", \"JANE.DOE@EMAIL.COM\"),\n",
        "    (3, \"data science\", \"bob_wilson@company.co.uk\"),\n",
        "    (4, \"  Machine Learning  \", \"alice-brown@domain.org\"),\n",
        "    (5, \"NLP is Great!!!\", \"charlie.lee123@test.com\"),\n",
        "]\n",
        "\n",
        "df_text = spark.createDataFrame(text_data, [\"id\", \"text\", \"email\"])\n",
        "print(\"Sample text data:\")\n",
        "df_text.show(truncate=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "case_normalization"
      },
      "source": [
        "## 4.2 Case Normalization and Trimming"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "case_conversion"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Case conversion:\n",
            "+--------------------+--------------------+--------------------+--------------------+\n",
            "|text                |lowercase           |uppercase           |titlecase           |\n",
            "+--------------------+--------------------+--------------------+--------------------+\n",
            "|   Hello World      |   hello world      |   HELLO WORLD      |   Hello World      |\n",
            "|APACHE SPARK        |apache spark        |APACHE SPARK        |Apache Spark        |\n",
            "|data science        |data science        |DATA SCIENCE        |Data Science        |\n",
            "|  Machine Learning  |  machine learning  |  MACHINE LEARNING  |  Machine Learning  |\n",
            "|NLP is Great!!!     |nlp is great!!!     |NLP IS GREAT!!!     |Nlp Is Great!!!     |\n",
            "+--------------------+--------------------+--------------------+--------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Case conversion\n",
        "df_case = df_text.select(\n",
        "    col(\"text\"),\n",
        "    lower(col(\"text\")).alias(\"lowercase\"),\n",
        "    upper(col(\"text\")).alias(\"uppercase\"),\n",
        "    expr(\"initcap(text)\").alias(\"titlecase\")\n",
        ")\n",
        "\n",
        "print(\"Case conversion:\")\n",
        "df_case.show(truncate=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "trimming"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Whitespace trimming:\n",
            "+--------------------+----------------+------------------+------------------+------------+-----------+\n",
            "|text                |trimmed         |left_trimmed      |right_trimmed     |original_len|trimmed_len|\n",
            "+--------------------+----------------+------------------+------------------+------------+-----------+\n",
            "|   Hello World      |Hello World     |Hello World       |   Hello World    |17          |11         |\n",
            "|APACHE SPARK        |APACHE SPARK    |APACHE SPARK      |APACHE SPARK      |12          |12         |\n",
            "|data science        |data science    |data science      |data science      |12          |12         |\n",
            "|  Machine Learning  |Machine Learning|Machine Learning  |  Machine Learning|20          |16         |\n",
            "|NLP is Great!!!     |NLP is Great!!! |NLP is Great!!!   |NLP is Great!!!   |15          |15         |\n",
            "+--------------------+----------------+------------------+------------------+------------+-----------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Trimming whitespace\n",
        "df_trim = df_text.select(\n",
        "    col(\"text\"),\n",
        "    trim(col(\"text\")).alias(\"trimmed\"),\n",
        "    ltrim(col(\"text\")).alias(\"left_trimmed\"),\n",
        "    rtrim(col(\"text\")).alias(\"right_trimmed\"),\n",
        "    length(col(\"text\")).alias(\"original_len\"),\n",
        "    length(trim(col(\"text\"))).alias(\"trimmed_len\")\n",
        ")\n",
        "\n",
        "print(\"Whitespace trimming:\")\n",
        "df_trim.show(truncate=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tokenization"
      },
      "source": [
        "## 4.3 Tokenization and Text Splitting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "sentences_data"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---+------------------------------------------------------------------+\n",
            "|id |sentence                                                          |\n",
            "+---+------------------------------------------------------------------+\n",
            "|1  |Apache Spark is a unified analytics engine for big data processing|\n",
            "|2  |Natural language processing enables computers to understand text  |\n",
            "|3  |Machine learning models require clean preprocessed data           |\n",
            "|4  |Text mining extracts valuable insights from unstructured data     |\n",
            "+---+------------------------------------------------------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Create sample sentences\n",
        "sentences_data = [\n",
        "    (1, \"Apache Spark is a unified analytics engine for big data processing\"),\n",
        "    (2, \"Natural language processing enables computers to understand text\"),\n",
        "    (3, \"Machine learning models require clean preprocessed data\"),\n",
        "    (4, \"Text mining extracts valuable insights from unstructured data\"),\n",
        "]\n",
        "\n",
        "df_sentences = spark.createDataFrame(sentences_data, [\"id\", \"sentence\"])\n",
        "df_sentences.show(truncate=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "basic_tokenization"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tokenized sentences:\n",
            "+---+------------------------------------------------------------------+------------------------------------------------------------------------------+----------+\n",
            "|id |sentence                                                          |words                                                                         |word_count|\n",
            "+---+------------------------------------------------------------------+------------------------------------------------------------------------------+----------+\n",
            "|1  |Apache Spark is a unified analytics engine for big data processing|[apache, spark, is, a, unified, analytics, engine, for, big, data, processing]|11        |\n",
            "|2  |Natural language processing enables computers to understand text  |[natural, language, processing, enables, computers, to, understand, text]     |8         |\n",
            "|3  |Machine learning models require clean preprocessed data           |[machine, learning, models, require, clean, preprocessed, data]               |7         |\n",
            "|4  |Text mining extracts valuable insights from unstructured data     |[text, mining, extracts, valuable, insights, from, unstructured, data]        |8         |\n",
            "+---+------------------------------------------------------------------+------------------------------------------------------------------------------+----------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Basic tokenization using split\n",
        "from pyspark.sql.functions import split, size, explode\n",
        "\n",
        "df_tokens = df_sentences.withColumn(\n",
        "    \"words\",\n",
        "    split(lower(col(\"sentence\")), \" \")\n",
        ").withColumn(\n",
        "    \"word_count\",\n",
        "    size(col(\"words\"))\n",
        ")\n",
        "\n",
        "print(\"Tokenized sentences:\")\n",
        "df_tokens.show(truncate=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "word_frequency"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Word frequencies:\n",
            "+----------+---------+\n",
            "|      word|frequency|\n",
            "+----------+---------+\n",
            "|      data|        3|\n",
            "|      text|        2|\n",
            "|processing|        2|\n",
            "| computers|        1|\n",
            "|       for|        1|\n",
            "|   natural|        1|\n",
            "|        is|        1|\n",
            "|    apache|        1|\n",
            "|     spark|        1|\n",
            "|  language|        1|\n",
            "| analytics|        1|\n",
            "|   enables|        1|\n",
            "|understand|        1|\n",
            "|         a|        1|\n",
            "|   unified|        1|\n",
            "+----------+---------+\n",
            "only showing top 15 rows\n"
          ]
        }
      ],
      "source": [
        "# Explode tokens and calculate word frequency\n",
        "df_exploded = df_tokens.select(\n",
        "    col(\"id\"),\n",
        "    explode(col(\"words\")).alias(\"word\")\n",
        ")\n",
        "# df_exploded.show()\n",
        "word_freq = df_exploded.groupBy(\"word\").agg(\n",
        "    count(\"*\").alias(\"frequency\")\n",
        ").orderBy(col(\"frequency\").desc())\n",
        "\n",
        "print(\"Word frequencies:\")\n",
        "word_freq.show(15)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "stop_words"
      },
      "source": [
        "## 4.4 Stop Word Removal"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "remove_stopwords"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Words with stop words removed:\n",
            "+------------------------------------------------------------------+------------------------------------------------------------------------------+---------------------------------------------------------------------+-------------------------------------------------------------+\n",
            "|sentence                                                          |words                                                                         |words_filtered                                                       |words_filtered_sentence                                      |\n",
            "+------------------------------------------------------------------+------------------------------------------------------------------------------+---------------------------------------------------------------------+-------------------------------------------------------------+\n",
            "|Apache Spark is a unified analytics engine for big data processing|[apache, spark, is, a, unified, analytics, engine, for, big, data, processing]|[apache, spark, unified, analytics, engine, big, data, processing]   |apache spark unified analytics engine big data processing    |\n",
            "|Natural language processing enables computers to understand text  |[natural, language, processing, enables, computers, to, understand, text]     |[natural, language, processing, enables, computers, understand, text]|natural language processing enables computers understand text|\n",
            "|Machine learning models require clean preprocessed data           |[machine, learning, models, require, clean, preprocessed, data]               |[machine, learning, models, require, clean, preprocessed, data]      |machine learning models require clean preprocessed data      |\n",
            "|Text mining extracts valuable insights from unstructured data     |[text, mining, extracts, valuable, insights, from, unstructured, data]        |[text, mining, extracts, valuable, insights, unstructured, data]     |text mining extracts valuable insights unstructured data     |\n",
            "+------------------------------------------------------------------+------------------------------------------------------------------------------+---------------------------------------------------------------------+-------------------------------------------------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Define common English stop words\n",
        "stop_words = [\n",
        "    \"a\", \"an\", \"the\", \"and\", \"or\", \"but\", \"is\", \"are\", \"was\", \"were\",\n",
        "    \"to\", \"of\", \"in\", \"for\", \"on\", \"with\", \"at\", \"by\", \"from\",\n",
        "    \"it\", \"its\", \"this\", \"that\", \"these\", \"those\",\n",
        "    \"be\", \"been\", \"being\", \"have\", \"has\", \"had\"\n",
        "]\n",
        "\n",
        "from pyspark.sql.functions import array_except, array\n",
        "\n",
        "#converting into array for spark to use\n",
        "stop_words_array = array([lit(w) for w in stop_words])\n",
        "\n",
        "df_no_stopwords = df_tokens.withColumn(\n",
        "    \"words_filtered\",\n",
        "    array_except(col(\"words\"), stop_words_array)\n",
        ").withColumn(\n",
        "    \"words_filtered_sentence\",\n",
        "    concat_ws(\" \",col(\"words_filtered\"))\n",
        ")\n",
        "\n",
        "print(\"Words with stop words removed:\")\n",
        "df_no_stopwords.select(\"sentence\", \"words\", \"words_filtered\",\"words_filtered_sentence\").show(truncate=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "ml_stopwords"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using Spark ML StopWordsRemover:\n",
            "+------------------------------------------------------------------+------------------------------------------------------------------------------+---------------------------------------------------------------------+\n",
            "|sentence                                                          |words_ml                                                                      |words_cleaned                                                        |\n",
            "+------------------------------------------------------------------+------------------------------------------------------------------------------+---------------------------------------------------------------------+\n",
            "|Apache Spark is a unified analytics engine for big data processing|[apache, spark, is, a, unified, analytics, engine, for, big, data, processing]|[apache, spark, unified, analytics, engine, big, data, processing]   |\n",
            "|Natural language processing enables computers to understand text  |[natural, language, processing, enables, computers, to, understand, text]     |[natural, language, processing, enables, computers, understand, text]|\n",
            "|Machine learning models require clean preprocessed data           |[machine, learning, models, require, clean, preprocessed, data]               |[machine, learning, models, require, clean, preprocessed, data]      |\n",
            "|Text mining extracts valuable insights from unstructured data     |[text, mining, extracts, valuable, insights, from, unstructured, data]        |[text, mining, extracts, valuable, insights, unstructured, data]     |\n",
            "+------------------------------------------------------------------+------------------------------------------------------------------------------+---------------------------------------------------------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Using Spark ML's StopWordsRemover\n",
        "from pyspark.ml.feature import StopWordsRemover, Tokenizer\n",
        "\n",
        "tokenizer = Tokenizer(inputCol=\"sentence\", outputCol=\"words_ml\")\n",
        "df_tokenized_ml = tokenizer.transform(df_sentences)\n",
        "\n",
        "remover = StopWordsRemover(inputCol=\"words_ml\", outputCol=\"words_cleaned\")\n",
        "df_cleaned_ml = remover.transform(df_tokenized_ml)\n",
        "\n",
        "print(\"Using Spark ML StopWordsRemover:\")\n",
        "df_cleaned_ml.select(\"sentence\", \"words_ml\", \"words_cleaned\").show(truncate=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "text_pipeline"
      },
      "source": [
        "## 4.5 Text Cleaning Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "reviews_data"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Raw reviews:\n",
            "+---+---------------------------------------+------+\n",
            "|id |review                                 |rating|\n",
            "+---+---------------------------------------+------+\n",
            "|1  |  GREAT product!!!  Would buy again.   |5     |\n",
            "|2  |Terrible... don't waste your money     |1     |\n",
            "|3  |Good quality, fast shipping!!!         |4     |\n",
            "|4  |   not bad, could be better...         |3     |\n",
            "|5  |AMAZING!!!!! Best purchase EVER!!!!!   |5     |\n",
            "|6  |meh, it's OK I guess                   |3     |\n",
            "|7  |Product arrived damaged                |1     |\n",
            "|8  |10/10 would recommend to friends!      |5     |\n",
            "+---+---------------------------------------+------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Sample product reviews (messy text data)\n",
        "reviews_data = [\n",
        "    (1, \"  GREAT product!!!  Would buy again.   \", 5),\n",
        "    (2, \"Terrible... don't waste your money \", 1),\n",
        "    (3, \"Good quality, fast shipping!!!\", 4),\n",
        "    (4, \"   not bad, could be better...   \", 3),\n",
        "    (5, \"AMAZING!!!!! Best purchase EVER!!!!!\", 5),\n",
        "    (6, \"meh, it's OK I guess\", 3),\n",
        "    (7, \"Product arrived damaged\", 1),\n",
        "    (8, \"10/10 would recommend to friends!\", 5),\n",
        "]\n",
        "\n",
        "df_reviews = spark.createDataFrame(reviews_data, [\"id\", \"review\", \"rating\"])\n",
        "print(\"Raw reviews:\")\n",
        "df_reviews.show(truncate=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "text_pipeline_function"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cleaned reviews:\n",
            "+---------------------------------------+-------------------------------+\n",
            "|review                                 |review_cleaned                 |\n",
            "+---------------------------------------+-------------------------------+\n",
            "|  GREAT product!!!  Would buy again.   |great product would buy again  |\n",
            "|Terrible... don't waste your money     |terrible dont waste your money |\n",
            "|Good quality, fast shipping!!!         |good quality fast shipping     |\n",
            "|   not bad, could be better...         |not bad could be better        |\n",
            "|AMAZING!!!!! Best purchase EVER!!!!!   |amazing best purchase ever     |\n",
            "|meh, it's OK I guess                   |meh its ok i guess             |\n",
            "|Product arrived damaged                |product arrived damaged        |\n",
            "|10/10 would recommend to friends!      |1010 would recommend to friends|\n",
            "+---------------------------------------+-------------------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "def clean_text_pipeline(df, text_col, output_col=\"text_cleaned\"):\n",
        "    result = df \\\n",
        "        .withColumn(\"_step1_trim\", trim(col(text_col))) \\\n",
        "        .withColumn(\"_step2_lower\", lower(col(\"_step1_trim\"))) \\\n",
        "        .withColumn(\"_step3_alphanum\",\n",
        "                    regexp_replace(col(\"_step2_lower\"), \"[^a-z0-9\\\\s]\", \"\")) \\\n",
        "        .withColumn(\"_step4_spaces\",\n",
        "                    regexp_replace(col(\"_step3_alphanum\"), \"\\\\s+\", \" \")) \\\n",
        "        .withColumn(output_col, trim(col(\"_step4_spaces\"))) \\\n",
        "        .drop(\"_step1_trim\", \"_step2_lower\", \"_step3_alphanum\", \"_step4_spaces\")\n",
        "    return result\n",
        "\n",
        "df_reviews_cleaned = clean_text_pipeline(df_reviews, \"review\", \"review_cleaned\")\n",
        "\n",
        "print(\"Cleaned reviews:\")\n",
        "df_reviews_cleaned.select(\"review\", \"review_cleaned\").show(truncate=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "exercise_text"
      },
      "source": [
        "## 4.6 Practical Exercise: Product Review Cleaning\n",
        "\n",
        "**Tasks:**\n",
        "1. Clean the review text (remove special chars, normalize case, trim)\n",
        "2. Extract the word count from each cleaned review\n",
        "3. Identify reviews that mention negative keywords (return, refund, broken)\n",
        "4. Calculate the average rating for reviews with negative keywords vs others"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "extended_reviews"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---+----------------------------------------------------------+------+\n",
            "|id |review                                                    |rating|\n",
            "+---+----------------------------------------------------------+------+\n",
            "|1  |LOVE IT! Fast shipping, great quality. Will order more!   |5     |\n",
            "|2  |Product broke after 1 week. Requesting refund immediately.|1     |\n",
            "|3  |Decent product for the price. Nothing special but works.  |3     |\n",
            "|4  |DO NOT BUY! Cheap quality, had to return it!              |1     |\n",
            "|5  |Excellent! Better than expected. 100% recommend.          |5     |\n",
            "|6  |Item was damaged during shipping. Broken on arrival.      |1     |\n",
            "|7  |Good value for money. Satisfied with purchase.            |4     |\n",
            "|8  |WORST PURCHASE EVER! Want my money back!!!                |1     |\n",
            "|9  |Nice product, works as described. Happy customer.         |4     |\n",
            "|10 |Meh, it's okay. Nothing to complain about.                |3     |\n",
            "+---+----------------------------------------------------------+------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Extended reviews dataset\n",
        "extended_reviews = [\n",
        "    (1, \"LOVE IT! Fast shipping, great quality. Will order more!\", 5),\n",
        "    (2, \"Product broke after 1 week. Requesting refund immediately.\", 1),\n",
        "    (3, \"Decent product for the price. Nothing special but works.\", 3),\n",
        "    (4, \"DO NOT BUY! Cheap quality, had to return it!\", 1),\n",
        "    (5, \"Excellent! Better than expected. 100% recommend.\", 5),\n",
        "    (6, \"Item was damaged during shipping. Broken on arrival.\", 1),\n",
        "    (7, \"Good value for money. Satisfied with purchase.\", 4),\n",
        "    (8, \"WORST PURCHASE EVER! Want my money back!!!\", 1),\n",
        "    (9, \"Nice product, works as described. Happy customer.\", 4),\n",
        "    (10, \"Meh, it's okay. Nothing to complain about.\", 3),\n",
        "]\n",
        "\n",
        "df_extended = spark.createDataFrame(extended_reviews, [\"id\", \"review\", \"rating\"])\n",
        "df_extended.show(truncate=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Tasks:**\n",
        "1. Clean the review text (remove special chars, normalize case, trim)\n",
        "2. Extract the word count from each cleaned review\n",
        "3. Identify reviews that mention negative keywords (return, refund, broken)\n",
        "4. Calculate the average rating for reviews with negative keywords vs others"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "exercise_text_code"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---+----------------------------------------------------------+------+--------------------------------------------------------+\n",
            "|id |review                                                    |rating|cleaned_review                                          |\n",
            "+---+----------------------------------------------------------+------+--------------------------------------------------------+\n",
            "|1  |LOVE IT! Fast shipping, great quality. Will order more!   |5     |love it fast shipping great quality will order more     |\n",
            "|2  |Product broke after 1 week. Requesting refund immediately.|1     |product broke after 1 week requesting refund immediately|\n",
            "|3  |Decent product for the price. Nothing special but works.  |3     |decent product for the price nothing special but works  |\n",
            "|4  |DO NOT BUY! Cheap quality, had to return it!              |1     |do not buy cheap quality had to return it               |\n",
            "|5  |Excellent! Better than expected. 100% recommend.          |5     |excellent better than expected 100 recommend            |\n",
            "|6  |Item was damaged during shipping. Broken on arrival.      |1     |item was damaged during shipping broken on arrival      |\n",
            "|7  |Good value for money. Satisfied with purchase.            |4     |good value for money satisfied with purchase            |\n",
            "|8  |WORST PURCHASE EVER! Want my money back!!!                |1     |worst purchase ever want my money back                  |\n",
            "|9  |Nice product, works as described. Happy customer.         |4     |nice product works as described happy customer          |\n",
            "|10 |Meh, it's okay. Nothing to complain about.                |3     |meh it s okay nothing to complain about                 |\n",
            "+---+----------------------------------------------------------+------+--------------------------------------------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql.functions import col, regexp_replace, lower, trim\n",
        "\n",
        "cleaned_df = df_extended.withColumn(\n",
        "    \"step1_no_chars\",\n",
        "    # 1. Replace punctuation with a space (keeps words from sticking together)\n",
        "    # We moved the hyphen to the end to avoid range errors\n",
        "    regexp_replace(col(\"review\"), r\"[$&+,:;=?@#|'<>._^*()%!+-]\", \" \") \n",
        ").withColumn(\n",
        "    \"step2_single_space\",\n",
        "    # 2. Collapse \"one or more\" spaces (including the ones we just created) into ONE\n",
        "    regexp_replace(col(\"step1_no_chars\"), r\"\\s+\", \" \")\n",
        ").withColumn(\n",
        "    \"cleaned_review\",\n",
        "    # 3. Final polish: Lowercase and Trim the very ends\n",
        "    trim(lower(col(\"step2_single_space\")))\n",
        ").drop(\"step1_no_chars\", \"step2_single_space\")\n",
        "\n",
        "cleaned_df.show(truncate=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Tasks:**\n",
        "2. Extract the word count from each cleaned review"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+--------------------+----------+--------------------+\n",
            "|      cleaned_review|word_count|          word_array|\n",
            "+--------------------+----------+--------------------+\n",
            "|love it fast ship...|         9|[love, it, fast, ...|\n",
            "|product broke aft...|         8|[product, broke, ...|\n",
            "|decent product fo...|         9|[decent, product,...|\n",
            "|do not buy cheap ...|         9|[do, not, buy, ch...|\n",
            "|excellent better ...|         6|[excellent, bette...|\n",
            "|item was damaged ...|         8|[item, was, damag...|\n",
            "|good value for mo...|         7|[good, value, for...|\n",
            "|worst purchase ev...|         7|[worst, purchase,...|\n",
            "|nice product work...|         7|[nice, product, w...|\n",
            "|meh it s okay not...|         8|[meh, it, s, okay...|\n",
            "+--------------------+----------+--------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql.functions import col, split, size\n",
        "\n",
        "df_with_counts = cleaned_df.withColumn(\n",
        "    \"word_count\",\n",
        "    size(split(col(\"cleaned_review\"), \" \"))\n",
        ").withColumn(\n",
        "    \"word_array\",\n",
        "    split(trim(col(\"cleaned_review\")),\" \")\n",
        ")\n",
        "\n",
        "df_with_counts.select(\"cleaned_review\", \"word_count\",\"word_array\").show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---+----------------------------------------------------------+------+--------------------------------------------------------+----------+-----------------------------------------------------------------+-----------+\n",
            "|id |review                                                    |rating|cleaned_review                                          |word_count|word_array                                                       |word       |\n",
            "+---+----------------------------------------------------------+------+--------------------------------------------------------+----------+-----------------------------------------------------------------+-----------+\n",
            "|1  |LOVE IT! Fast shipping, great quality. Will order more!   |5     |love it fast shipping great quality will order more     |9         |[love, it, fast, shipping, great, quality, will, order, more]    |love       |\n",
            "|1  |LOVE IT! Fast shipping, great quality. Will order more!   |5     |love it fast shipping great quality will order more     |9         |[love, it, fast, shipping, great, quality, will, order, more]    |it         |\n",
            "|1  |LOVE IT! Fast shipping, great quality. Will order more!   |5     |love it fast shipping great quality will order more     |9         |[love, it, fast, shipping, great, quality, will, order, more]    |fast       |\n",
            "|1  |LOVE IT! Fast shipping, great quality. Will order more!   |5     |love it fast shipping great quality will order more     |9         |[love, it, fast, shipping, great, quality, will, order, more]    |shipping   |\n",
            "|1  |LOVE IT! Fast shipping, great quality. Will order more!   |5     |love it fast shipping great quality will order more     |9         |[love, it, fast, shipping, great, quality, will, order, more]    |great      |\n",
            "|1  |LOVE IT! Fast shipping, great quality. Will order more!   |5     |love it fast shipping great quality will order more     |9         |[love, it, fast, shipping, great, quality, will, order, more]    |quality    |\n",
            "|1  |LOVE IT! Fast shipping, great quality. Will order more!   |5     |love it fast shipping great quality will order more     |9         |[love, it, fast, shipping, great, quality, will, order, more]    |will       |\n",
            "|1  |LOVE IT! Fast shipping, great quality. Will order more!   |5     |love it fast shipping great quality will order more     |9         |[love, it, fast, shipping, great, quality, will, order, more]    |order      |\n",
            "|1  |LOVE IT! Fast shipping, great quality. Will order more!   |5     |love it fast shipping great quality will order more     |9         |[love, it, fast, shipping, great, quality, will, order, more]    |more       |\n",
            "|2  |Product broke after 1 week. Requesting refund immediately.|1     |product broke after 1 week requesting refund immediately|8         |[product, broke, after, 1, week, requesting, refund, immediately]|product    |\n",
            "|2  |Product broke after 1 week. Requesting refund immediately.|1     |product broke after 1 week requesting refund immediately|8         |[product, broke, after, 1, week, requesting, refund, immediately]|broke      |\n",
            "|2  |Product broke after 1 week. Requesting refund immediately.|1     |product broke after 1 week requesting refund immediately|8         |[product, broke, after, 1, week, requesting, refund, immediately]|after      |\n",
            "|2  |Product broke after 1 week. Requesting refund immediately.|1     |product broke after 1 week requesting refund immediately|8         |[product, broke, after, 1, week, requesting, refund, immediately]|1          |\n",
            "|2  |Product broke after 1 week. Requesting refund immediately.|1     |product broke after 1 week requesting refund immediately|8         |[product, broke, after, 1, week, requesting, refund, immediately]|week       |\n",
            "|2  |Product broke after 1 week. Requesting refund immediately.|1     |product broke after 1 week requesting refund immediately|8         |[product, broke, after, 1, week, requesting, refund, immediately]|requesting |\n",
            "|2  |Product broke after 1 week. Requesting refund immediately.|1     |product broke after 1 week requesting refund immediately|8         |[product, broke, after, 1, week, requesting, refund, immediately]|refund     |\n",
            "|2  |Product broke after 1 week. Requesting refund immediately.|1     |product broke after 1 week requesting refund immediately|8         |[product, broke, after, 1, week, requesting, refund, immediately]|immediately|\n",
            "|3  |Decent product for the price. Nothing special but works.  |3     |decent product for the price nothing special but works  |9         |[decent, product, for, the, price, nothing, special, but, works] |decent     |\n",
            "|3  |Decent product for the price. Nothing special but works.  |3     |decent product for the price nothing special but works  |9         |[decent, product, for, the, price, nothing, special, but, works] |product    |\n",
            "|3  |Decent product for the price. Nothing special but works.  |3     |decent product for the price nothing special but works  |9         |[decent, product, for, the, price, nothing, special, but, works] |for        |\n",
            "+---+----------------------------------------------------------+------+--------------------------------------------------------+----------+-----------------------------------------------------------------+-----------+\n",
            "only showing top 20 rows\n"
          ]
        }
      ],
      "source": [
        "df_exploed = df_with_counts.select(\n",
        "    col(\"*\"),\n",
        "    explode(col(\"word_array\")).alias(\"word\"),\n",
        ")\n",
        "\n",
        "df_exploed.show(truncate=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "3. Identify reviews that mention negative keywords"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---+----------------------------------------------------------+------+--------------------------------------------------------+----------+-----------------------------------------------------------------+\n",
            "|id |review                                                    |rating|cleaned_review                                          |word_count|word_array                                                       |\n",
            "+---+----------------------------------------------------------+------+--------------------------------------------------------+----------+-----------------------------------------------------------------+\n",
            "|1  |LOVE IT! Fast shipping, great quality. Will order more!   |5     |love it fast shipping great quality will order more     |9         |[love, it, fast, shipping, great, quality, will, order, more]    |\n",
            "|2  |Product broke after 1 week. Requesting refund immediately.|1     |product broke after 1 week requesting refund immediately|8         |[product, broke, after, 1, week, requesting, refund, immediately]|\n",
            "|3  |Decent product for the price. Nothing special but works.  |3     |decent product for the price nothing special but works  |9         |[decent, product, for, the, price, nothing, special, but, works] |\n",
            "|4  |DO NOT BUY! Cheap quality, had to return it!              |1     |do not buy cheap quality had to return it               |9         |[do, not, buy, cheap, quality, had, to, return, it]              |\n",
            "|5  |Excellent! Better than expected. 100% recommend.          |5     |excellent better than expected 100 recommend            |6         |[excellent, better, than, expected, 100, recommend]              |\n",
            "|6  |Item was damaged during shipping. Broken on arrival.      |1     |item was damaged during shipping broken on arrival      |8         |[item, was, damaged, during, shipping, broken, on, arrival]      |\n",
            "|7  |Good value for money. Satisfied with purchase.            |4     |good value for money satisfied with purchase            |7         |[good, value, for, money, satisfied, with, purchase]             |\n",
            "|8  |WORST PURCHASE EVER! Want my money back!!!                |1     |worst purchase ever want my money back                  |7         |[worst, purchase, ever, want, my, money, back]                   |\n",
            "|9  |Nice product, works as described. Happy customer.         |4     |nice product works as described happy customer          |7         |[nice, product, works, as, described, happy, customer]           |\n",
            "|10 |Meh, it's okay. Nothing to complain about.                |3     |meh it s okay nothing to complain about                 |8         |[meh, it, s, okay, nothing, to, complain, about]                 |\n",
            "+---+----------------------------------------------------------+------+--------------------------------------------------------+----------+-----------------------------------------------------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df_with_counts.show(truncate=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---+--------------------+------+--------------------+----------+--------------------+------------+\n",
            "| id|              review|rating|      cleaned_review|word_count|          word_array|has_neg_word|\n",
            "+---+--------------------+------+--------------------+----------+--------------------+------------+\n",
            "|  1|LOVE IT! Fast shi...|     5|love it fast ship...|         9|[love, it, fast, ...|       false|\n",
            "|  2|Product broke aft...|     1|product broke aft...|         8|[product, broke, ...|        true|\n",
            "|  3|Decent product fo...|     3|decent product fo...|         9|[decent, product,...|       false|\n",
            "|  4|DO NOT BUY! Cheap...|     1|do not buy cheap ...|         9|[do, not, buy, ch...|        true|\n",
            "|  5|Excellent! Better...|     5|excellent better ...|         6|[excellent, bette...|       false|\n",
            "|  6|Item was damaged ...|     1|item was damaged ...|         8|[item, was, damag...|        true|\n",
            "|  7|Good value for mo...|     4|good value for mo...|         7|[good, value, for...|       false|\n",
            "|  8|WORST PURCHASE EV...|     1|worst purchase ev...|         7|[worst, purchase,...|       false|\n",
            "|  9|Nice product, wor...|     4|nice product work...|         7|[nice, product, w...|       false|\n",
            "| 10|Meh, it's okay. N...|     3|meh it s okay not...|         8|[meh, it, s, okay...|       false|\n",
            "+---+--------------------+------+--------------------+----------+--------------------+------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "negative_keyword_array = ['refund','broke','return']\n",
        "\n",
        "df_reviews_with_neg = df_with_counts.withColumn(\n",
        "    \"has_neg_word\",\n",
        "    col(\"cleaned_review\").rlike(\"\\\\b(return|refund|broken|broke)\\\\b\")\n",
        ")\n",
        "df_reviews_with_neg.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        " Task 4: Average rating for reviews with negative keywords vs others"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---+------+--------------------------------------------------------+----------+------------+\n",
            "|id |rating|cleaned_review                                          |word_count|has_neg_word|\n",
            "+---+------+--------------------------------------------------------+----------+------------+\n",
            "|1  |5     |love it fast shipping great quality will order more     |9         |false       |\n",
            "|2  |1     |product broke after 1 week requesting refund immediately|8         |true        |\n",
            "|3  |3     |decent product for the price nothing special but works  |9         |false       |\n",
            "|4  |1     |do not buy cheap quality had to return it               |9         |true        |\n",
            "|5  |5     |excellent better than expected 100 recommend            |6         |false       |\n",
            "|6  |1     |item was damaged during shipping broken on arrival      |8         |true        |\n",
            "|7  |4     |good value for money satisfied with purchase            |7         |false       |\n",
            "|8  |1     |worst purchase ever want my money back                  |7         |false       |\n",
            "|9  |4     |nice product works as described happy customer          |7         |false       |\n",
            "|10 |3     |meh it s okay nothing to complain about                 |8         |false       |\n",
            "+---+------+--------------------------------------------------------+----------+------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df_review_cleaned = df_reviews_with_neg.drop(\"review\",\"word_array\")\n",
        "df_review_cleaned.show(truncate=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+------------+------------------+-----+\n",
            "|has_neg_word|avg(rating)       |count|\n",
            "+------------+------------------+-----+\n",
            "|true        |1.0               |3    |\n",
            "|false       |3.5714285714285716|7    |\n",
            "+------------+------------------+-----+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df_reviews_avg = df_review_cleaned.groupBy(\"has_neg_word\").agg(\n",
        "    avg(col=col(\"rating\")),\n",
        "    count(\"*\").alias(\"count\")\n",
        ")\n",
        "\n",
        "df_reviews_avg.show(truncate=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-------------------+---------------------+------------+\n",
            "|count(has_neg_word)|round(avg(rating), 2)|has_neg_word|\n",
            "+-------------------+---------------------+------------+\n",
            "|3                  |1.0                  |true        |\n",
            "|7                  |3.57                 |false       |\n",
            "+-------------------+---------------------+------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df_review_cleaned.createOrReplaceTempView(\"df_reviews\")\n",
        "\n",
        "df_reviews_avg = spark.sql(\"select count(has_neg_word),round(avg(rating),2),has_neg_word from df_reviews group by has_neg_word\")\n",
        "df_reviews_avg.show(truncate=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "part5_intro"
      },
      "source": [
        "---\n",
        "# Part 5: Regular Expressions in Spark\n",
        "---\n",
        "\n",
        "## 5.1 Introduction to Regex in PySpark\n",
        "\n",
        "**Key Functions:**\n",
        "- `rlike(pattern)` - Returns true if string matches pattern\n",
        "- `regexp_extract(col, pattern, idx)` - Extracts groups matching the pattern\n",
        "- `regexp_replace(col, pattern, replacement)` - Replaces matches with new text\n",
        "\n",
        "**Common Regex Patterns:**\n",
        "| Pattern | Meaning |\n",
        "|---------|----------|\n",
        "| `\\d` | Any digit (0-9) |\n",
        "| `\\w` | Any word character |\n",
        "| `\\s` | Any whitespace |\n",
        "| `.` | Any character |\n",
        "| `*` | Zero or more |\n",
        "| `+` | One or more |\n",
        "| `[]` | Character class |\n",
        "| `()` | Capture group |"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "regex_data"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Contact data:\n",
            "+---+-----------+----------------------+-------------------+----------------------------------+\n",
            "|id |name       |email                 |phone              |address                           |\n",
            "+---+-----------+----------------------+-------------------+----------------------------------+\n",
            "|1  |John Smith |john.smith@email.com  |(555) 123-4567     |123 Main St, New York, NY 10001   |\n",
            "|2  |Jane Doe   |jane_doe@company.co.uk|555-987-6543       |456 Oak Ave, Los Angeles, CA 90001|\n",
            "|3  |Bob Wilson |bob123@test.org       |5551234567         |789 Pine Rd, Chicago, IL 60601    |\n",
            "|4  |Alice Brown|alice.brown@domain.net|(555)456-7890      |321 Elm Blvd, Houston, TX 77001   |\n",
            "|5  |Charlie Lee|invalid-email         |phone: 555-111-2222|PO Box 100, Phoenix, AZ 85001     |\n",
            "+---+-----------+----------------------+-------------------+----------------------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Sample data for regex operations\n",
        "contact_data = [\n",
        "    (1, \"John Smith\", \"john.smith@email.com\", \"(555) 123-4567\", \"123 Main St, New York, NY 10001\"),\n",
        "    (2, \"Jane Doe\", \"jane_doe@company.co.uk\", \"555-987-6543\", \"456 Oak Ave, Los Angeles, CA 90001\"),\n",
        "    (3, \"Bob Wilson\", \"bob123@test.org\", \"5551234567\", \"789 Pine Rd, Chicago, IL 60601\"),\n",
        "    (4, \"Alice Brown\", \"alice.brown@domain.net\", \"(555)456-7890\", \"321 Elm Blvd, Houston, TX 77001\"),\n",
        "    (5, \"Charlie Lee\", \"invalid-email\", \"phone: 555-111-2222\", \"PO Box 100, Phoenix, AZ 85001\"),\n",
        "]\n",
        "\n",
        "df_contacts = spark.createDataFrame(\n",
        "    contact_data,\n",
        "    [\"id\", \"name\", \"email\", \"phone\", \"address\"]\n",
        ")\n",
        "\n",
        "print(\"Contact data:\")\n",
        "df_contacts.show(truncate=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rlike_section"
      },
      "source": [
        "## 5.2 Pattern Matching with rlike"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "rlike_email"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Email validation:\n",
            "+-----------+----------------------+--------------+\n",
            "|name       |email                 |is_valid_email|\n",
            "+-----------+----------------------+--------------+\n",
            "|John Smith |john.smith@email.com  |true          |\n",
            "|Jane Doe   |jane_doe@company.co.uk|false         |\n",
            "|Bob Wilson |bob123@test.org       |true          |\n",
            "|Alice Brown|alice.brown@domain.net|true          |\n",
            "|Charlie Lee|invalid-email         |false         |\n",
            "+-----------+----------------------+--------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Check if email is valid format\n",
        "email_pattern = r\"^[\\w.+-]+@[\\w-]+\\.[a-zA-Z]{2,}$\"\n",
        "\n",
        "df_email_check = df_contacts.withColumn(\n",
        "    \"is_valid_email\",\n",
        "    col(\"email\").rlike(email_pattern)\n",
        ")\n",
        "\n",
        "print(\"Email validation:\")\n",
        "df_email_check.select(\"name\", \"email\", \"is_valid_email\").show(truncate=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "rlike_filter"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Company domain emails:\n",
            "+--------+--------------------+\n",
            "|    name|               email|\n",
            "+--------+--------------------+\n",
            "|Jane Doe|jane_doe@company....|\n",
            "+--------+--------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Filter for specific domain emails\n",
        "df_company_emails = df_contacts.filter(\n",
        "    col(\"email\").rlike(r\"@company\\.\")\n",
        ")\n",
        "\n",
        "print(\"Company domain emails:\")\n",
        "df_company_emails.select(\"name\", \"email\").show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "regexp_extract_section"
      },
      "source": [
        "## 5.3 Extracting Data with regexp_extract"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "extract_email"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracted email components:\n",
            "+----------------------+--------------+------------+---------+\n",
            "|email                 |email_username|email_domain|email_tld|\n",
            "+----------------------+--------------+------------+---------+\n",
            "|john.smith@email.com  |john.smith    |email       |com      |\n",
            "|jane_doe@company.co.uk|jane_doe      |company     |co.uk    |\n",
            "|bob123@test.org       |bob123        |test        |org      |\n",
            "|alice.brown@domain.net|alice.brown   |domain      |net      |\n",
            "|invalid-email         |              |            |         |\n",
            "+----------------------+--------------+------------+---------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Extract email components\n",
        "df_email_parts = df_contacts.withColumn(\n",
        "    \"email_username\",\n",
        "    regexp_extract(col(\"email\"), r\"^([\\w.+-]+)@\", 1)\n",
        ").withColumn(\n",
        "    \"email_domain\",\n",
        "    regexp_extract(col(\"email\"), r\"@([\\w-]+)\\.\", 1)\n",
        ").withColumn(\n",
        "    \"email_tld\",\n",
        "    regexp_extract(col(\"email\"), r\"\\.([a-zA-Z.]+)$\", 1)\n",
        ")\n",
        "\n",
        "print(\"Extracted email components:\")\n",
        "df_email_parts.select(\"email\", \"email_username\", \"email_domain\", \"email_tld\").show(truncate=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "extract_phone"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Phone number extraction and normalization:\n",
            "+-------------------+------------+---------+----------------+\n",
            "|phone              |phone_digits|area_code|phone_normalized|\n",
            "+-------------------+------------+---------+----------------+\n",
            "|(555) 123-4567     |5551234567  |555      |(555) 123-4567  |\n",
            "|555-987-6543       |5559876543  |555      |(555) 987-6543  |\n",
            "|5551234567         |5551234567  |555      |(555) 123-4567  |\n",
            "|(555)456-7890      |5554567890  |555      |(555) 456-7890  |\n",
            "|phone: 555-111-2222|5551112222  |555      |(555) 111-2222  |\n",
            "+-------------------+------------+---------+----------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Extract and normalize phone numbers\n",
        "df_phone_extract = df_contacts.withColumn(\n",
        "    \"phone_digits\",\n",
        "    regexp_replace(col(\"phone\"), r\"[^\\d]\", \"\")\n",
        ").withColumn(\n",
        "    \"area_code\",\n",
        "    regexp_extract(regexp_replace(col(\"phone\"), r\"[^\\d]\", \"\"), r\"^(\\d{3})\", 1)\n",
        ").withColumn(\n",
        "    \"phone_normalized\",\n",
        "    concat(\n",
        "        lit(\"(\"),\n",
        "        substring(regexp_replace(col(\"phone\"), r\"[^\\d]\", \"\"), 1, 3),\n",
        "        lit(\") \"),\n",
        "        substring(regexp_replace(col(\"phone\"), r\"[^\\d]\", \"\"), 4, 3),\n",
        "        lit(\"-\"),\n",
        "        substring(regexp_replace(col(\"phone\"), r\"[^\\d]\", \"\"), 7, 4)\n",
        "    )\n",
        ")\n",
        "\n",
        "print(\"Phone number extraction and normalization:\")\n",
        "df_phone_extract.select(\"phone\", \"phone_digits\", \"area_code\", \"phone_normalized\").show(truncate=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "extract_address"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracted address components:\n",
            "+----------------------------------+------------------------+-----------+-----+--------+\n",
            "|address                           |street                  |city       |state|zip_code|\n",
            "+----------------------------------+------------------------+-----------+-----+--------+\n",
            "|123 Main St, New York, NY 10001   |123 Main St, New York   |New York   |NY   |10001   |\n",
            "|456 Oak Ave, Los Angeles, CA 90001|456 Oak Ave, Los Angeles|Los Angeles|CA   |90001   |\n",
            "|789 Pine Rd, Chicago, IL 60601    |789 Pine Rd, Chicago    |Chicago    |IL   |60601   |\n",
            "|321 Elm Blvd, Houston, TX 77001   |321 Elm Blvd, Houston   |Houston    |TX   |77001   |\n",
            "|PO Box 100, Phoenix, AZ 85001     |PO Box 100, Phoenix     |Phoenix    |AZ   |85001   |\n",
            "+----------------------------------+------------------------+-----------+-----+--------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Extract address components\n",
        "df_address_parts = df_contacts.withColumn(\n",
        "    \"street\",\n",
        "    regexp_extract(col(\"address\"), r\"^(.+),\", 1)\n",
        ").withColumn(\n",
        "    \"city\",\n",
        "    regexp_extract(col(\"address\"), r\", ([^,]+), [A-Z]{2}\", 1)\n",
        ").withColumn(\n",
        "    \"state\",\n",
        "    regexp_extract(col(\"address\"), r\", ([A-Z]{2}) \\d\", 1)\n",
        ").withColumn(\n",
        "    \"zip_code\",\n",
        "    regexp_extract(col(\"address\"), r\"(\\d{5})$\", 1)\n",
        ")\n",
        "\n",
        "print(\"Extracted address components:\")\n",
        "df_address_parts.select(\"address\", \"street\", \"city\", \"state\", \"zip_code\").show(truncate=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "regexp_replace_section"
      },
      "source": [
        "## 5.4 Replacing Patterns with regexp_replace"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "mask_data"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Masked sensitive data:\n",
            "+-----------+----------------------+-------------------+-------------------+--------------+\n",
            "|name       |email                 |email_masked       |phone              |phone_masked  |\n",
            "+-----------+----------------------+-------------------+-------------------+--------------+\n",
            "|John Smith |john.smith@email.com  |jo***@email.com    |(555) 123-4567     |(***) ***-4567|\n",
            "|Jane Doe   |jane_doe@company.co.uk|ja***@company.co.uk|555-987-6543       |(***) ***-6543|\n",
            "|Bob Wilson |bob123@test.org       |bo***@test.org     |5551234567         |(***) ***-4567|\n",
            "|Alice Brown|alice.brown@domain.net|al***@domain.net   |(555)456-7890      |(***) ***-7890|\n",
            "|Charlie Lee|invalid-email         |invalid-email      |phone: 555-111-2222|(***) ***-2222|\n",
            "+-----------+----------------------+-------------------+-------------------+--------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Mask sensitive data\n",
        "df_masked = df_contacts.withColumn(\n",
        "    \"email_masked\",\n",
        "    regexp_replace(\n",
        "        col(\"email\"),\n",
        "        r\"^(.{2}).*(@.*)$\",\n",
        "        r\"$1***$2\"\n",
        "    )\n",
        ").withColumn(\n",
        "    \"phone_masked\",\n",
        "    regexp_replace(\n",
        "        regexp_replace(col(\"phone\"), r\"[^\\d]\", \"\"),\n",
        "        r\"^(\\d{3})(\\d{3})(\\d{4})$\",\n",
        "        r\"(***) ***-$3\"\n",
        "    )\n",
        ")\n",
        "\n",
        "print(\"Masked sensitive data:\")\n",
        "df_masked.select(\"name\", \"email\", \"email_masked\", \"phone\", \"phone_masked\").show(truncate=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "standardize_text"
      },
      "outputs": [],
      "source": [
        "# Clean and standardize text\n",
        "messy_text_data = [\n",
        "    (1, \"Hello    World!!!\"),\n",
        "    (2, \"Multiple   spaces   here\"),\n",
        "    (3, \"Lots of!!!!! punctuation???\"),\n",
        "    (4, \"   Leading and trailing   \"),\n",
        "    (5, \"MixED CaSe TeXt\"),\n",
        "]\n",
        "\n",
        "df_messy_text = spark.createDataFrame(messy_text_data, [\"id\", \"text\"])\n",
        "\n",
        "df_standardized = df_messy_text.withColumn(\n",
        "    \"text_clean\",\n",
        "    trim(\n",
        "        regexp_replace(\n",
        "            regexp_replace(\n",
        "                lower(col(\"text\")),\n",
        "                r\"([!?.]){2,}\",\n",
        "                r\"$1\"\n",
        "            ),\n",
        "            r\"\\s+\",\n",
        "            \" \"\n",
        "        )\n",
        "    )\n",
        ")\n",
        "\n",
        "print(\"Text standardization:\")\n",
        "df_standardized.show(truncate=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "exercise_logs"
      },
      "source": [
        "## 5.5 Practical Exercise: Log File Parsing\n",
        "\n",
        "Parse web server access logs using regex to extract structured information.\n",
        "\n",
        "**Tasks:**\n",
        "1. Extract IP address, timestamp, HTTP method, URL path, status code, and response size\n",
        "2. Filter for error status codes (4xx and 5xx)\n",
        "3. Count requests by HTTP method\n",
        "4. Identify the most accessed URLs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "log_data"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sample access logs:\n",
            "+---+------------------------------------------------------------------------------------+\n",
            "|id |log_line                                                                            |\n",
            "+---+------------------------------------------------------------------------------------+\n",
            "|1  |192.168.1.100 - - [15/Jan/2024:10:30:45 +0000] \"GET /index.html HTTP/1.1\" 200 1234  |\n",
            "|2  |10.0.0.50 - - [15/Jan/2024:10:31:00 +0000] \"POST /api/users HTTP/1.1\" 201 567       |\n",
            "|3  |192.168.1.101 - - [15/Jan/2024:10:31:15 +0000] \"GET /about.html HTTP/1.1\" 200 890   |\n",
            "|4  |172.16.0.25 - - [15/Jan/2024:10:31:30 +0000] \"GET /missing.html HTTP/1.1\" 404 234   |\n",
            "|5  |192.168.1.100 - - [15/Jan/2024:10:31:45 +0000] \"GET /api/data HTTP/1.1\" 500 100     |\n",
            "|6  |10.0.0.75 - - [15/Jan/2024:10:32:00 +0000] \"PUT /api/users/1 HTTP/1.1\" 200 456      |\n",
            "|7  |192.168.1.102 - - [15/Jan/2024:10:32:15 +0000] \"DELETE /api/users/2 HTTP/1.1\" 204 0 |\n",
            "|8  |172.16.0.30 - - [15/Jan/2024:10:32:30 +0000] \"GET /index.html HTTP/1.1\" 200 1234    |\n",
            "|9  |10.0.0.50 - - [15/Jan/2024:10:32:45 +0000] \"POST /api/login HTTP/1.1\" 401 89        |\n",
            "|10 |192.168.1.100 - - [15/Jan/2024:10:33:00 +0000] \"GET /api/products HTTP/1.1\" 200 5678|\n",
            "+---+------------------------------------------------------------------------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Sample Apache access logs\n",
        "log_data = [\n",
        "    (1, ' '),\n",
        "    (2, '10.0.0.50 - - [15/Jan/2024:10:31:00 +0000] \"POST /api/users HTTP/1.1\" 201 567'),\n",
        "    (3, '192.168.1.101 - - [15/Jan/2024:10:31:15 +0000] \"GET /about.html HTTP/1.1\" 200 890'),\n",
        "    (4, '172.16.0.25 - - [15/Jan/2024:10:31:30 +0000] \"GET /missing.html HTTP/1.1\" 404 234'),\n",
        "    (5, '192.168.1.100 - - [15/Jan/2024:10:31:45 +0000] \"GET /api/data HTTP/1.1\" 500 100'),\n",
        "    (6, '10.0.0.75 - - [15/Jan/2024:10:32:00 +0000] \"PUT /api/users/1 HTTP/1.1\" 200 456'),\n",
        "    (7, '192.168.1.102 - - [15/Jan/2024:10:32:15 +0000] \"DELETE /api/users/2 HTTP/1.1\" 204 0'),\n",
        "    (8, '172.16.0.30 - - [15/Jan/2024:10:32:30 +0000] \"GET /index.html HTTP/1.1\" 200 1234'),\n",
        "    (9, '10.0.0.50 - - [15/Jan/2024:10:32:45 +0000] \"POST /api/login HTTP/1.1\" 401 89'),\n",
        "    (10, '192.168.1.100 - - [15/Jan/2024:10:33:00 +0000] \"GET /api/products HTTP/1.1\" 200 5678'),\n",
        "]\n",
        "\n",
        "df_logs = spark.createDataFrame(log_data, [\"id\", \"log_line\"])\n",
        "print(\"Sample access logs:\")\n",
        "df_logs.show(truncate=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "extracting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "metadata": {
        "id": "exercise_logs_code"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---+------------------------------------------------------------------------------------+-------------+----------------------------+------+-------------+-----------+-------------+\n",
            "|id |log_line                                                                            |IP_Address   |timeStamp                   |method|path         |status_code|response_size|\n",
            "+---+------------------------------------------------------------------------------------+-------------+----------------------------+------+-------------+-----------+-------------+\n",
            "|1  |192.168.1.100 - - [15/Jan/2024:10:30:45 +0000] \"GET /index.html HTTP/1.1\" 200 1234  |192.168.1.100|[15/Jan/2024:10:30:45 +0000]|GET   |/index.html  |200        |1234         |\n",
            "|2  |10.0.0.50 - - [15/Jan/2024:10:31:00 +0000] \"POST /api/users HTTP/1.1\" 201 567       |10.0.0.50    |[15/Jan/2024:10:31:00 +0000]|POST  |/api/users   |201        |567          |\n",
            "|3  |192.168.1.101 - - [15/Jan/2024:10:31:15 +0000] \"GET /about.html HTTP/1.1\" 200 890   |192.168.1.101|[15/Jan/2024:10:31:15 +0000]|GET   |/about.html  |200        |890          |\n",
            "|4  |172.16.0.25 - - [15/Jan/2024:10:31:30 +0000] \"GET /missing.html HTTP/1.1\" 404 234   |172.16.0.25  |[15/Jan/2024:10:31:30 +0000]|GET   |/missing.html|404        |234          |\n",
            "|5  |192.168.1.100 - - [15/Jan/2024:10:31:45 +0000] \"GET /api/data HTTP/1.1\" 500 100     |192.168.1.100|[15/Jan/2024:10:31:45 +0000]|GET   |/api/data    |500        |100          |\n",
            "|6  |10.0.0.75 - - [15/Jan/2024:10:32:00 +0000] \"PUT /api/users/1 HTTP/1.1\" 200 456      |10.0.0.75    |[15/Jan/2024:10:32:00 +0000]|PUT   |/api/users/1 |200        |456          |\n",
            "|7  |192.168.1.102 - - [15/Jan/2024:10:32:15 +0000] \"DELETE /api/users/2 HTTP/1.1\" 204 0 |192.168.1.102|[15/Jan/2024:10:32:15 +0000]|DELETE|/api/users/2 |204        |0            |\n",
            "|8  |172.16.0.30 - - [15/Jan/2024:10:32:30 +0000] \"GET /index.html HTTP/1.1\" 200 1234    |172.16.0.30  |[15/Jan/2024:10:32:30 +0000]|GET   |/index.html  |200        |1234         |\n",
            "|9  |10.0.0.50 - - [15/Jan/2024:10:32:45 +0000] \"POST /api/login HTTP/1.1\" 401 89        |10.0.0.50    |[15/Jan/2024:10:32:45 +0000]|POST  |/api/login   |401        |89           |\n",
            "|10 |192.168.1.100 - - [15/Jan/2024:10:33:00 +0000] \"GET /api/products HTTP/1.1\" 200 5678|192.168.1.100|[15/Jan/2024:10:33:00 +0000]|GET   |/api/products|200        |5678         |\n",
            "+---+------------------------------------------------------------------------------------+-------------+----------------------------+------+-------------+-----------+-------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Your code here: Parse the log files\n",
        "from pyspark.sql.functions import substring_index\n",
        "df_extracted = df_logs.withColumn(\n",
        "    \"IP_Address\",\n",
        "    regexp_extract(\"log_line\",r\"^(\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3})\",1)\n",
        ").withColumn(\n",
        "    \"timeStamp\",\n",
        "    regexp_extract(\"log_line\",r\"\\[[^\\]]+\\]\",0)\n",
        ").withColumn(\n",
        "    \"method\",\n",
        "    regexp_extract(\"log_line\",r'\\\"(\\w+)\\s',1)\n",
        ").withColumn(\n",
        "    \"path\",\n",
        "    regexp_extract(\"log_line\",r'\\\"\\w+\\s(\\S+)\\s',1)\n",
        "    ).withColumn(\n",
        "    \"status_code\",\n",
        "    trim(regexp_extract(\"log_line\",r'\\s(\\d{1,3})\\s',1)).cast(IntegerType())\n",
        ").withColumn(\n",
        "    \"response_size\",\n",
        "    trim(regexp_extract(\"log_line\",r'\\s(\\d+)$',1)).cast(IntegerType())\n",
        ")\n",
        "df_extracted.show(truncate=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "count requests by HTTP method"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+------+--------+\n",
            "|method|count(1)|\n",
            "+------+--------+\n",
            "|  POST|       2|\n",
            "|   GET|       6|\n",
            "|DELETE|       1|\n",
            "|   PUT|       1|\n",
            "+------+--------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df_extracted.groupBy(\"method\").agg(count(\"*\")).show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-------------+--------+\n",
            "|         path|count(1)|\n",
            "+-------------+--------+\n",
            "|  /index.html|       2|\n",
            "|/missing.html|       1|\n",
            "|   /api/users|       1|\n",
            "|  /about.html|       1|\n",
            "|    /api/data|       1|\n",
            "|/api/products|       1|\n",
            "|   /api/login|       1|\n",
            "| /api/users/2|       1|\n",
            "| /api/users/1|       1|\n",
            "+-------------+--------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df_extracted.groupBy(\"path\").agg(count(\"*\")).show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "part6_intro"
      },
      "source": [
        "---\n",
        "# Part 6: Building Complete Preprocessing Pipelines\n",
        "---\n",
        "\n",
        "## 6.1 Combining Multiple Preprocessing Steps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "raw_messy_data"
      },
      "outputs": [],
      "source": [
        "# Real-world messy dataset\n",
        "raw_data = [\n",
        "    (\"1\", \"  JOHN SMITH  \", \"john.smith@EMAIL.com\", \"(555) 123-4567\", \"25\", \"$50,000.00\", \"2024-01-15\"),\n",
        "    (\"2\", \"jane doe\", \"JANE_DOE@COMPANY.CO.UK\", \"555-987-6543\", \"N/A\", \"60000\", \"15/02/2024\"),\n",
        "    (\"3\", \"Bob Wilson\", \"invalid-email\", \"5551234567\", \"35\", \"75,000\", \"March 20, 2024\"),\n",
        "    (\"4\", None, \"alice@domain.net\", \"(555)456-7890\", \"28\", None, \"2024-04-10\"),\n",
        "    (\"5\", \"Charlie Lee\", \"\", \"phone: 555-111-2222\", \"forty\", \"$80,000\", None),\n",
        "]\n",
        "\n",
        "df_raw = spark.createDataFrame(\n",
        "    raw_data,\n",
        "    [\"id\", \"name\", \"email\", \"phone\", \"age\", \"salary\", \"hire_date\"]\n",
        ")\n",
        "\n",
        "print(\"Raw messy data:\")\n",
        "df_raw.show(truncate=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "complete_pipeline"
      },
      "outputs": [],
      "source": [
        "def preprocess_employee_data(df):\n",
        "    # Step 1: Clean and standardize name\n",
        "    df = df.withColumn(\n",
        "        \"name_clean\",\n",
        "        when(col(\"name\").isNull() | (trim(col(\"name\")) == \"\"),\n",
        "             concat(lit(\"Employee_\"), col(\"id\")))\n",
        "        .otherwise(expr(\"initcap(trim(name))\"))\n",
        "    )\n",
        "    # Step 2: Clean and validate email\n",
        "    email_pattern = r\"^[\\w.+-]+@[\\w-]+\\.[a-zA-Z]{2,}$\"\n",
        "    df = df.withColumn(\n",
        "        \"email_clean\",\n",
        "        when(lower(trim(col(\"email\"))).rlike(email_pattern),\n",
        "             lower(trim(col(\"email\"))))\n",
        "        .otherwise(None)\n",
        "    )\n",
        "    # Step 3: Normalize phone number\n",
        "    df = df.withColumn(\n",
        "        \"phone_digits\",\n",
        "        regexp_replace(col(\"phone\"), r\"[^\\d]\", \"\")\n",
        "    ).withColumn(\n",
        "        \"phone_clean\",\n",
        "        when(length(col(\"phone_digits\")) == 10,\n",
        "             concat(\n",
        "                 lit(\"(\"), substring(col(\"phone_digits\"), 1, 3), lit(\") \"),\n",
        "                 substring(col(\"phone_digits\"), 4, 3), lit(\"-\"),\n",
        "                 substring(col(\"phone_digits\"), 7, 4)\n",
        "             ))\n",
        "        .otherwise(None)\n",
        "    )\n",
        "    # Step 4: Convert age to integer\n",
        "    df = df.withColumn(\"age_clean\", col(\"age\").cast(IntegerType()))\n",
        "    # Step 5: Clean and convert salary\n",
        "    df = df.withColumn(\n",
        "        \"salary_clean\",\n",
        "        regexp_replace(col(\"salary\"), r\"[$,]\", \"\").cast(DoubleType())\n",
        "    )\n",
        "    # Step 6: Parse hire date\n",
        "    df = df.withColumn(\n",
        "        \"hire_date_clean\",\n",
        "        coalesce(\n",
        "            to_date(col(\"hire_date\"), \"yyyy-MM-dd\"),\n",
        "            to_date(col(\"hire_date\"), \"dd/MM/yyyy\"),\n",
        "            to_date(col(\"hire_date\"), \"MMMM dd, yyyy\")\n",
        "        )\n",
        "    )\n",
        "    return df.select(\n",
        "        col(\"id\").cast(IntegerType()).alias(\"id\"),\n",
        "        col(\"name_clean\").alias(\"name\"),\n",
        "        col(\"email_clean\").alias(\"email\"),\n",
        "        col(\"phone_clean\").alias(\"phone\"),\n",
        "        col(\"age_clean\").alias(\"age\"),\n",
        "        col(\"salary_clean\").alias(\"salary\"),\n",
        "        col(\"hire_date_clean\").alias(\"hire_date\")\n",
        "    )\n",
        "\n",
        "df_clean = preprocess_employee_data(df_raw)\n",
        "\n",
        "print(\"Cleaned data:\")\n",
        "df_clean.show(truncate=False)\n",
        "df_clean.printSchema()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "data_quality"
      },
      "source": [
        "## 6.2 Data Quality Reporting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "quality_report"
      },
      "outputs": [],
      "source": [
        "def generate_quality_report(df_original, df_cleaned):\n",
        "    total_rows = df_original.count()\n",
        "    print(\"=\"*60)\n",
        "    print(\"DATA QUALITY REPORT\")\n",
        "    print(\"=\"*60)\n",
        "    print(f\"\\nTotal rows: {total_rows}\")\n",
        "    print(\"\\n--- Missing Value Summary (Cleaned Data) ---\")\n",
        "    for col_name in df_cleaned.columns:\n",
        "        null_count = df_cleaned.filter(col(col_name).isNull()).count()\n",
        "        null_pct = (null_count / total_rows) * 100\n",
        "        status = \"OK\" if null_pct < 10 else \"WARNING\" if null_pct < 30 else \"CRITICAL\"\n",
        "        print(f\"  {col_name}: {null_count} nulls ({null_pct:.1f}%) [{status}]\")\n",
        "    total_cells = total_rows * len(df_cleaned.columns)\n",
        "    null_cells = sum(df_cleaned.filter(col(c).isNull()).count() for c in df_cleaned.columns)\n",
        "    completeness = ((total_cells - null_cells) / total_cells) * 100\n",
        "    print(f\"\\n--- Overall Data Quality Score ---\")\n",
        "    print(f\"  Completeness: {completeness:.1f}%\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "generate_quality_report(df_raw, df_clean)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "final_challenge"
      },
      "source": [
        "## 6.3 Final Challenge: End-to-End Data Cleaning\n",
        "\n",
        "Apply everything you've learned to clean a complex real-world dataset.\n",
        "\n",
        "**Dataset:** E-commerce transaction data with multiple quality issues\n",
        "\n",
        "**Tasks:**\n",
        "1. Identify and report all data quality issues\n",
        "2. Handle missing values appropriately for each column\n",
        "3. Standardize text fields (product names, categories)\n",
        "4. Parse and validate dates\n",
        "5. Clean and validate email addresses\n",
        "6. Extract useful information using regex\n",
        "7. Create a comprehensive data quality report\n",
        "8. Output the cleaned dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ecommerce_data"
      },
      "outputs": [],
      "source": [
        "# Complex e-commerce dataset\n",
        "ecommerce_data = [\n",
        "    (\"ORD001\", \"  LAPTOP Computer  \", \"ELECTRONICS\", \"john@email.com\", \"$999.99\", \"2\", \"2024-01-15 10:30:00\", \"Completed\"),\n",
        "    (\"ORD002\", \"wireless mouse\", \"electronics\", \"JANE@COMPANY.CO.UK\", \"29.99\", None, \"15/01/2024\", \"completed\"),\n",
        "    (\"ORD003\", \"USB-C Cable (6ft)\", \"Accessories\", \"invalid-email\", \"$12.50\", \"5\", \"January 16, 2024\", \"PENDING\"),\n",
        "    (\"ORD004\", None, \"CLOTHING\", \"bob@test.org\", \"N/A\", \"1\", \"2024-01-16\", \"Cancelled\"),\n",
        "    (\"ORD005\", \"Running Shoes - Size 10\", \"  clothing  \", \"\", \"89.00\", \"2\", None, \"Shipped\"),\n",
        "    (\"ORD006\", \"HDMI Cable!!!\", \"accessories\", \"alice@domain.net\", \"$15.00\", \"10\", \"2024-01-17 14:20:00\", \"completed\"),\n",
        "    (\"ORD007\", \"smartphone case\", \"Electronics\", \"charlie@email.com\", \"$25\", \"3\", \"17-Jan-2024\", \"Processing\"),\n",
        "    (\"ORD008\", \"  Winter Jacket  \", \"Clothing\", \"N/A\", \"$149.99\", None, \"2024-01-18\", \"SHIPPED\"),\n",
        "]\n",
        "\n",
        "df_ecommerce = spark.createDataFrame(\n",
        "    ecommerce_data,\n",
        "    [\"order_id\", \"product\", \"category\", \"email\", \"price\", \"quantity\", \"order_date\", \"status\"]\n",
        ")\n",
        "\n",
        "print(\"Raw e-commerce data:\")\n",
        "df_ecommerce.show(truncate=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "final_challenge_code"
      },
      "outputs": [],
      "source": [
        "# Your code here: Build a complete preprocessing pipeline\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cleanup"
      },
      "source": [
        "---\n",
        "# Cleanup\n",
        "---\n",
        "\n",
        "Always remember to stop your Spark session when you're done."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 101,
      "metadata": {
        "id": "stop_spark"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Spark session stopped.\n"
          ]
        }
      ],
      "source": [
        "# Stop the Spark session\n",
        "spark.stop()\n",
        "print(\"Spark session stopped.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "summary"
      },
      "source": [
        "---\n",
        "# Summary\n",
        "---\n",
        "\n",
        "In this lab, you learned:\n",
        "\n",
        "1. **Missing Value Handling**\n",
        "   - Identifying missing data (null, NaN, empty strings, placeholders)\n",
        "   - Dropping rows/columns with missing values\n",
        "   - Imputation strategies (mean, median, mode, group-wise)\n",
        "   - Forward/backward fill for time series\n",
        "   - Creating missing value indicators\n",
        "\n",
        "2. **Data Type Management**\n",
        "   - Understanding Spark data types\n",
        "   - Type casting and conversion\n",
        "   - Handling dates and timestamps\n",
        "   - Schema validation and enforcement\n",
        "\n",
        "3. **Text Preprocessing**\n",
        "   - Case normalization and trimming\n",
        "   - Tokenization and text splitting\n",
        "   - Stop word removal\n",
        "   - Building text cleaning pipelines\n",
        "\n",
        "4. **Regular Expressions**\n",
        "   - Pattern matching with rlike\n",
        "   - Data extraction with regexp_extract\n",
        "   - Pattern replacement with regexp_replace\n",
        "   - Complex parsing tasks (emails, phones, addresses, logs)\n",
        "\n",
        "5. **Building Pipelines**\n",
        "   - Combining multiple preprocessing steps\n",
        "   - Creating reusable functions\n",
        "   - Data quality reporting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "resources"
      },
      "source": [
        "---\n",
        "# Additional Resources\n",
        "---\n",
        "\n",
        "- Apache Spark Documentation: https://spark.apache.org/docs/latest/\n",
        "- PySpark API Reference: https://spark.apache.org/docs/latest/api/python/\n",
        "- Spark SQL Functions: https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/functions.html\n",
        "- Regular Expression Reference: https://docs.python.org/3/library/re.html\n",
        "- Spark ML Feature Transformers: https://spark.apache.org/docs/latest/ml-features.html"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
