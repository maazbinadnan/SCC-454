{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "![Lancaster University](https://www.lancaster.ac.uk/media/lancaster-university/content-assets/images/fst/logos/SCC-Logo.svg)\n",
        "\n",
        "# SCC.454: Large Scale Platforms for AI and Data Analysis\n",
        "## Lab 3: Data Preprocessing with Apache Spark\n",
        "\n",
        "**Duration:** 2 hours\n",
        "\n",
        "**Learning Objectives:**\n",
        "- Understand the importance of data preprocessing in big data pipelines\n",
        "- Master techniques for handling missing values in Spark DataFrames\n",
        "- Learn data type conversions and schema management\n",
        "- Apply text preprocessing techniques at scale\n",
        "- Use regular expressions (regex) for pattern matching and data extraction\n",
        "- Build complete data cleaning pipelines in PySpark"
      ],
      "metadata": {
        "id": "intro_header"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Table of Contents\n",
        "\n",
        "1. **Part 1: Introduction to Data Preprocessing** (15 minutes)\n",
        "   - Why Data Preprocessing Matters\n",
        "   - Setting up the Environment\n",
        "   - Overview of Spark's Data Cleaning Capabilities\n",
        "\n",
        "2. **Part 2: Handling Missing Values** (30 minutes)\n",
        "   - Identifying Missing Data\n",
        "   - Strategies for Missing Value Treatment\n",
        "   - Dropping Null Values\n",
        "   - Filling Missing Values (Imputation)\n",
        "   - Advanced Imputation Techniques\n",
        "   - Practical Exercise: Customer Data Cleaning\n",
        "\n",
        "3. **Part 3: Data Type Handling and Schema Management** (20 minutes)\n",
        "   - Understanding Spark Data Types\n",
        "   - Type Casting and Conversion\n",
        "   - Handling Date and Timestamp Fields\n",
        "   - Schema Validation and Enforcement\n",
        "\n",
        "4. **Part 4: Text Preprocessing** (25 minutes)\n",
        "   - String Functions in Spark\n",
        "   - Case Normalization and Trimming\n",
        "   - Tokenization and Text Splitting\n",
        "   - Stop Word Removal\n",
        "   - Text Cleaning Pipeline\n",
        "   - Practical Exercise: Product Review Cleaning\n",
        "\n",
        "5. **Part 5: Regular Expressions in Spark** (30 minutes)\n",
        "   - Introduction to Regex in PySpark\n",
        "   - Pattern Matching with rlike\n",
        "   - Extracting Data with regexp_extract\n",
        "   - Replacing Patterns with regexp_replace\n",
        "   - Complex Pattern Matching\n",
        "   - Practical Exercise: Log File Parsing\n",
        "\n",
        "6. **Part 6: Building Complete Preprocessing Pipelines** (15 minutes)\n",
        "   - Combining Multiple Preprocessing Steps\n",
        "   - Creating Reusable Cleaning Functions\n",
        "   - Data Quality Reporting\n",
        "   - Final Challenge: End-to-End Data Cleaning"
      ],
      "metadata": {
        "id": "toc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "# Part 1: Introduction to Data Preprocessing\n",
        "---\n",
        "\n",
        "## 1.1 Why Data Preprocessing Matters\n",
        "\n",
        "Data preprocessing is the foundation of any successful data analysis or machine learning project. In the context of big data, preprocessing becomes even more critical due to:\n",
        "\n",
        "**The Scale Challenge:**\n",
        "- Real-world datasets often contain millions or billions of records\n",
        "- Traditional preprocessing tools (like pandas) may not scale effectively\n",
        "- Distributed processing is essential for handling large volumes\n",
        "\n",
        "**Common Data Quality Issues:**\n",
        "- Missing values (null, NaN, empty strings)\n",
        "- Inconsistent formatting (dates, currencies, units)\n",
        "- Duplicate records\n",
        "- Invalid or out-of-range values\n",
        "- Inconsistent text data (case, whitespace, special characters)\n",
        "- Unstructured or semi-structured data requiring parsing\n",
        "\n",
        "**The Garbage In, Garbage Out Principle:**\n",
        "```\n",
        "Raw Data (Messy) --> Preprocessing Pipeline --> Clean Data (Analysis-Ready)\n",
        "```\n",
        "\n",
        "**Apache Spark's Advantages for Preprocessing:**\n",
        "- Distributed processing across clusters\n",
        "- Lazy evaluation for optimization\n",
        "- Rich built-in functions for data manipulation\n",
        "- SQL interface for familiar syntax\n",
        "- Seamless integration with ML pipelines"
      ],
      "metadata": {
        "id": "part1_intro"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.2 Setting up the Environment\n",
        "\n",
        "Let's set up PySpark in Google Colab. If you're running this on a different system, ensure Java is installed and JAVA_HOME is configured correctly."
      ],
      "metadata": {
        "id": "setup_intro"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install PySpark\n",
        "!pip install pyspark==3.5.0 -q\n",
        "\n",
        "# Install Java (Spark requires Java)\n",
        "!apt-get install openjdk-11-jdk-headless -qq > /dev/null\n",
        "\n",
        "# Set Java environment variable\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\"\n",
        "\n",
        "print(\"PySpark and Java installed successfully!\")"
      ],
      "metadata": {
        "id": "install_spark"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark import SparkConf\n",
        "\n",
        "# Create a SparkSession configured for data preprocessing\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"SCC454-DataPreprocessing\") \\\n",
        "    .config(\"spark.driver.memory\", \"4g\") \\\n",
        "    .config(\"spark.sql.legacy.timeParserPolicy\", \"LEGACY\") \\\n",
        "    .config(\"spark.ui.port\", \"4050\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Get the underlying SparkContext\n",
        "sc = spark.sparkContext\n",
        "\n",
        "print(f\"Spark Version: {spark.version}\")\n",
        "print(f\"Application Name: {spark.sparkContext.appName}\")\n",
        "print(f\"Master: {spark.sparkContext.master}\")\n",
        "print(\"\\nSpark Session ready for data preprocessing!\")"
      ],
      "metadata": {
        "id": "create_session"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.3 Overview of Spark's Data Cleaning Capabilities\n",
        "\n",
        "Spark provides several modules and functions for data preprocessing:\n",
        "\n",
        "**Key Modules:**\n",
        "- `pyspark.sql.functions`: Built-in functions for transformations\n",
        "- `pyspark.sql.types`: Data type definitions\n",
        "- `pyspark.ml.feature`: Feature engineering tools\n",
        "\n",
        "**Important Function Categories:**\n",
        "\n",
        "| Category | Functions |\n",
        "|----------|----------|\n",
        "| Null Handling | `isNull()`, `isNotNull()`, `na.drop()`, `na.fill()` |\n",
        "| String Functions | `lower()`, `upper()`, `trim()`, `split()`, `concat()` |\n",
        "| Regex Functions | `regexp_extract()`, `regexp_replace()`, `rlike()` |\n",
        "| Type Conversion | `cast()`, `to_date()`, `to_timestamp()` |\n",
        "| Aggregation | `count()`, `avg()`, `sum()`, `mean()` |\n",
        "| Conditional | `when()`, `otherwise()`, `coalesce()` |"
      ],
      "metadata": {
        "id": "capabilities_overview"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import commonly used functions for data preprocessing\n",
        "from pyspark.sql.functions import (\n",
        "    col, lit, isnan, isnull, coalesce,\n",
        "    lower, upper, trim, ltrim, rtrim, length,\n",
        "    split, concat, concat_ws, substring, replace,\n",
        "    regexp_extract, regexp_replace,\n",
        "    to_date, to_timestamp,\n",
        "    count, avg, sum as spark_sum, mean, stddev,\n",
        "    min as spark_min, max as spark_max,\n",
        "    when, expr,\n",
        "    year, month, dayofmonth, datediff, date_format,\n",
        "    monotonically_increasing_id, round as spark_round\n",
        ")\n",
        "\n",
        "from pyspark.sql.types import (\n",
        "    StructType, StructField, StringType, IntegerType,\n",
        "    FloatType, DoubleType, DateType, TimestampType, BooleanType\n",
        ")\n",
        "\n",
        "print(\"All preprocessing functions imported successfully!\")"
      ],
      "metadata": {
        "id": "import_functions"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "# Part 2: Handling Missing Values\n",
        "---\n",
        "\n",
        "## 2.1 Identifying Missing Data\n",
        "\n",
        "Missing data is one of the most common data quality issues. In Spark, missing values can appear as:\n",
        "- `null`: The absence of a value\n",
        "- `NaN` (Not a Number): Result of undefined numerical operations\n",
        "- Empty strings: For string columns\n",
        "- Placeholder values (e.g., -999, \"N/A\", \"Unknown\")\n",
        "\n",
        "Let's create a sample dataset with various types of missing values."
      ],
      "metadata": {
        "id": "part2_intro"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a DataFrame with various missing value patterns\n",
        "customer_data = [\n",
        "    (1, \"John Smith\", 28, \"john@email.com\", 75000.0, \"New York\", \"2023-01-15\"),\n",
        "    (2, \"Jane Doe\", None, \"jane@email.com\", 82000.0, \"Los Angeles\", \"2023-02-20\"),\n",
        "    (3, \"Bob Wilson\", 35, None, 65000.0, \"Chicago\", \"2023-03-10\"),\n",
        "    (4, \"Alice Brown\", 42, \"alice@email.com\", None, \"Houston\", None),\n",
        "    (5, None, 31, \"unknown@email.com\", 70000.0, \"Phoenix\", \"2023-05-01\"),\n",
        "    (6, \"Charlie Lee\", 29, \"\", 58000.0, \"\", \"2023-06-15\"),\n",
        "    (7, \"Diana Prince\", None, \"diana@email.com\", 95000.0, None, \"2023-07-20\"),\n",
        "    (8, \"Edward Kim\", 38, \"N/A\", float('nan'), \"Boston\", \"2023-08-25\"),\n",
        "    (9, \"Fiona Garcia\", 45, \"fiona@email.com\", 88000.0, \"Seattle\", \"2023-09-30\"),\n",
        "    (10, \"George Hall\", None, None, None, None, None),\n",
        "    (11, \"Hannah White\", 33, \"hannah@email.com\", 72000.0, \"Denver\", \"2023-11-10\"),\n",
        "    (12, \"Ivan Torres\", 27, \"ivan@email.com\", 63000.0, \"Miami\", \"2023-12-05\")\n",
        "]\n",
        "\n",
        "columns = [\"customer_id\", \"name\", \"age\", \"email\", \"salary\", \"city\", \"signup_date\"]\n",
        "df_customers = spark.createDataFrame(customer_data, columns)\n",
        "\n",
        "print(\"Customer Data with Missing Values:\")\n",
        "df_customers.show(truncate=False)"
      ],
      "metadata": {
        "id": "create_missing_data"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the schema\n",
        "print(\"Schema:\")\n",
        "df_customers.printSchema()"
      ],
      "metadata": {
        "id": "check_schema"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Counting Missing Values\n",
        "\n",
        "The first step in handling missing data is to understand its extent."
      ],
      "metadata": {
        "id": "count_missing_intro"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Method 1: Count nulls per column\n",
        "print(\"Null counts per column:\")\n",
        "null_counts = df_customers.select([\n",
        "    count(when(col(c).isNull(), c)).alias(c)\n",
        "    for c in df_customers.columns\n",
        "])\n",
        "null_counts.show()"
      ],
      "metadata": {
        "id": "count_nulls_basic"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Method 2: Count nulls AND NaN values (important for numeric columns)\n",
        "print(\"Null + NaN counts for numeric columns:\")\n",
        "df_customers.select([\n",
        "    count(when(col(\"salary\").isNull() | isnan(col(\"salary\")), 1)).alias(\"salary_missing\")\n",
        "]).show()"
      ],
      "metadata": {
        "id": "count_nulls_nan"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Method 3: Comprehensive missing value report\n",
        "def missing_value_report(df):\n",
        "    total_rows = df.count()\n",
        "    report_data = []\n",
        "    for col_name in df.columns:\n",
        "        col_type = str(df.schema[col_name].dataType)\n",
        "        null_count = df.filter(col(col_name).isNull()).count()\n",
        "        empty_count = 0\n",
        "        if \"String\" in col_type:\n",
        "            empty_count = df.filter(\n",
        "                (col(col_name) == \"\") | (col(col_name) == \"N/A\") | (col(col_name) == \"Unknown\")\n",
        "            ).count()\n",
        "        nan_count = 0\n",
        "        if \"Double\" in col_type or \"Float\" in col_type:\n",
        "            nan_count = df.filter(isnan(col(col_name))).count()\n",
        "        total_missing = null_count + empty_count + nan_count\n",
        "        missing_pct = (total_missing / total_rows) * 100\n",
        "        report_data.append((col_name, col_type, null_count, empty_count, nan_count, total_missing, round(missing_pct, 2)))\n",
        "    report_columns = [\"Column\", \"Type\", \"Nulls\", \"Empty\", \"NaN\", \"Total_Missing\", \"Missing_%\"]\n",
        "    return spark.createDataFrame(report_data, report_columns)\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"COMPREHENSIVE MISSING VALUE REPORT\")\n",
        "print(\"=\"*60)\n",
        "report = missing_value_report(df_customers)\n",
        "report.show(truncate=False)"
      ],
      "metadata": {
        "id": "missing_report"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.2 Strategies for Missing Value Treatment\n",
        "\n",
        "There are several strategies for handling missing values:\n",
        "\n",
        "1. **Deletion**: Remove rows or columns with missing values\n",
        "2. **Imputation**: Fill missing values with estimated values (mean, median, mode)\n",
        "3. **Flagging**: Create indicator variables for missingness\n",
        "4. **Keep as-is**: Leave missing values for algorithms that can handle them"
      ],
      "metadata": {
        "id": "strategies_intro"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.3 Dropping Null Values\n",
        "\n",
        "Spark's `na.drop()` method provides flexible options for removing rows with missing values."
      ],
      "metadata": {
        "id": "dropping_nulls"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Original count\n",
        "print(f\"Original row count: {df_customers.count()}\")\n",
        "\n",
        "# Drop rows with ANY null value (most aggressive)\n",
        "df_drop_any = df_customers.na.drop(how=\"any\")\n",
        "print(f\"After dropping rows with ANY null: {df_drop_any.count()}\")\n",
        "df_drop_any.show()"
      ],
      "metadata": {
        "id": "drop_any"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop rows where ALL values are null (least aggressive)\n",
        "df_drop_all = df_customers.na.drop(how=\"all\")\n",
        "print(f\"After dropping rows where ALL are null: {df_drop_all.count()}\")"
      ],
      "metadata": {
        "id": "drop_all"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop rows where specific columns are null (most practical)\n",
        "df_drop_subset = df_customers.na.drop(subset=[\"name\", \"email\"])\n",
        "print(f\"After dropping rows where name OR email is null: {df_drop_subset.count()}\")\n",
        "df_drop_subset.show()"
      ],
      "metadata": {
        "id": "drop_subset"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop rows with fewer than a threshold of non-null values\n",
        "df_drop_thresh = df_customers.na.drop(thresh=5)\n",
        "print(f\"After dropping rows with < 5 non-null values: {df_drop_thresh.count()}\")\n",
        "df_drop_thresh.show()"
      ],
      "metadata": {
        "id": "drop_thresh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.4 Filling Missing Values (Imputation)\n",
        "\n",
        "Spark's `na.fill()` method allows filling missing values with specified values."
      ],
      "metadata": {
        "id": "filling_nulls"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fill all numeric columns with a single value\n",
        "print(\"Fill all numeric nulls with 0:\")\n",
        "df_customers.na.fill(0).show()"
      ],
      "metadata": {
        "id": "fill_single"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fill specific columns with specific values (dictionary approach)\n",
        "fill_values = {\n",
        "    \"name\": \"Unknown\",\n",
        "    \"age\": 30,\n",
        "    \"email\": \"no_email@placeholder.com\",\n",
        "    \"salary\": 0.0,\n",
        "    \"city\": \"Unknown\",\n",
        "    \"signup_date\": \"1970-01-01\"\n",
        "}\n",
        "\n",
        "print(\"Fill with specific values per column:\")\n",
        "df_filled = df_customers.na.fill(fill_values)\n",
        "df_filled.show(truncate=False)"
      ],
      "metadata": {
        "id": "fill_dict"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Statistical Imputation\n",
        "\n",
        "For numerical columns, it's often better to use statistical measures like mean or median."
      ],
      "metadata": {
        "id": "statistical_imputation"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate mean for age column (excluding nulls)\n",
        "age_stats = df_customers.select(\n",
        "    mean(col(\"age\")).alias(\"mean_age\"),\n",
        "    expr(\"percentile_approx(age, 0.5)\").alias(\"median_age\")\n",
        ").collect()[0]\n",
        "\n",
        "mean_age = age_stats[\"mean_age\"]\n",
        "median_age = age_stats[\"median_age\"]\n",
        "\n",
        "print(f\"Mean age: {mean_age:.2f}\")\n",
        "print(f\"Median age: {median_age}\")\n",
        "\n",
        "# Fill with mean\n",
        "df_mean_imputed = df_customers.na.fill({\"age\": int(mean_age)})\n",
        "print(\"\\nAge filled with mean:\")\n",
        "df_mean_imputed.select(\"customer_id\", \"name\", \"age\").show()"
      ],
      "metadata": {
        "id": "mean_imputation"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate and impute with median for salary\n",
        "salary_stats = df_customers.filter(\n",
        "    col(\"salary\").isNotNull() & ~isnan(col(\"salary\"))\n",
        ").select(\n",
        "    mean(col(\"salary\")).alias(\"mean_salary\"),\n",
        "    expr(\"percentile_approx(salary, 0.5)\").alias(\"median_salary\")\n",
        ").collect()[0]\n",
        "\n",
        "print(f\"Mean salary: ${salary_stats['mean_salary']:,.2f}\")\n",
        "print(f\"Median salary: ${salary_stats['median_salary']:,.2f}\")"
      ],
      "metadata": {
        "id": "median_imputation"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.5 Advanced Imputation Techniques"
      ],
      "metadata": {
        "id": "advanced_imputation"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Group-wise imputation: Fill missing values based on group statistics\n",
        "from pyspark.sql.window import Window\n",
        "\n",
        "# Calculate average salary per city\n",
        "city_avg_salary = df_customers.filter(\n",
        "    col(\"salary\").isNotNull() & ~isnan(col(\"salary\")) & col(\"city\").isNotNull()\n",
        ").groupBy(\"city\").agg(\n",
        "    spark_round(mean(\"salary\"), 2).alias(\"city_avg_salary\")\n",
        ")\n",
        "\n",
        "print(\"Average salary per city:\")\n",
        "city_avg_salary.show()"
      ],
      "metadata": {
        "id": "group_imputation_prep"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Using coalesce for conditional filling\n",
        "overall_avg_salary = salary_stats['mean_salary']\n",
        "\n",
        "df_smart_imputed = df_customers.join(\n",
        "    city_avg_salary, on=\"city\", how=\"left\"\n",
        ").withColumn(\n",
        "    \"salary_imputed\",\n",
        "    coalesce(\n",
        "        when(~isnan(col(\"salary\")), col(\"salary\")),\n",
        "        col(\"city_avg_salary\"),\n",
        "        lit(overall_avg_salary)\n",
        "    )\n",
        ").select(\n",
        "    \"customer_id\", \"name\", \"city\", \"salary\", \"city_avg_salary\", \"salary_imputed\"\n",
        ")\n",
        "\n",
        "print(\"Smart imputation using city averages:\")\n",
        "df_smart_imputed.show()"
      ],
      "metadata": {
        "id": "group_imputation_apply"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Forward Fill using Window functions (useful for time-series)\n",
        "from pyspark.sql.functions import last, first\n",
        "\n",
        "ts_data = [\n",
        "    (\"2024-01-01\", 100.0),\n",
        "    (\"2024-01-02\", None),\n",
        "    (\"2024-01-03\", None),\n",
        "    (\"2024-01-04\", 105.0),\n",
        "    (\"2024-01-05\", None),\n",
        "    (\"2024-01-06\", 108.0),\n",
        "    (\"2024-01-07\", None),\n",
        "]\n",
        "\n",
        "df_ts = spark.createDataFrame(ts_data, [\"date\", \"value\"])\n",
        "print(\"Time series with missing values:\")\n",
        "df_ts.show()\n",
        "\n",
        "# Forward fill\n",
        "window_forward = Window.orderBy(\"date\").rowsBetween(Window.unboundedPreceding, 0)\n",
        "\n",
        "df_forward_fill = df_ts.withColumn(\n",
        "    \"value_ffill\",\n",
        "    last(col(\"value\"), ignorenulls=True).over(window_forward)\n",
        ")\n",
        "\n",
        "print(\"After forward fill:\")\n",
        "df_forward_fill.show()"
      ],
      "metadata": {
        "id": "forward_fill"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating missing value indicator columns\n",
        "df_with_indicators = df_customers.withColumn(\n",
        "    \"age_was_missing\",\n",
        "    when(col(\"age\").isNull(), 1).otherwise(0)\n",
        ").withColumn(\n",
        "    \"salary_was_missing\",\n",
        "    when(col(\"salary\").isNull() | isnan(col(\"salary\")), 1).otherwise(0)\n",
        ")\n",
        "\n",
        "print(\"Data with missing value indicators:\")\n",
        "df_with_indicators.select(\n",
        "    \"customer_id\", \"name\", \"age\", \"age_was_missing\", \"salary\", \"salary_was_missing\"\n",
        ").show()"
      ],
      "metadata": {
        "id": "missing_indicators"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.6 Practical Exercise: Customer Data Cleaning\n",
        "\n",
        "Now it's your turn! Clean the customer dataset by applying appropriate imputation strategies.\n",
        "\n",
        "**Tasks:**\n",
        "1. Handle missing names (fill with \"Customer_[ID]\")\n",
        "2. Impute missing ages with the median age\n",
        "3. Replace empty/placeholder emails with a standardized format\n",
        "4. Impute missing salaries with the median salary\n",
        "5. Fill missing cities with \"Unknown\"\n",
        "6. Handle missing signup dates appropriately\n",
        "7. Remove the NaN value from salary column\n",
        "8. Create a data quality score"
      ],
      "metadata": {
        "id": "exercise1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your code here\n",
        "\n"
      ],
      "metadata": {
        "id": "exercise1_code"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "# Part 3: Data Type Handling and Schema Management\n",
        "---\n",
        "\n",
        "## 3.1 Understanding Spark Data Types\n",
        "\n",
        "Proper data types are essential for efficient processing.\n",
        "\n",
        "**Numeric Types:** ByteType, ShortType, IntegerType, LongType, FloatType, DoubleType, DecimalType\n",
        "\n",
        "**String and Binary:** StringType, BinaryType\n",
        "\n",
        "**Date/Time:** DateType, TimestampType\n",
        "\n",
        "**Complex Types:** ArrayType, MapType, StructType"
      ],
      "metadata": {
        "id": "part3_intro"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a DataFrame with mixed type issues\n",
        "messy_data = [\n",
        "    (\"1\", \"25\", \"50000.50\", \"2024-01-15\", \"true\"),\n",
        "    (\"2\", \"thirty\", \"60000\", \"15/02/2024\", \"yes\"),\n",
        "    (\"3\", \"35\", \"$75,000\", \"2024-03-20\", \"1\"),\n",
        "    (\"4\", \"40\", \"80000.00\", \"March 25, 2024\", \"false\"),\n",
        "    (\"5\", \"N/A\", \"invalid\", \"2024-04-30\", \"no\"),\n",
        "]\n",
        "\n",
        "df_messy = spark.createDataFrame(messy_data, [\"id\", \"age\", \"salary\", \"date\", \"active\"])\n",
        "\n",
        "print(\"Messy data with type issues:\")\n",
        "df_messy.show()\n",
        "df_messy.printSchema()"
      ],
      "metadata": {
        "id": "messy_types"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.2 Type Casting and Conversion"
      ],
      "metadata": {
        "id": "type_casting"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Simple casting\n",
        "df_cast = df_messy.withColumn(\"id_int\", col(\"id\").cast(IntegerType()))\n",
        "print(\"ID cast to integer:\")\n",
        "df_cast.select(\"id\", \"id_int\").show()"
      ],
      "metadata": {
        "id": "simple_cast"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Casting with potential failures - non-numeric strings become null\n",
        "df_age_cast = df_messy.withColumn(\"age_int\", col(\"age\").cast(IntegerType()))\n",
        "print(\"Age cast to integer (note nulls for invalid values):\")\n",
        "df_age_cast.select(\"age\", \"age_int\").show()"
      ],
      "metadata": {
        "id": "cast_with_nulls"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Clean salary string before casting\n",
        "df_salary_clean = df_messy.withColumn(\n",
        "    \"salary_cleaned\",\n",
        "    regexp_replace(col(\"salary\"), \"[$,]\", \"\")\n",
        ").withColumn(\n",
        "    \"salary_double\",\n",
        "    col(\"salary_cleaned\").cast(DoubleType())\n",
        ")\n",
        "\n",
        "print(\"Salary cleaning and casting:\")\n",
        "df_salary_clean.select(\"salary\", \"salary_cleaned\", \"salary_double\").show()"
      ],
      "metadata": {
        "id": "clean_before_cast"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Boolean conversion with multiple representations\n",
        "df_bool = df_messy.withColumn(\n",
        "    \"active_bool\",\n",
        "    when(lower(col(\"active\")).isin(\"true\", \"yes\", \"1\", \"t\", \"y\"), True)\n",
        "    .when(lower(col(\"active\")).isin(\"false\", \"no\", \"0\", \"f\", \"n\"), False)\n",
        "    .otherwise(None)\n",
        ")\n",
        "\n",
        "print(\"Boolean conversion:\")\n",
        "df_bool.select(\"active\", \"active_bool\").show()"
      ],
      "metadata": {
        "id": "boolean_convert"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.3 Handling Date and Timestamp Fields"
      ],
      "metadata": {
        "id": "date_handling"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Common date formats\n",
        "date_examples = [\n",
        "    (\"2024-01-15\", \"ISO format\"),\n",
        "    (\"15/02/2024\", \"European DD/MM/YYYY\"),\n",
        "    (\"03/20/2024\", \"American MM/DD/YYYY\"),\n",
        "    (\"March 25, 2024\", \"Written format\"),\n",
        "    (\"2024.04.30\", \"Dot separator\"),\n",
        "]\n",
        "\n",
        "df_dates = spark.createDataFrame(date_examples, [\"date_string\", \"format_name\"])\n",
        "df_dates.show(truncate=False)"
      ],
      "metadata": {
        "id": "date_examples"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Parse dates with multiple format patterns using coalesce\n",
        "df_parsed = df_dates.withColumn(\n",
        "    \"parsed_date\",\n",
        "    coalesce(\n",
        "        to_date(col(\"date_string\"), \"yyyy-MM-dd\"),\n",
        "        to_date(col(\"date_string\"), \"dd/MM/yyyy\"),\n",
        "        to_date(col(\"date_string\"), \"MM/dd/yyyy\"),\n",
        "        to_date(col(\"date_string\"), \"MMMM dd, yyyy\"),\n",
        "        to_date(col(\"date_string\"), \"yyyy.MM.dd\")\n",
        "    )\n",
        ")\n",
        "\n",
        "print(\"Dates parsed from multiple formats:\")\n",
        "df_parsed.show(truncate=False)"
      ],
      "metadata": {
        "id": "parse_dates"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract components from dates\n",
        "from pyspark.sql.functions import year, month, dayofmonth, dayofweek, quarter, date_format\n",
        "\n",
        "df_date_parts = df_parsed.filter(col(\"parsed_date\").isNotNull()).select(\n",
        "    col(\"date_string\"),\n",
        "    col(\"parsed_date\"),\n",
        "    year(col(\"parsed_date\")).alias(\"year\"),\n",
        "    month(col(\"parsed_date\")).alias(\"month\"),\n",
        "    dayofmonth(col(\"parsed_date\")).alias(\"day\"),\n",
        "    quarter(col(\"parsed_date\")).alias(\"quarter\"),\n",
        "    date_format(col(\"parsed_date\"), \"EEEE\").alias(\"day_name\")\n",
        ")\n",
        "\n",
        "print(\"Date components extracted:\")\n",
        "df_date_parts.show(truncate=False)"
      ],
      "metadata": {
        "id": "date_components"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.4 Schema Validation and Enforcement"
      ],
      "metadata": {
        "id": "schema_validation"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define an expected schema\n",
        "expected_schema = StructType([\n",
        "    StructField(\"customer_id\", IntegerType(), nullable=False),\n",
        "    StructField(\"name\", StringType(), nullable=True),\n",
        "    StructField(\"age\", IntegerType(), nullable=True),\n",
        "    StructField(\"email\", StringType(), nullable=True),\n",
        "    StructField(\"salary\", DoubleType(), nullable=True),\n",
        "    StructField(\"city\", StringType(), nullable=True),\n",
        "    StructField(\"signup_date\", DateType(), nullable=True)\n",
        "])\n",
        "\n",
        "print(\"Expected Schema:\")\n",
        "for field in expected_schema.fields:\n",
        "    print(f\"  {field.name}: {field.dataType} (nullable={field.nullable})\")"
      ],
      "metadata": {
        "id": "define_schema"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to validate and transform data to match expected schema\n",
        "def enforce_schema(df, target_schema):\n",
        "    result_df = df\n",
        "    for field in target_schema.fields:\n",
        "        col_name = field.name\n",
        "        col_type = field.dataType\n",
        "        if col_name in df.columns:\n",
        "            result_df = result_df.withColumn(col_name, col(col_name).cast(col_type))\n",
        "        else:\n",
        "            result_df = result_df.withColumn(col_name, lit(None).cast(col_type))\n",
        "    return result_df.select([field.name for field in target_schema.fields])\n",
        "\n",
        "df_enforced = enforce_schema(df_customers, expected_schema)\n",
        "print(\"Schema-enforced DataFrame:\")\n",
        "df_enforced.printSchema()\n",
        "df_enforced.show(5)"
      ],
      "metadata": {
        "id": "enforce_schema"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "# Part 4: Text Preprocessing\n",
        "---\n",
        "\n",
        "## 4.1 String Functions in Spark\n",
        "\n",
        "Text data often requires extensive preprocessing before analysis."
      ],
      "metadata": {
        "id": "part4_intro"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create sample text data\n",
        "text_data = [\n",
        "    (1, \"   Hello World   \", \"john.doe@email.com\"),\n",
        "    (2, \"APACHE SPARK\", \"JANE.DOE@EMAIL.COM\"),\n",
        "    (3, \"data science\", \"bob_wilson@company.co.uk\"),\n",
        "    (4, \"  Machine Learning  \", \"alice-brown@domain.org\"),\n",
        "    (5, \"NLP is Great!!!\", \"charlie.lee123@test.com\"),\n",
        "]\n",
        "\n",
        "df_text = spark.createDataFrame(text_data, [\"id\", \"text\", \"email\"])\n",
        "print(\"Sample text data:\")\n",
        "df_text.show(truncate=False)"
      ],
      "metadata": {
        "id": "sample_text_data"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.2 Case Normalization and Trimming"
      ],
      "metadata": {
        "id": "case_normalization"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Case conversion\n",
        "df_case = df_text.select(\n",
        "    col(\"text\"),\n",
        "    lower(col(\"text\")).alias(\"lowercase\"),\n",
        "    upper(col(\"text\")).alias(\"uppercase\"),\n",
        "    expr(\"initcap(text)\").alias(\"titlecase\")\n",
        ")\n",
        "\n",
        "print(\"Case conversion:\")\n",
        "df_case.show(truncate=False)"
      ],
      "metadata": {
        "id": "case_conversion"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Trimming whitespace\n",
        "df_trim = df_text.select(\n",
        "    col(\"text\"),\n",
        "    trim(col(\"text\")).alias(\"trimmed\"),\n",
        "    ltrim(col(\"text\")).alias(\"left_trimmed\"),\n",
        "    rtrim(col(\"text\")).alias(\"right_trimmed\"),\n",
        "    length(col(\"text\")).alias(\"original_len\"),\n",
        "    length(trim(col(\"text\"))).alias(\"trimmed_len\")\n",
        ")\n",
        "\n",
        "print(\"Whitespace trimming:\")\n",
        "df_trim.show(truncate=False)"
      ],
      "metadata": {
        "id": "trimming"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.3 Tokenization and Text Splitting"
      ],
      "metadata": {
        "id": "tokenization"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create sample sentences\n",
        "sentences_data = [\n",
        "    (1, \"Apache Spark is a unified analytics engine for big data processing\"),\n",
        "    (2, \"Natural language processing enables computers to understand text\"),\n",
        "    (3, \"Machine learning models require clean preprocessed data\"),\n",
        "    (4, \"Text mining extracts valuable insights from unstructured data\"),\n",
        "]\n",
        "\n",
        "df_sentences = spark.createDataFrame(sentences_data, [\"id\", \"sentence\"])\n",
        "df_sentences.show(truncate=False)"
      ],
      "metadata": {
        "id": "sentences_data"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Basic tokenization using split\n",
        "from pyspark.sql.functions import split, size, explode\n",
        "\n",
        "df_tokens = df_sentences.withColumn(\n",
        "    \"words\",\n",
        "    split(lower(col(\"sentence\")), \" \")\n",
        ").withColumn(\n",
        "    \"word_count\",\n",
        "    size(col(\"words\"))\n",
        ")\n",
        "\n",
        "print(\"Tokenized sentences:\")\n",
        "df_tokens.show(truncate=False)"
      ],
      "metadata": {
        "id": "basic_tokenization"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Explode tokens and calculate word frequency\n",
        "df_exploded = df_tokens.select(\n",
        "    col(\"id\"),\n",
        "    explode(col(\"words\")).alias(\"word\")\n",
        ")\n",
        "\n",
        "word_freq = df_exploded.groupBy(\"word\").agg(\n",
        "    count(\"*\").alias(\"frequency\")\n",
        ").orderBy(col(\"frequency\").desc())\n",
        "\n",
        "print(\"Word frequencies:\")\n",
        "word_freq.show(15)"
      ],
      "metadata": {
        "id": "word_frequency"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.4 Stop Word Removal"
      ],
      "metadata": {
        "id": "stop_words"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define common English stop words\n",
        "stop_words = [\n",
        "    \"a\", \"an\", \"the\", \"and\", \"or\", \"but\", \"is\", \"are\", \"was\", \"were\",\n",
        "    \"to\", \"of\", \"in\", \"for\", \"on\", \"with\", \"at\", \"by\", \"from\",\n",
        "    \"it\", \"its\", \"this\", \"that\", \"these\", \"those\",\n",
        "    \"be\", \"been\", \"being\", \"have\", \"has\", \"had\"\n",
        "]\n",
        "\n",
        "from pyspark.sql.functions import array_except, array\n",
        "\n",
        "stop_words_array = array([lit(w) for w in stop_words])\n",
        "\n",
        "df_no_stopwords = df_tokens.withColumn(\n",
        "    \"words_filtered\",\n",
        "    array_except(col(\"words\"), stop_words_array)\n",
        ")\n",
        "\n",
        "print(\"Words with stop words removed:\")\n",
        "df_no_stopwords.select(\"sentence\", \"words\", \"words_filtered\").show(truncate=False)"
      ],
      "metadata": {
        "id": "remove_stopwords"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Using Spark ML's StopWordsRemover\n",
        "from pyspark.ml.feature import StopWordsRemover, Tokenizer\n",
        "\n",
        "tokenizer = Tokenizer(inputCol=\"sentence\", outputCol=\"words_ml\")\n",
        "df_tokenized_ml = tokenizer.transform(df_sentences)\n",
        "\n",
        "remover = StopWordsRemover(inputCol=\"words_ml\", outputCol=\"words_cleaned\")\n",
        "df_cleaned_ml = remover.transform(df_tokenized_ml)\n",
        "\n",
        "print(\"Using Spark ML StopWordsRemover:\")\n",
        "df_cleaned_ml.select(\"sentence\", \"words_ml\", \"words_cleaned\").show(truncate=False)"
      ],
      "metadata": {
        "id": "ml_stopwords"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.5 Text Cleaning Pipeline"
      ],
      "metadata": {
        "id": "text_pipeline"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample product reviews (messy text data)\n",
        "reviews_data = [\n",
        "    (1, \"  GREAT product!!!  Would buy again.   \", 5),\n",
        "    (2, \"Terrible... don't waste your money \", 1),\n",
        "    (3, \"Good quality, fast shipping!!!\", 4),\n",
        "    (4, \"   not bad, could be better...   \", 3),\n",
        "    (5, \"AMAZING!!!!! Best purchase EVER!!!!!\", 5),\n",
        "    (6, \"meh, it's OK I guess\", 3),\n",
        "    (7, \"Product arrived damaged\", 1),\n",
        "    (8, \"10/10 would recommend to friends!\", 5),\n",
        "]\n",
        "\n",
        "df_reviews = spark.createDataFrame(reviews_data, [\"id\", \"review\", \"rating\"])\n",
        "print(\"Raw reviews:\")\n",
        "df_reviews.show(truncate=False)"
      ],
      "metadata": {
        "id": "reviews_data"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_text_pipeline(df, text_col, output_col=\"text_cleaned\"):\n",
        "    result = df \\\n",
        "        .withColumn(\"_step1_trim\", trim(col(text_col))) \\\n",
        "        .withColumn(\"_step2_lower\", lower(col(\"_step1_trim\"))) \\\n",
        "        .withColumn(\"_step3_alphanum\",\n",
        "                    regexp_replace(col(\"_step2_lower\"), \"[^a-z0-9\\\\s]\", \"\")) \\\n",
        "        .withColumn(\"_step4_spaces\",\n",
        "                    regexp_replace(col(\"_step3_alphanum\"), \"\\\\s+\", \" \")) \\\n",
        "        .withColumn(output_col, trim(col(\"_step4_spaces\"))) \\\n",
        "        .drop(\"_step1_trim\", \"_step2_lower\", \"_step3_alphanum\", \"_step4_spaces\")\n",
        "    return result\n",
        "\n",
        "df_reviews_cleaned = clean_text_pipeline(df_reviews, \"review\", \"review_cleaned\")\n",
        "\n",
        "print(\"Cleaned reviews:\")\n",
        "df_reviews_cleaned.select(\"review\", \"review_cleaned\").show(truncate=False)"
      ],
      "metadata": {
        "id": "text_pipeline_function"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.6 Practical Exercise: Product Review Cleaning\n",
        "\n",
        "**Tasks:**\n",
        "1. Clean the review text (remove special chars, normalize case, trim)\n",
        "2. Extract the word count from each cleaned review\n",
        "3. Identify reviews that mention negative keywords (return, refund, broken)\n",
        "4. Calculate the average rating for reviews with negative keywords vs others"
      ],
      "metadata": {
        "id": "exercise_text"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Extended reviews dataset\n",
        "extended_reviews = [\n",
        "    (1, \"LOVE IT! Fast shipping, great quality. Will order more!\", 5),\n",
        "    (2, \"Product broke after 1 week. Requesting refund immediately.\", 1),\n",
        "    (3, \"Decent product for the price. Nothing special but works.\", 3),\n",
        "    (4, \"DO NOT BUY! Cheap quality, had to return it!\", 1),\n",
        "    (5, \"Excellent! Better than expected. 100% recommend.\", 5),\n",
        "    (6, \"Item was damaged during shipping. Broken on arrival.\", 1),\n",
        "    (7, \"Good value for money. Satisfied with purchase.\", 4),\n",
        "    (8, \"WORST PURCHASE EVER! Want my money back!!!\", 1),\n",
        "    (9, \"Nice product, works as described. Happy customer.\", 4),\n",
        "    (10, \"Meh, it's okay. Nothing to complain about.\", 3),\n",
        "]\n",
        "\n",
        "df_extended = spark.createDataFrame(extended_reviews, [\"id\", \"review\", \"rating\"])\n",
        "df_extended.show(truncate=False)"
      ],
      "metadata": {
        "id": "extended_reviews"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Your code here\n",
        "\n"
      ],
      "metadata": {
        "id": "exercise_text_code"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "# Part 5: Regular Expressions in Spark\n",
        "---\n",
        "\n",
        "## 5.1 Introduction to Regex in PySpark\n",
        "\n",
        "**Key Functions:**\n",
        "- `rlike(pattern)` - Returns true if string matches pattern\n",
        "- `regexp_extract(col, pattern, idx)` - Extracts groups matching the pattern\n",
        "- `regexp_replace(col, pattern, replacement)` - Replaces matches with new text\n",
        "\n",
        "**Common Regex Patterns:**\n",
        "| Pattern | Meaning |\n",
        "|---------|----------|\n",
        "| `\\d` | Any digit (0-9) |\n",
        "| `\\w` | Any word character |\n",
        "| `\\s` | Any whitespace |\n",
        "| `.` | Any character |\n",
        "| `*` | Zero or more |\n",
        "| `+` | One or more |\n",
        "| `[]` | Character class |\n",
        "| `()` | Capture group |"
      ],
      "metadata": {
        "id": "part5_intro"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample data for regex operations\n",
        "contact_data = [\n",
        "    (1, \"John Smith\", \"john.smith@email.com\", \"(555) 123-4567\", \"123 Main St, New York, NY 10001\"),\n",
        "    (2, \"Jane Doe\", \"jane_doe@company.co.uk\", \"555-987-6543\", \"456 Oak Ave, Los Angeles, CA 90001\"),\n",
        "    (3, \"Bob Wilson\", \"bob123@test.org\", \"5551234567\", \"789 Pine Rd, Chicago, IL 60601\"),\n",
        "    (4, \"Alice Brown\", \"alice.brown@domain.net\", \"(555)456-7890\", \"321 Elm Blvd, Houston, TX 77001\"),\n",
        "    (5, \"Charlie Lee\", \"invalid-email\", \"phone: 555-111-2222\", \"PO Box 100, Phoenix, AZ 85001\"),\n",
        "]\n",
        "\n",
        "df_contacts = spark.createDataFrame(\n",
        "    contact_data,\n",
        "    [\"id\", \"name\", \"email\", \"phone\", \"address\"]\n",
        ")\n",
        "\n",
        "print(\"Contact data:\")\n",
        "df_contacts.show(truncate=False)"
      ],
      "metadata": {
        "id": "regex_data"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5.2 Pattern Matching with rlike"
      ],
      "metadata": {
        "id": "rlike_section"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check if email is valid format\n",
        "email_pattern = r\"^[\\w.+-]+@[\\w-]+\\.[a-zA-Z]{2,}$\"\n",
        "\n",
        "df_email_check = df_contacts.withColumn(\n",
        "    \"is_valid_email\",\n",
        "    col(\"email\").rlike(email_pattern)\n",
        ")\n",
        "\n",
        "print(\"Email validation:\")\n",
        "df_email_check.select(\"name\", \"email\", \"is_valid_email\").show(truncate=False)"
      ],
      "metadata": {
        "id": "rlike_email"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Filter for specific domain emails\n",
        "df_company_emails = df_contacts.filter(\n",
        "    col(\"email\").rlike(r\"@company\\.\")\n",
        ")\n",
        "\n",
        "print(\"Company domain emails:\")\n",
        "df_company_emails.select(\"name\", \"email\").show()"
      ],
      "metadata": {
        "id": "rlike_filter"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5.3 Extracting Data with regexp_extract"
      ],
      "metadata": {
        "id": "regexp_extract_section"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract email components\n",
        "df_email_parts = df_contacts.withColumn(\n",
        "    \"email_username\",\n",
        "    regexp_extract(col(\"email\"), r\"^([\\w.+-]+)@\", 1)\n",
        ").withColumn(\n",
        "    \"email_domain\",\n",
        "    regexp_extract(col(\"email\"), r\"@([\\w-]+)\\.\", 1)\n",
        ").withColumn(\n",
        "    \"email_tld\",\n",
        "    regexp_extract(col(\"email\"), r\"\\.([a-zA-Z.]+)$\", 1)\n",
        ")\n",
        "\n",
        "print(\"Extracted email components:\")\n",
        "df_email_parts.select(\"email\", \"email_username\", \"email_domain\", \"email_tld\").show(truncate=False)"
      ],
      "metadata": {
        "id": "extract_email"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract and normalize phone numbers\n",
        "df_phone_extract = df_contacts.withColumn(\n",
        "    \"phone_digits\",\n",
        "    regexp_replace(col(\"phone\"), r\"[^\\d]\", \"\")\n",
        ").withColumn(\n",
        "    \"area_code\",\n",
        "    regexp_extract(regexp_replace(col(\"phone\"), r\"[^\\d]\", \"\"), r\"^(\\d{3})\", 1)\n",
        ").withColumn(\n",
        "    \"phone_normalized\",\n",
        "    concat(\n",
        "        lit(\"(\"),\n",
        "        substring(regexp_replace(col(\"phone\"), r\"[^\\d]\", \"\"), 1, 3),\n",
        "        lit(\") \"),\n",
        "        substring(regexp_replace(col(\"phone\"), r\"[^\\d]\", \"\"), 4, 3),\n",
        "        lit(\"-\"),\n",
        "        substring(regexp_replace(col(\"phone\"), r\"[^\\d]\", \"\"), 7, 4)\n",
        "    )\n",
        ")\n",
        "\n",
        "print(\"Phone number extraction and normalization:\")\n",
        "df_phone_extract.select(\"phone\", \"phone_digits\", \"area_code\", \"phone_normalized\").show(truncate=False)"
      ],
      "metadata": {
        "id": "extract_phone"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract address components\n",
        "df_address_parts = df_contacts.withColumn(\n",
        "    \"street\",\n",
        "    regexp_extract(col(\"address\"), r\"^(.+),\", 1)\n",
        ").withColumn(\n",
        "    \"city\",\n",
        "    regexp_extract(col(\"address\"), r\", ([^,]+), [A-Z]{2}\", 1)\n",
        ").withColumn(\n",
        "    \"state\",\n",
        "    regexp_extract(col(\"address\"), r\", ([A-Z]{2}) \\d\", 1)\n",
        ").withColumn(\n",
        "    \"zip_code\",\n",
        "    regexp_extract(col(\"address\"), r\"(\\d{5})$\", 1)\n",
        ")\n",
        "\n",
        "print(\"Extracted address components:\")\n",
        "df_address_parts.select(\"address\", \"street\", \"city\", \"state\", \"zip_code\").show(truncate=False)"
      ],
      "metadata": {
        "id": "extract_address"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5.4 Replacing Patterns with regexp_replace"
      ],
      "metadata": {
        "id": "regexp_replace_section"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Mask sensitive data\n",
        "df_masked = df_contacts.withColumn(\n",
        "    \"email_masked\",\n",
        "    regexp_replace(\n",
        "        col(\"email\"),\n",
        "        r\"^(.{2}).*(@.*)$\",\n",
        "        r\"$1***$2\"\n",
        "    )\n",
        ").withColumn(\n",
        "    \"phone_masked\",\n",
        "    regexp_replace(\n",
        "        regexp_replace(col(\"phone\"), r\"[^\\d]\", \"\"),\n",
        "        r\"^(\\d{3})(\\d{3})(\\d{4})$\",\n",
        "        r\"(***) ***-$3\"\n",
        "    )\n",
        ")\n",
        "\n",
        "print(\"Masked sensitive data:\")\n",
        "df_masked.select(\"name\", \"email\", \"email_masked\", \"phone\", \"phone_masked\").show(truncate=False)"
      ],
      "metadata": {
        "id": "mask_data"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Clean and standardize text\n",
        "messy_text_data = [\n",
        "    (1, \"Hello    World!!!\"),\n",
        "    (2, \"Multiple   spaces   here\"),\n",
        "    (3, \"Lots of!!!!! punctuation???\"),\n",
        "    (4, \"   Leading and trailing   \"),\n",
        "    (5, \"MixED CaSe TeXt\"),\n",
        "]\n",
        "\n",
        "df_messy_text = spark.createDataFrame(messy_text_data, [\"id\", \"text\"])\n",
        "\n",
        "df_standardized = df_messy_text.withColumn(\n",
        "    \"text_clean\",\n",
        "    trim(\n",
        "        regexp_replace(\n",
        "            regexp_replace(\n",
        "                lower(col(\"text\")),\n",
        "                r\"([!?.]){2,}\",\n",
        "                r\"$1\"\n",
        "            ),\n",
        "            r\"\\s+\",\n",
        "            \" \"\n",
        "        )\n",
        "    )\n",
        ")\n",
        "\n",
        "print(\"Text standardization:\")\n",
        "df_standardized.show(truncate=False)"
      ],
      "metadata": {
        "id": "standardize_text"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5.5 Practical Exercise: Log File Parsing\n",
        "\n",
        "Parse web server access logs using regex to extract structured information.\n",
        "\n",
        "**Tasks:**\n",
        "1. Extract IP address, timestamp, HTTP method, URL path, status code, and response size\n",
        "2. Filter for error status codes (4xx and 5xx)\n",
        "3. Count requests by HTTP method\n",
        "4. Identify the most accessed URLs"
      ],
      "metadata": {
        "id": "exercise_logs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample Apache access logs\n",
        "log_data = [\n",
        "    (1, '192.168.1.100 - - [15/Jan/2024:10:30:45 +0000] \"GET /index.html HTTP/1.1\" 200 1234'),\n",
        "    (2, '10.0.0.50 - - [15/Jan/2024:10:31:00 +0000] \"POST /api/users HTTP/1.1\" 201 567'),\n",
        "    (3, '192.168.1.101 - - [15/Jan/2024:10:31:15 +0000] \"GET /about.html HTTP/1.1\" 200 890'),\n",
        "    (4, '172.16.0.25 - - [15/Jan/2024:10:31:30 +0000] \"GET /missing.html HTTP/1.1\" 404 234'),\n",
        "    (5, '192.168.1.100 - - [15/Jan/2024:10:31:45 +0000] \"GET /api/data HTTP/1.1\" 500 100'),\n",
        "    (6, '10.0.0.75 - - [15/Jan/2024:10:32:00 +0000] \"PUT /api/users/1 HTTP/1.1\" 200 456'),\n",
        "    (7, '192.168.1.102 - - [15/Jan/2024:10:32:15 +0000] \"DELETE /api/users/2 HTTP/1.1\" 204 0'),\n",
        "    (8, '172.16.0.30 - - [15/Jan/2024:10:32:30 +0000] \"GET /index.html HTTP/1.1\" 200 1234'),\n",
        "    (9, '10.0.0.50 - - [15/Jan/2024:10:32:45 +0000] \"POST /api/login HTTP/1.1\" 401 89'),\n",
        "    (10, '192.168.1.100 - - [15/Jan/2024:10:33:00 +0000] \"GET /api/products HTTP/1.1\" 200 5678'),\n",
        "]\n",
        "\n",
        "df_logs = spark.createDataFrame(log_data, [\"id\", \"log_line\"])\n",
        "print(\"Sample access logs:\")\n",
        "df_logs.show(truncate=False)"
      ],
      "metadata": {
        "id": "log_data"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Your code here: Parse the log files\n",
        "\n"
      ],
      "metadata": {
        "id": "exercise_logs_code"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "# Part 6: Building Complete Preprocessing Pipelines\n",
        "---\n",
        "\n",
        "## 6.1 Combining Multiple Preprocessing Steps"
      ],
      "metadata": {
        "id": "part6_intro"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Real-world messy dataset\n",
        "raw_data = [\n",
        "    (\"1\", \"  JOHN SMITH  \", \"john.smith@EMAIL.com\", \"(555) 123-4567\", \"25\", \"$50,000.00\", \"2024-01-15\"),\n",
        "    (\"2\", \"jane doe\", \"JANE_DOE@COMPANY.CO.UK\", \"555-987-6543\", \"N/A\", \"60000\", \"15/02/2024\"),\n",
        "    (\"3\", \"Bob Wilson\", \"invalid-email\", \"5551234567\", \"35\", \"75,000\", \"March 20, 2024\"),\n",
        "    (\"4\", None, \"alice@domain.net\", \"(555)456-7890\", \"28\", None, \"2024-04-10\"),\n",
        "    (\"5\", \"Charlie Lee\", \"\", \"phone: 555-111-2222\", \"forty\", \"$80,000\", None),\n",
        "]\n",
        "\n",
        "df_raw = spark.createDataFrame(\n",
        "    raw_data,\n",
        "    [\"id\", \"name\", \"email\", \"phone\", \"age\", \"salary\", \"hire_date\"]\n",
        ")\n",
        "\n",
        "print(\"Raw messy data:\")\n",
        "df_raw.show(truncate=False)"
      ],
      "metadata": {
        "id": "raw_messy_data"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_employee_data(df):\n",
        "    # Step 1: Clean and standardize name\n",
        "    df = df.withColumn(\n",
        "        \"name_clean\",\n",
        "        when(col(\"name\").isNull() | (trim(col(\"name\")) == \"\"),\n",
        "             concat(lit(\"Employee_\"), col(\"id\")))\n",
        "        .otherwise(expr(\"initcap(trim(name))\"))\n",
        "    )\n",
        "    # Step 2: Clean and validate email\n",
        "    email_pattern = r\"^[\\w.+-]+@[\\w-]+\\.[a-zA-Z]{2,}$\"\n",
        "    df = df.withColumn(\n",
        "        \"email_clean\",\n",
        "        when(lower(trim(col(\"email\"))).rlike(email_pattern),\n",
        "             lower(trim(col(\"email\"))))\n",
        "        .otherwise(None)\n",
        "    )\n",
        "    # Step 3: Normalize phone number\n",
        "    df = df.withColumn(\n",
        "        \"phone_digits\",\n",
        "        regexp_replace(col(\"phone\"), r\"[^\\d]\", \"\")\n",
        "    ).withColumn(\n",
        "        \"phone_clean\",\n",
        "        when(length(col(\"phone_digits\")) == 10,\n",
        "             concat(\n",
        "                 lit(\"(\"), substring(col(\"phone_digits\"), 1, 3), lit(\") \"),\n",
        "                 substring(col(\"phone_digits\"), 4, 3), lit(\"-\"),\n",
        "                 substring(col(\"phone_digits\"), 7, 4)\n",
        "             ))\n",
        "        .otherwise(None)\n",
        "    )\n",
        "    # Step 4: Convert age to integer\n",
        "    df = df.withColumn(\"age_clean\", col(\"age\").cast(IntegerType()))\n",
        "    # Step 5: Clean and convert salary\n",
        "    df = df.withColumn(\n",
        "        \"salary_clean\",\n",
        "        regexp_replace(col(\"salary\"), r\"[$,]\", \"\").cast(DoubleType())\n",
        "    )\n",
        "    # Step 6: Parse hire date\n",
        "    df = df.withColumn(\n",
        "        \"hire_date_clean\",\n",
        "        coalesce(\n",
        "            to_date(col(\"hire_date\"), \"yyyy-MM-dd\"),\n",
        "            to_date(col(\"hire_date\"), \"dd/MM/yyyy\"),\n",
        "            to_date(col(\"hire_date\"), \"MMMM dd, yyyy\")\n",
        "        )\n",
        "    )\n",
        "    return df.select(\n",
        "        col(\"id\").cast(IntegerType()).alias(\"id\"),\n",
        "        col(\"name_clean\").alias(\"name\"),\n",
        "        col(\"email_clean\").alias(\"email\"),\n",
        "        col(\"phone_clean\").alias(\"phone\"),\n",
        "        col(\"age_clean\").alias(\"age\"),\n",
        "        col(\"salary_clean\").alias(\"salary\"),\n",
        "        col(\"hire_date_clean\").alias(\"hire_date\")\n",
        "    )\n",
        "\n",
        "df_clean = preprocess_employee_data(df_raw)\n",
        "\n",
        "print(\"Cleaned data:\")\n",
        "df_clean.show(truncate=False)\n",
        "df_clean.printSchema()"
      ],
      "metadata": {
        "id": "complete_pipeline"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6.2 Data Quality Reporting"
      ],
      "metadata": {
        "id": "data_quality"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_quality_report(df_original, df_cleaned):\n",
        "    total_rows = df_original.count()\n",
        "    print(\"=\"*60)\n",
        "    print(\"DATA QUALITY REPORT\")\n",
        "    print(\"=\"*60)\n",
        "    print(f\"\\nTotal rows: {total_rows}\")\n",
        "    print(\"\\n--- Missing Value Summary (Cleaned Data) ---\")\n",
        "    for col_name in df_cleaned.columns:\n",
        "        null_count = df_cleaned.filter(col(col_name).isNull()).count()\n",
        "        null_pct = (null_count / total_rows) * 100\n",
        "        status = \"OK\" if null_pct < 10 else \"WARNING\" if null_pct < 30 else \"CRITICAL\"\n",
        "        print(f\"  {col_name}: {null_count} nulls ({null_pct:.1f}%) [{status}]\")\n",
        "    total_cells = total_rows * len(df_cleaned.columns)\n",
        "    null_cells = sum(df_cleaned.filter(col(c).isNull()).count() for c in df_cleaned.columns)\n",
        "    completeness = ((total_cells - null_cells) / total_cells) * 100\n",
        "    print(f\"\\n--- Overall Data Quality Score ---\")\n",
        "    print(f\"  Completeness: {completeness:.1f}%\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "generate_quality_report(df_raw, df_clean)"
      ],
      "metadata": {
        "id": "quality_report"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6.3 Final Challenge: End-to-End Data Cleaning\n",
        "\n",
        "Apply everything you've learned to clean a complex real-world dataset.\n",
        "\n",
        "**Dataset:** E-commerce transaction data with multiple quality issues\n",
        "\n",
        "**Tasks:**\n",
        "1. Identify and report all data quality issues\n",
        "2. Handle missing values appropriately for each column\n",
        "3. Standardize text fields (product names, categories)\n",
        "4. Parse and validate dates\n",
        "5. Clean and validate email addresses\n",
        "6. Extract useful information using regex\n",
        "7. Create a comprehensive data quality report\n",
        "8. Output the cleaned dataset"
      ],
      "metadata": {
        "id": "final_challenge"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Complex e-commerce dataset\n",
        "ecommerce_data = [\n",
        "    (\"ORD001\", \"  LAPTOP Computer  \", \"ELECTRONICS\", \"john@email.com\", \"$999.99\", \"2\", \"2024-01-15 10:30:00\", \"Completed\"),\n",
        "    (\"ORD002\", \"wireless mouse\", \"electronics\", \"JANE@COMPANY.CO.UK\", \"29.99\", None, \"15/01/2024\", \"completed\"),\n",
        "    (\"ORD003\", \"USB-C Cable (6ft)\", \"Accessories\", \"invalid-email\", \"$12.50\", \"5\", \"January 16, 2024\", \"PENDING\"),\n",
        "    (\"ORD004\", None, \"CLOTHING\", \"bob@test.org\", \"N/A\", \"1\", \"2024-01-16\", \"Cancelled\"),\n",
        "    (\"ORD005\", \"Running Shoes - Size 10\", \"  clothing  \", \"\", \"89.00\", \"2\", None, \"Shipped\"),\n",
        "    (\"ORD006\", \"HDMI Cable!!!\", \"accessories\", \"alice@domain.net\", \"$15.00\", \"10\", \"2024-01-17 14:20:00\", \"completed\"),\n",
        "    (\"ORD007\", \"smartphone case\", \"Electronics\", \"charlie@email.com\", \"$25\", \"3\", \"17-Jan-2024\", \"Processing\"),\n",
        "    (\"ORD008\", \"  Winter Jacket  \", \"Clothing\", \"N/A\", \"$149.99\", None, \"2024-01-18\", \"SHIPPED\"),\n",
        "]\n",
        "\n",
        "df_ecommerce = spark.createDataFrame(\n",
        "    ecommerce_data,\n",
        "    [\"order_id\", \"product\", \"category\", \"email\", \"price\", \"quantity\", \"order_date\", \"status\"]\n",
        ")\n",
        "\n",
        "print(\"Raw e-commerce data:\")\n",
        "df_ecommerce.show(truncate=False)"
      ],
      "metadata": {
        "id": "ecommerce_data"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Your code here: Build a complete preprocessing pipeline\n",
        "\n"
      ],
      "metadata": {
        "id": "final_challenge_code"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "# Cleanup\n",
        "---\n",
        "\n",
        "Always remember to stop your Spark session when you're done."
      ],
      "metadata": {
        "id": "cleanup"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Stop the Spark session\n",
        "spark.stop()\n",
        "print(\"Spark session stopped.\")"
      ],
      "metadata": {
        "id": "stop_spark"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "# Summary\n",
        "---\n",
        "\n",
        "In this lab, you learned:\n",
        "\n",
        "1. **Missing Value Handling**\n",
        "   - Identifying missing data (null, NaN, empty strings, placeholders)\n",
        "   - Dropping rows/columns with missing values\n",
        "   - Imputation strategies (mean, median, mode, group-wise)\n",
        "   - Forward/backward fill for time series\n",
        "   - Creating missing value indicators\n",
        "\n",
        "2. **Data Type Management**\n",
        "   - Understanding Spark data types\n",
        "   - Type casting and conversion\n",
        "   - Handling dates and timestamps\n",
        "   - Schema validation and enforcement\n",
        "\n",
        "3. **Text Preprocessing**\n",
        "   - Case normalization and trimming\n",
        "   - Tokenization and text splitting\n",
        "   - Stop word removal\n",
        "   - Building text cleaning pipelines\n",
        "\n",
        "4. **Regular Expressions**\n",
        "   - Pattern matching with rlike\n",
        "   - Data extraction with regexp_extract\n",
        "   - Pattern replacement with regexp_replace\n",
        "   - Complex parsing tasks (emails, phones, addresses, logs)\n",
        "\n",
        "5. **Building Pipelines**\n",
        "   - Combining multiple preprocessing steps\n",
        "   - Creating reusable functions\n",
        "   - Data quality reporting"
      ],
      "metadata": {
        "id": "summary"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "# Additional Resources\n",
        "---\n",
        "\n",
        "- Apache Spark Documentation: https://spark.apache.org/docs/latest/\n",
        "- PySpark API Reference: https://spark.apache.org/docs/latest/api/python/\n",
        "- Spark SQL Functions: https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/functions.html\n",
        "- Regular Expression Reference: https://docs.python.org/3/library/re.html\n",
        "- Spark ML Feature Transformers: https://spark.apache.org/docs/latest/ml-features.html"
      ],
      "metadata": {
        "id": "resources"
      }
    }
  ]
}